1
00:00:00,001 --> 00:00:10,800
So, I'm Saiz Choudhury from Carnegie Mellon.

2
00:00:10,800 --> 00:00:13,240
Those are my roles at Carnegie Mellon.

3
00:00:13,240 --> 00:00:16,420
Like some of you, I have left Twitter and I am on Blue Sky, so if you want to engage

4
00:00:16,420 --> 00:00:20,920
on social media, please go to Blue Sky instead of Twitter, at least in my case.

5
00:00:20,920 --> 00:00:26,720
I'm here to talk about the Open Forum for Artificial Intelligence, or OFAI, as we call

6
00:00:26,720 --> 00:00:28,580
it, at CMU.

7
00:00:28,580 --> 00:00:33,840
This was launched this past July 2024, and it's received funding from Carnegie Mellon

8
00:00:33,840 --> 00:00:39,200
University Libraries, from our reserve funds, and also two organizations called the Amidyar

9
00:00:39,200 --> 00:00:41,540
Network and the Noble Reach Foundation.

10
00:00:41,540 --> 00:00:44,100
You may not be familiar with those groups.

11
00:00:44,100 --> 00:00:49,920
They were started by people from the private equity, venture finance, and tech world, but

12
00:00:49,920 --> 00:00:54,260
they have non-profit wings through which we are getting our funding.

13
00:00:54,260 --> 00:00:56,960
The CEO of Noble Reach Foundation is Arun Gupta.

14
00:00:56,960 --> 00:01:01,560
He's written a book called Venture Meets Mission, and the idea is how does the venture Silicon

15
00:01:01,560 --> 00:01:05,440
Valley way of doing things meet the public sector?

16
00:01:05,440 --> 00:01:12,200
So he likes to use the term mission capital, and that's what they've provided for OFAI.

17
00:01:12,200 --> 00:01:19,320
So the overarching goal for OFAI is to bend the art towards human-centered AI.

18
00:01:19,320 --> 00:01:24,560
And by human-centered, what OFAI means is that the people who use and are affected by

19
00:01:24,560 --> 00:01:30,920
AI should be the ones who evaluate, test, and affirm the safety, the transparency, the

20
00:01:30,920 --> 00:01:36,320
lack of bias, and so on, rather than the builders or deployers of AI systems.

21
00:01:36,320 --> 00:01:42,120
So right now we have large companies deploying these models on some frequent basis.

22
00:01:42,120 --> 00:01:46,080
We all react to those and look at them and study them.

23
00:01:46,080 --> 00:01:50,500
It's largely difficult to understand the impact from a transparency or reproducibility or

24
00:01:50,500 --> 00:01:51,620
safety perspective.

25
00:01:51,620 --> 00:01:53,160
We're relying on them to self-report.

26
00:01:53,160 --> 00:01:59,360
OFAI, in some sense, is trying to invert that so that it's a more proactive approach, that

27
00:01:59,360 --> 00:02:04,720
we can say these are the desirable attributes of an AI system, and please build and design

28
00:02:04,720 --> 00:02:07,560
and deploy your models accordingly.

29
00:02:07,560 --> 00:02:11,640
If you think about the development of the internet and the web, there was a triad or

30
00:02:11,640 --> 00:02:15,600
a triple helix of industry, government, and university.

31
00:02:15,600 --> 00:02:19,660
And if you think of the shift from the internet to the web to AI, the role of the private

32
00:02:19,660 --> 00:02:23,600
sector has grown considerably in that triple helix.

33
00:02:23,600 --> 00:02:26,720
And I'm not saying that in a values type of way.

34
00:02:26,720 --> 00:02:33,080
I'm just noting that that imbalance, I think, has some consequences, some of them unintended.

35
00:02:33,080 --> 00:02:39,040
But we need to address that and try and rebalance that partnership or triad.

36
00:02:39,040 --> 00:02:44,080
And the way that we're trying to do that through OFAI is to raise the voice of the universities

37
00:02:44,080 --> 00:02:46,060
as a sector.

38
00:02:46,060 --> 00:02:52,400
So while this is a CMU-led initiative, we have currently three other universities involved,

39
00:02:52,400 --> 00:02:56,480
Georgia Tech, George Washington University, and the University of Texas at Austin.

40
00:02:56,480 --> 00:03:02,200
And all of those organizations are involved through their open source programs offices.

41
00:03:02,200 --> 00:03:07,100
Carnegie Mellon has a more broad type of involvement through the OSPO, but also through things

42
00:03:07,100 --> 00:03:10,360
like our Responsible AI program.

43
00:03:10,360 --> 00:03:15,060
But we have these universities coming together to work on OFAI issues.

44
00:03:15,060 --> 00:03:18,560
And we are looking for additional partners, of course.

45
00:03:18,560 --> 00:03:22,560
Now in addition to the university voice, we recognize there's other types of capacity

46
00:03:22,560 --> 00:03:25,620
or expertise that is very important to tap into.

47
00:03:25,620 --> 00:03:28,880
So a couple of examples include the Open Source Initiative.

48
00:03:28,880 --> 00:03:31,600
So I'm a board member at OSI.

49
00:03:31,600 --> 00:03:35,560
This is the group that is the official steward of the definition and licenses around open

50
00:03:35,560 --> 00:03:37,160
source software.

51
00:03:37,160 --> 00:03:40,920
And we have gone through a process to define open source AI, which I'll talk about in a

52
00:03:40,920 --> 00:03:42,520
few slides.

53
00:03:42,520 --> 00:03:47,800
The Atlantic Council is a think tank or a policy group in Washington, DC.

54
00:03:47,800 --> 00:03:53,160
And the Creative Commons, I think many of you know from the open data world.

55
00:03:53,160 --> 00:03:56,120
Conscience is a group you may not have heard of.

56
00:03:56,120 --> 00:04:02,160
It's a nonprofit that is using AI for drug discovery where there are market failures.

57
00:04:02,160 --> 00:04:06,600
So when the pharmaceutical industry says there's no profit here, there's no incentive here,

58
00:04:06,600 --> 00:04:10,840
Conscience has done some really amazing work in drug discovery in that area.

59
00:04:10,840 --> 00:04:18,600
Both Creative Commons and Conscience are providing a lot of capacity around public data.

60
00:04:18,600 --> 00:04:23,180
We're engaging the private and public sector as well, but not as partners.

61
00:04:23,180 --> 00:04:29,120
So we have individuals from the following organizations that are involved in OFAI.

62
00:04:29,120 --> 00:04:32,460
And in the case of the Navy, it's the chief technology officer.

63
00:04:32,460 --> 00:04:37,200
So this is very intentional in that we recognize it's, of course, valuable to get the perspective

64
00:04:37,200 --> 00:04:41,200
of the private and public sector, but we don't want people to feel like they're having the

65
00:04:41,200 --> 00:04:45,760
same level of influence, quite frankly, within our network.

66
00:04:45,760 --> 00:04:48,820
So they're not even considered an advisory board.

67
00:04:48,820 --> 00:04:51,640
They're more like sounding boards or connectors.

68
00:04:51,640 --> 00:04:54,520
We are trying to form an OFAI advisory board as well.

69
00:04:54,520 --> 00:04:58,400
I'm glad to note that somebody from the Mozilla Foundation has agreed.

70
00:04:58,400 --> 00:05:00,740
Somebody from the Atlantic Council has agreed.

71
00:05:00,740 --> 00:05:06,160
And we have a colleague at CMU, Anne Lambright, who's doing a lot of great work with my colleague,

72
00:05:06,160 --> 00:05:10,840
Nikki Agate, who's in the crowd, on humanities perspectives on AI.

73
00:05:10,840 --> 00:05:17,080
So we're getting additional feedback in addition to the partners themselves.

74
00:05:17,080 --> 00:05:19,600
This is a snapshot of the current leadership.

75
00:05:19,600 --> 00:05:21,360
So I am the director of it.

76
00:05:21,360 --> 00:05:23,560
We have a lot of people from CMU.

77
00:05:23,560 --> 00:05:29,280
Hoda Adari is the co-lead for the Responsible AI program at Carnegie Mellon, and she leads

78
00:05:29,280 --> 00:05:31,240
the research working group.

79
00:05:31,880 --> 00:05:36,960
Vince Shaw, who you'll see in a video clip shortly, is an assistant dean in our school

80
00:05:36,960 --> 00:05:42,280
of our College of Humanities and Social Sciences and leads the technical prototypes group.

81
00:05:42,280 --> 00:05:46,600
Deb Bryant is a long-time, well-respected leader in the open source community who is

82
00:05:46,600 --> 00:05:50,160
leading our community engagement efforts.

83
00:05:50,160 --> 00:05:51,760
Trey Herr is at the Atlantic Council.

84
00:05:51,760 --> 00:05:56,880
He's one of our advisors, and he's helping us with the policy recommendations effort.

85
00:05:56,880 --> 00:05:59,960
And Rachel Zomback is another colleague of mine at Carnegie Mellon who's leading what's

86
00:05:59,960 --> 00:06:01,920
called the Talent for Service track.

87
00:06:01,920 --> 00:06:07,360
And that is basically how do you identify people who have the right skill sets and could

88
00:06:07,360 --> 00:06:13,280
be oriented towards a more systems view, a holistic view AI, and prepare them through

89
00:06:13,280 --> 00:06:17,800
training and curricular development for roles in the public sector.

90
00:06:17,800 --> 00:06:24,640
So we see all of these groups having distinct goals and scope, but there's a strong interrelationship

91
00:06:24,640 --> 00:06:25,640
between them.

92
00:06:26,160 --> 00:06:30,600
And I'll talk about that a little bit with something called our landscape map.

93
00:06:30,600 --> 00:06:36,120
So the open in OFAI has a few particular connotations, if you will.

94
00:06:36,120 --> 00:06:38,960
One of them is that the value of openness is a design principle.

95
00:06:38,960 --> 00:06:40,840
So I talked about the Internet and the web.

96
00:06:40,840 --> 00:06:47,160
If you think about TCP/IP or HTTP as protocols, these were open protocols.

97
00:06:47,160 --> 00:06:49,480
They were global protocols in many ways.

98
00:06:49,480 --> 00:06:53,040
What they did is remove all the local variants.

99
00:06:53,040 --> 00:06:57,240
If you remember, IBM had something called Token Ring, which is a way they were exchanging

100
00:06:57,240 --> 00:06:59,240
information over the Internet.

101
00:06:59,240 --> 00:07:03,880
When TCP/IP came out, it made no sense to run a local variant, right?

102
00:07:03,880 --> 00:07:09,000
So having these global open protocols and approach is really important for fostering

103
00:07:09,000 --> 00:07:13,360
innovation and participation and so on.

104
00:07:13,360 --> 00:07:18,160
There are now well-documented benefits from this kind of openness in the open source software

105
00:07:18,160 --> 00:07:19,160
community.

106
00:07:19,160 --> 00:07:23,400
The Harvard Business School has conducted a study that indicated that open source software

107
00:07:23,400 --> 00:07:29,600
has generated $8.8 trillion of value, and it's reduced the production costs by a factor

108
00:07:29,600 --> 00:07:31,640
of 3.5.

109
00:07:31,640 --> 00:07:34,640
So that's very significant, of course.

110
00:07:34,640 --> 00:07:39,240
Another way of being open is questioning and testing assumptions.

111
00:07:39,240 --> 00:07:45,160
So I'm not immune from, I'll just say, the anxieties around what AI might be doing to

112
00:07:45,160 --> 00:07:51,360
our society, might be doing to us, might be doing in the future on its own.

113
00:07:51,360 --> 00:07:56,840
But I think it's understandable that as we try to grapple with what that might mean,

114
00:07:56,840 --> 00:08:00,200
we start to make assumptions or assertions about how it works.

115
00:08:00,200 --> 00:08:04,760
But I think it's really important to focus on the evidence and the research and ground

116
00:08:04,760 --> 00:08:09,560
whatever types of decisions and strategies and policies we have on that evidence and

117
00:08:09,560 --> 00:08:11,200
on that research.

118
00:08:11,200 --> 00:08:16,760
And then finally, given the speed and really, quite frankly, the potential impact of AI

119
00:08:16,760 --> 00:08:19,560
to be as open as possible with producing our outputs.

120
00:08:19,560 --> 00:08:24,280
I know we can talk about lots of reasons for open access and so on, but I'm going back

121
00:08:24,280 --> 00:08:27,920
to what I think is one of the core principles is get the information and the knowledge out

122
00:08:27,920 --> 00:08:30,880
as soon as possible, as openly as possible.

123
00:08:30,880 --> 00:08:34,840
And that's not to say there are other ways you can't do that, but we're really asking

124
00:08:34,840 --> 00:08:39,960
all of our researchers to focus on preprints, and we're looking at creating policy briefs,

125
00:08:39,960 --> 00:08:43,680
and we're going to produce things like the landscape map, which I'll show you in just

126
00:08:43,680 --> 00:08:45,960
a second.

127
00:08:45,960 --> 00:08:51,600
So at the heart of our work is developing something called the openness and AI framework.

128
00:08:51,600 --> 00:08:58,080
I obviously can't say open AI for really annoying reasons.

129
00:08:58,080 --> 00:09:03,000
So openness and AI framework is a technical framework, but it's not intended to be only

130
00:09:03,000 --> 00:09:05,860
for engineers or AI developers or deployers.

131
00:09:05,860 --> 00:09:09,720
But again, grounding in that technical dimension of AI.

132
00:09:09,720 --> 00:09:14,680
But developed with community engagement, and it rigorously explores and defines what it

133
00:09:14,680 --> 00:09:19,760
means to be open in AI, not as defined by a company.

134
00:09:19,760 --> 00:09:22,200
And what is the ensuing or corresponding value?

135
00:09:22,200 --> 00:09:24,880
So openness is actually not the end goal.

136
00:09:24,880 --> 00:09:31,080
Openness is a means towards safety, transparency, lack of bias, and so on.

137
00:09:31,080 --> 00:09:36,000
And a way that we will hopefully accomplish that is moving from this sense of open source,

138
00:09:36,000 --> 00:09:41,280
which is more about the objects, sort of the components, to open governance, which is more

139
00:09:41,280 --> 00:09:46,800
about how those things are designed, deployed, and shared.

140
00:09:46,800 --> 00:09:51,600
How do we actually assess the openness of an AI system and think about making it more

141
00:09:51,600 --> 00:09:53,640
accessible and so on?

142
00:09:53,640 --> 00:09:57,860
And the research group in particular has made a very important point that openness needs

143
00:09:57,860 --> 00:10:01,880
to be considered in a specific context and for specific use cases.

144
00:10:01,880 --> 00:10:05,680
Again, it's not enough to say this system is open.

145
00:10:05,680 --> 00:10:10,280
I'm not trying to pick on something like the Hugging Face leaderboards that are looking

146
00:10:10,280 --> 00:10:12,480
at openness in AI models.

147
00:10:12,480 --> 00:10:13,960
It's very helpful.

148
00:10:13,960 --> 00:10:18,400
But what questions can I ask of those models when I look at something like that?

149
00:10:18,400 --> 00:10:22,440
It's not enough, in my opinion, to just have a ranking of those models, but to actually

150
00:10:22,440 --> 00:10:29,080
understand the who, the why, the what, the who, the when, and the where.

151
00:10:29,080 --> 00:10:36,160
So one foundational piece of this openness in AI framework is the open source AI definition.

152
00:10:36,160 --> 00:10:41,400
I mentioned that I'm a board member at the Open Source Initiative, and that OSI has led

153
00:10:41,400 --> 00:10:48,480
a at least two-year-long effort in an inclusive community co-design process to come up with

154
00:10:48,480 --> 00:10:51,280
this definition of open source AI.

155
00:10:51,280 --> 00:10:55,280
And in many ways, what it's trying to do is affirm the so-called four freedoms of open

156
00:10:55,280 --> 00:10:56,560
source software.

157
00:10:56,560 --> 00:11:02,440
So one of the benefits that led to that $8.8 trillion value is that if something is truly

158
00:11:02,440 --> 00:11:09,820
open source software, you can use, study, modify, and share the software.

159
00:11:09,820 --> 00:11:14,520
We're trying to bring those same four freedoms to open source AI so that I'll have to be

160
00:11:14,520 --> 00:11:17,720
able to do the same things with an AI system.

161
00:11:17,720 --> 00:11:22,220
Now it's really important to remember, though, that AI is fundamentally different and more

162
00:11:22,220 --> 00:11:24,160
complex than software.

163
00:11:24,160 --> 00:11:34,080
So in many ways, what the open source AI definition has done is introduce the legal and developer,

164
00:11:34,080 --> 00:11:38,360
software developer-oriented perspectives on what it means to have open source AI.

165
00:11:38,360 --> 00:11:43,040
And now what I see OFAI and others doing is bringing the engineering dimensions into that

166
00:11:43,040 --> 00:11:44,040
conversation.

167
00:11:44,040 --> 00:11:47,960
And one of the ways we do that is to obviously look at the definition itself.

168
00:11:47,960 --> 00:11:50,280
This is a screenshot from the definition.

169
00:11:50,280 --> 00:11:53,400
I encourage you to go take a look at it yourself.

170
00:11:53,400 --> 00:11:57,560
And probably just in full disclosure, the most contentious part of this was the requirements

171
00:11:57,560 --> 00:11:59,040
around data.

172
00:11:59,040 --> 00:12:04,960
So the definition did not require that you share all the data you used to train an AI

173
00:12:04,960 --> 00:12:05,960
model.

174
00:12:05,960 --> 00:12:09,400
And that's not because we wouldn't like that to happen, but that's because there are legal

175
00:12:09,400 --> 00:12:11,680
reasons you may not be able to share this.

176
00:12:11,680 --> 00:12:13,040
There are privacy reasons.

177
00:12:13,040 --> 00:12:15,940
Think of medical data, that you may not be able to share the data.

178
00:12:15,940 --> 00:12:17,940
There are practicality reasons.

179
00:12:17,940 --> 00:12:20,840
Some of the digital twins generate petabytes of data.

180
00:12:20,840 --> 00:12:24,040
Are you going to send that to me over the web?

181
00:12:24,040 --> 00:12:25,520
I don't think so.

182
00:12:25,520 --> 00:12:28,840
So we weren't saying that it's not important to have the data.

183
00:12:28,840 --> 00:12:33,200
It's just that in order to have a definition that can actually be actionable and pragmatic

184
00:12:33,200 --> 00:12:39,320
and usable, you can provide detailed information in cases where you can't share the data.

185
00:12:39,320 --> 00:12:43,680
I will say that I posted on LinkedIn a new model.

186
00:12:43,680 --> 00:12:44,760
I don't know how to pronounce it.

187
00:12:44,760 --> 00:12:46,440
I think it's Playos.

188
00:12:46,440 --> 00:12:49,500
That actually does provide the data that they use for training.

189
00:12:49,500 --> 00:12:54,400
The Allen Institute of Washington has Alma, which is another model that does the same

190
00:12:54,400 --> 00:12:55,400
thing.

191
00:12:55,400 --> 00:12:56,400
So there are examples of those.

192
00:12:56,400 --> 00:13:00,520
Those would be consistent with the open source AI definition.

193
00:13:00,520 --> 00:13:03,440
And then there are provisions around the code and the weights.

194
00:13:03,440 --> 00:13:04,840
Those are required to be shared.

195
00:13:04,840 --> 00:13:11,400
So the training code and the weights that you use to tune and configure a model.

196
00:13:11,400 --> 00:13:17,200
This is a screenshot from the Mozilla Foundation's endorsement of the open source AI definition.

197
00:13:17,200 --> 00:13:18,900
It's a very strong endorsement.

198
00:13:18,900 --> 00:13:21,660
And I've highlighted one line in particular.

199
00:13:21,660 --> 00:13:22,660
You can read that yourself.

200
00:13:22,660 --> 00:13:27,620
That it's marking a critical juncture in the evolution of the internet.

201
00:13:27,620 --> 00:13:32,280
I'm not the right person to comment on whether that's a fair statement or not.

202
00:13:32,280 --> 00:13:34,700
But I definitely respect the Mozilla Foundation.

203
00:13:34,700 --> 00:13:37,860
And if that's how they feel, then more power to them.

204
00:13:37,860 --> 00:13:40,840
But I definitely think the last line is really on the mark.

205
00:13:40,840 --> 00:13:45,340
It's about shaping the future of this technology and its impact on society.

206
00:13:45,340 --> 00:13:50,340
And we have a real opportunity to do this in a community-oriented way, in an open way.

207
00:13:50,340 --> 00:13:53,460
However, much challenges we may face.

208
00:13:53,460 --> 00:13:57,460
We think that the open source AI definition puts the proverbial stake in the ground.

209
00:13:57,460 --> 00:14:03,380
If you think about the fact that the EU AI Act mentions open source AI and has less stringent

210
00:14:03,380 --> 00:14:08,140
types of requirements from a regulatory basis, but does not define it.

211
00:14:08,140 --> 00:14:11,980
There are federal documents and memos that talk about open source AI.

212
00:14:11,980 --> 00:14:16,620
And the Office of Management and Budget talked about procuring open source AI.

213
00:14:16,620 --> 00:14:17,780
Doesn't define it.

214
00:14:17,780 --> 00:14:23,860
So we have clear signals that there is value to using open source AI, but not a definition.

215
00:14:23,860 --> 00:14:28,740
So we hope that our definition from the community process can provide that benchmark.

216
00:14:28,740 --> 00:14:33,900
So what we'd like to say is that releasing version 1.0 in October, which we did at the

217
00:14:33,900 --> 00:14:39,820
All Things Open meeting, provided a stable reference point, but not a permanent one.

218
00:14:39,820 --> 00:14:45,260
We do understand that it'll have to evolve over time.

219
00:14:45,260 --> 00:14:49,160
Not everybody is a fan of this definition.

220
00:14:49,160 --> 00:14:51,140
In particular, Meta.

221
00:14:51,140 --> 00:14:53,580
This is an article from The Economist.

222
00:14:53,580 --> 00:14:56,180
You can certainly go take a look at it for yourself.

223
00:14:56,180 --> 00:15:02,140
I don't know why the writer used the metaphor of Mark Zuckerberg and a mankini, but if you

224
00:15:02,140 --> 00:15:08,820
get past that, you can see that Meta has basically asserted its own definition of open source

225
00:15:08,820 --> 00:15:09,900
AI.

226
00:15:09,900 --> 00:15:13,220
It maps completely to their Lama models.

227
00:15:13,220 --> 00:15:17,060
I've noticed advertisements now about open source AI from Meta.

228
00:15:17,060 --> 00:15:21,940
So they have a marketing campaign that is now also being part of this effort to define

229
00:15:21,940 --> 00:15:24,020
open source AI.

230
00:15:24,020 --> 00:15:28,140
I will note that other big tech companies we've talked to have had a more nuanced approach

231
00:15:28,140 --> 00:15:33,580
going so far as saying, we may not even agree with the definition, but we respect the process

232
00:15:33,580 --> 00:15:36,340
you used to come to the definition.

233
00:15:36,340 --> 00:15:40,220
We would like to engage in how this definition might evolve.

234
00:15:40,220 --> 00:15:44,700
So whatever you may think of the definition, I think one big question is, do you want any

235
00:15:44,700 --> 00:15:51,060
single company, not just Meta, any company defining what open source AI is for everybody,

236
00:15:51,060 --> 00:15:54,580
or do you want an inclusive community process for that to happen?

237
00:15:54,580 --> 00:15:59,380
And I think you know what my answer to that is.

238
00:15:59,380 --> 00:16:05,660
So one thing I want to bring up and show you is what we're calling the OFAI landscape map.

239
00:16:05,660 --> 00:16:10,860
And this is a way of basically showing what's happening within the OFAI in a programmatic

240
00:16:10,860 --> 00:16:11,860
way.

241
00:16:11,860 --> 00:16:19,260
So a visual, concise way of saying, here are the types of activities, projects, organizations,

242
00:16:19,260 --> 00:16:26,060
programs, working groups of OFAI so that people can either find out what's happening or can

243
00:16:26,060 --> 00:16:32,860
say we can contribute, we can identify new partners that way, or in essence, we can compare

244
00:16:32,860 --> 00:16:37,980
OFAI alignment to other efforts like partnerships in AI or the AI Alliance and so on.

245
00:16:37,980 --> 00:16:41,980
That's a question I get often is, how is this different from those other efforts?

246
00:16:41,980 --> 00:16:47,500
So rather than me sending lots of emails or commenting on Blue Sky or whatever, you can

247
00:16:47,500 --> 00:16:49,780
come to this map and take a look at it.

248
00:16:49,780 --> 00:16:56,060
And the way that the map is organized is along one dimension are what our beliefs are about

249
00:16:56,060 --> 00:17:01,860
the current components of an AI model or system, and then the post deployment types of activities

250
00:17:01,860 --> 00:17:05,220
like evaluation, oversight, and so on.

251
00:17:05,220 --> 00:17:10,460
And then the other dimension are the working groups, research, technical prototypes, community

252
00:17:10,460 --> 00:17:13,020
engagement, policy recognitions, and talent for service.

253
00:17:13,020 --> 00:17:18,060
So now we get into the dangerous, I'm going to get on Wi-Fi and see if this works.

254
00:17:18,060 --> 00:17:21,180
So this is the OFAI landscape map.

255
00:17:21,180 --> 00:17:26,300
The vision is the map that the Cloud Native Computing Federation has created.

256
00:17:26,300 --> 00:17:30,820
We're using the same software, so another benefit for open source software.

257
00:17:30,820 --> 00:17:34,660
You can see that in essence, this is a map of all sorts of projects organized along different

258
00:17:34,660 --> 00:17:37,780
dimensions, and that's where we're aiming to go.

259
00:17:37,780 --> 00:17:42,180
So now you can see there are activities and there are projects from each of the partners.

260
00:17:42,180 --> 00:17:46,140
They've been put onto the map according to those two dimensions.

261
00:17:46,140 --> 00:17:51,540
The card view is another way of looking at this, maybe even a more accessible way.

262
00:17:51,540 --> 00:17:56,740
So you have two ways of sifting through the information in any given case.

263
00:17:56,740 --> 00:18:01,020
I will not pick Carnegie Mellon to be as objective as possible.

264
00:18:01,020 --> 00:18:05,320
This is a project that's taking place at Georgia Washington University called Trails.

265
00:18:05,320 --> 00:18:09,980
You can click on the particular link, see a brief description, and then you can of course

266
00:18:09,980 --> 00:18:18,420
go to the website itself for additional information.

267
00:18:18,420 --> 00:18:23,980
The idea being you can come at any time and see this map and see what we're doing.

268
00:18:23,980 --> 00:18:28,820
Probably other organizations will engage with us and come along and you can see what they're

269
00:18:28,820 --> 00:18:29,820
doing.

270
00:18:29,820 --> 00:18:36,900
I guess what I'll say is regardless of what happens January 20th at 12.01, I would like

271
00:18:36,900 --> 00:18:43,500
to believe that having a map of what's happening in the AI ecosystem will be helpful to anyone.

272
00:18:43,500 --> 00:18:47,260
For whatever policy conversations might be happening or partnerships that might happen

273
00:18:47,260 --> 00:18:49,940
and so on.

274
00:18:49,940 --> 00:18:54,860
I will keep reminding people of this map, but I encourage you to check it out yourself

275
00:18:54,860 --> 00:18:57,540
of course.

276
00:18:57,540 --> 00:19:02,100
So the next thing I want to show you is a video presentation from Vince Shaw.

277
00:19:02,100 --> 00:19:06,540
I mentioned he's the head of the Technical Prototypes Group.

278
00:19:06,540 --> 00:19:11,020
He has done some amazing work that we'll show you in this clip and then I'll make a couple

279
00:19:11,020 --> 00:19:13,020
of comments afterwards.

280
00:19:13,020 --> 00:19:14,500
Hello.

281
00:19:14,660 --> 00:19:15,660
Hello.

282
00:19:15,660 --> 00:19:19,860
I'm Vincent Shaw, Associate Dean for IT and Operations for the Detroit College of Management

283
00:19:19,860 --> 00:19:24,020
and Social Sciences and Technical Lead for the Open Forum for AI here at Carnegie Mellon

284
00:19:24,020 --> 00:19:25,020
University.

285
00:19:25,020 --> 00:19:28,980
Today I'll be sharing with you a little bit about our adoption strategy and one of our

286
00:19:28,980 --> 00:19:32,460
LLM powered prototypes.

287
00:19:32,460 --> 00:19:37,140
Our journey kind of started in April 2023 where I co-led a committee commissioned by

288
00:19:37,140 --> 00:19:42,060
our Dean Richard Scheines with faculty across our college to examine the disruption of what

289
00:19:42,060 --> 00:19:48,500
chat GPD meant for us as a higher education institution and all of these new LLM tools.

290
00:19:48,500 --> 00:19:52,780
We met weekly performing a SWOT analysis, strength, weakness, opportunities and threats

291
00:19:52,780 --> 00:19:58,180
for those of you not familiar with the technical term about how we should be kind of what this

292
00:19:58,180 --> 00:20:01,380
means for each discipline and how we should be responding to it.

293
00:20:01,380 --> 00:20:06,060
At the end of that we published best practices guide for our faculty for teaching and for

294
00:20:06,060 --> 00:20:10,620
in policies, but we also formulated a strategy of adoption and innovation which kind of led

295
00:20:10,620 --> 00:20:16,460
to this collaboration with Saeed and I here in the founding to open for AI.

296
00:20:16,460 --> 00:20:22,500
Our primary focus for our technical prototypes is reducing technical logistical barriers

297
00:20:22,500 --> 00:20:23,500
to access.

298
00:20:23,500 --> 00:20:28,700
We feel that as people gain a deeper appreciation for these tools, instructors and students

299
00:20:28,700 --> 00:20:34,060
will better be able to scaffold the uses of these tools to use them more responsibly,

300
00:20:34,060 --> 00:20:39,500
more productively in research and education also in their professional lives as they graduate.

301
00:20:39,500 --> 00:20:45,220
The result of our work has been the creation of this platform that we call DER, Dietrich

302
00:20:45,220 --> 00:20:52,820
Analysis Research Education Suite, which is a productivity suite set of tools that has

303
00:20:52,820 --> 00:20:57,860
things like basic chat, document summarization, retrieve augmented generation capabilities

304
00:20:57,860 --> 00:20:59,700
on a campus controlled environment.

305
00:20:59,700 --> 00:21:04,180
It's a sandbox designed to allow users to create and store custom system prompt, give

306
00:21:04,180 --> 00:21:08,540
them access to all the different major large language models for the API, with there's

307
00:21:08,540 --> 00:21:12,420
cost controls and budgets and they can leverage data from any source.

308
00:21:12,420 --> 00:21:16,580
And more importantly, they can also conduct research in the platform.

309
00:21:16,580 --> 00:21:20,920
And what we hope is that as we have greater adoption, our faculty will come up with their

310
00:21:20,920 --> 00:21:23,100
own great ideas on these types of tools we can create.

311
00:21:23,100 --> 00:21:26,380
And one of the great ideas that came out of the collaboration and the committee that I

312
00:21:26,380 --> 00:21:32,140
led was Socratic Books, which was proposed by my colleague, Daniel Oppenheimer, who is

313
00:21:32,140 --> 00:21:36,860
a psychologist in the social decision sciences department in psychology.

314
00:21:36,860 --> 00:21:41,460
What Socratic Books is, is I like to consider our entry in the pantheon of trying to solve

315
00:21:41,460 --> 00:21:42,980
Bloom's two sigma problem.

316
00:21:42,980 --> 00:21:47,500
Benjamin Bloom published a paper in 1984, which showed that one-on-one mentoring and

317
00:21:47,500 --> 00:21:52,020
tutoring was superior than any other teaching intervention.

318
00:21:52,020 --> 00:21:56,580
Because it can meet, you know, when you get, we all know who have had great mentors is

319
00:21:56,580 --> 00:22:00,980
that when somebody can adapt to you, meet you where you are, you learn faster, you learn

320
00:22:00,980 --> 00:22:01,980
better.

321
00:22:01,980 --> 00:22:06,340
And, you know, textbooks often are, they don't do that.

322
00:22:06,340 --> 00:22:07,740
They're just one size fits all.

323
00:22:07,740 --> 00:22:11,180
They could be designed for many different locations.

324
00:22:11,180 --> 00:22:14,100
You might be from, excuse me, you might be from a location that you don't understand

325
00:22:14,100 --> 00:22:15,980
analogies or things like that.

326
00:22:15,980 --> 00:22:18,580
So we created Socratic Books.

327
00:22:18,580 --> 00:22:21,260
So let's go ahead and hop right in.

328
00:22:21,260 --> 00:22:25,740
So first I want to give a quick layout of how Socratic Books is set up and why an instructor

329
00:22:25,740 --> 00:22:26,980
might want to use it.

330
00:22:26,980 --> 00:22:30,940
What it does is it gives you the ability to really extend your pedagogy into what you

331
00:22:30,940 --> 00:22:33,900
feel is important about teaching lessons and concepts.

332
00:22:33,900 --> 00:22:37,780
So one of the first things an instructor will do is go ahead and upload their own files

333
00:22:37,780 --> 00:22:43,000
or papers or research papers or could be chapters of things and to ground the knowledge.

334
00:22:43,000 --> 00:22:47,580
So Socratic Books is essentially a multi-agent rag tool, right?

335
00:22:47,580 --> 00:22:49,940
On the left here, so you can see the files are here.

336
00:22:49,940 --> 00:22:52,860
You can assign chapters of books, students can join them.

337
00:22:52,860 --> 00:22:56,580
And you can have a choice between using GPT or Quad.

338
00:22:56,580 --> 00:22:58,640
You set the context of lesson here.

339
00:22:58,640 --> 00:22:59,640
You can set the learning goals.

340
00:22:59,640 --> 00:23:02,940
You can get as nuanced and granular as you want.

341
00:23:02,940 --> 00:23:07,580
This particular one is about GPT's mixture of expert frameworks and the tree of thought

342
00:23:07,580 --> 00:23:08,980
system prompting.

343
00:23:08,980 --> 00:23:13,540
It was created as a prototype, one of our colleagues who teaches about large language

344
00:23:13,540 --> 00:23:14,940
models in high school.

345
00:23:14,940 --> 00:23:19,740
Below the learning goals, the next three windows concatenate into one large prompt about how

346
00:23:19,740 --> 00:23:24,220
you want the teaching to occur and how you want the Socratic tutor to interact with the

347
00:23:24,220 --> 00:23:27,820
student, whether that's how you're going to engage a question and answer, how you're going

348
00:23:27,820 --> 00:23:31,540
to guide them along in the lesson, some restrictions and formatting.

349
00:23:31,540 --> 00:23:35,420
And the bottom is a second bot that works in tandem with the first one and helps to

350
00:23:35,420 --> 00:23:38,900
track how the student's progress is going, helps track mastery if the student wants to

351
00:23:38,900 --> 00:23:40,460
take mastery tests.

352
00:23:40,460 --> 00:23:44,260
And those are all voluntary because we want to encourage a learning environment, not necessarily

353
00:23:44,260 --> 00:23:45,400
assessment here.

354
00:23:45,400 --> 00:23:48,900
We have plans for a different assessment tool, but that's a different project.

355
00:23:48,900 --> 00:23:51,900
Let's jump right into the tool.

356
00:23:51,900 --> 00:23:53,860
And we'll go through this really quickly.

357
00:23:53,860 --> 00:23:57,540
On the left side, you see where the tracking happens, that the learning goals and the progress

358
00:23:57,540 --> 00:23:58,540
tracking will happen.

359
00:23:58,540 --> 00:24:00,700
The center is the main window where your interaction happens.

360
00:24:00,700 --> 00:24:04,180
On the right is where a student can take notes and you can save them to your account.

361
00:24:04,180 --> 00:24:06,140
They can log back in, they can look at their old notes.

362
00:24:06,140 --> 00:24:09,740
We'll be also creating buttons and prompts so they can create their own study guides

363
00:24:09,740 --> 00:24:13,420
from them, audio files for download, some of what you've seen if you're familiar with

364
00:24:13,420 --> 00:24:15,140
Google Notebook LLM.

365
00:24:15,140 --> 00:24:18,420
Unfortunately, we had to plan earlier, but we couldn't release in time.

366
00:24:18,420 --> 00:24:21,940
If anyone has a few billion dollars laying around, we'd be happy to have it and we'll

367
00:24:21,940 --> 00:24:23,300
get that released shortly.

368
00:24:23,300 --> 00:24:27,020
So let's have a quick interaction so we can see how it works.

369
00:24:27,020 --> 00:24:30,180
We just say hello and the system immediately comes online.

370
00:24:30,180 --> 00:24:32,420
We'll start telling us what the lesson is.

371
00:24:32,420 --> 00:24:33,900
Let's just jump right in.

372
00:24:33,900 --> 00:24:36,020
Let's do LG1.

373
00:24:36,020 --> 00:24:40,380
And as you can see, before I hit that, the progress tracker immediately starts to track

374
00:24:40,380 --> 00:24:44,700
and it looks at the conversation after each iteration to see what's been done, what's

375
00:24:44,700 --> 00:24:47,380
been updated, how the student's progressing.

376
00:24:47,380 --> 00:24:48,860
And it tells you what you need to be doing.

377
00:24:48,860 --> 00:24:53,060
So let's do LG1.

378
00:24:53,060 --> 00:24:58,620
And immediately it's going to start teaching a lesson about the mixture of experts framework,

379
00:24:58,620 --> 00:25:02,500
which is a quite technical framework for people who maybe don't have a deep background in

380
00:25:02,500 --> 00:25:03,500
this type of work.

381
00:25:03,500 --> 00:25:07,540
You know, I happen to like Iron Man, so I'm going to ask it if it can relate that to Iron

382
00:25:07,540 --> 00:25:08,540
Man.

383
00:25:08,540 --> 00:25:20,820
Can you relate this information?

384
00:25:20,820 --> 00:25:25,100
Let's see how that system pivots.

385
00:25:25,100 --> 00:25:30,260
So immediately what you see is a system meeting me for where I'm at as a learner, right?

386
00:25:30,260 --> 00:25:34,820
I'm asking it to, you know, I just, I grew up, I love comic books and Tony Stark and

387
00:25:34,820 --> 00:25:38,780
immediately kind of pivots the lesson around something I can ground myself in and it wraps

388
00:25:38,780 --> 00:25:44,820
a lesson around that, which is we think will really enhance one, you know, when people

389
00:25:44,820 --> 00:25:49,020
are attached to things that make them more excited, when people are attached to information

390
00:25:49,020 --> 00:25:51,900
in their brains that are salient, that have really helped with the learning process and

391
00:25:51,900 --> 00:25:53,180
make the cover.

392
00:25:53,180 --> 00:25:57,060
And it has the ability to do that in whatever kind of pop culture or anything is within

393
00:25:57,060 --> 00:25:58,820
the training data, right?

394
00:25:58,820 --> 00:26:03,540
And fortunately, because of the pretty good representation, different languages that actually

395
00:26:03,540 --> 00:26:08,340
pivot to most of the major languages that are represented in large language models.

396
00:26:08,340 --> 00:26:11,740
And since we're short on time for this demo, I just want to move through really quickly

397
00:26:11,740 --> 00:26:13,980
and also show you how the mastery work.

398
00:26:13,980 --> 00:26:20,860
You did mastery as this learning model.

399
00:26:20,860 --> 00:26:26,700
And the way that that works is the agent knows that it should only be taking in my input.

400
00:26:26,700 --> 00:26:30,660
So you know, I can't just, it doesn't read that the fact that the answer has already

401
00:26:30,660 --> 00:26:34,660
been set in the context, what it does is it goes ahead and looks through the conversation

402
00:26:34,660 --> 00:26:36,900
to see of my responses, right?

403
00:26:36,900 --> 00:26:38,660
So I'm just going to go ahead and answer one of them.

404
00:26:38,660 --> 00:26:39,660
Let's see here.

405
00:26:39,660 --> 00:26:47,660
MQ one, Jarvis works as eight.

406
00:26:47,660 --> 00:26:56,620
And you can see, go ahead and we'll go ahead and it gives me, it's going to give me credit

407
00:26:56,620 --> 00:26:59,300
here in a moment here on the left for mastery.

408
00:26:59,300 --> 00:27:03,020
But what I really like seeing is that, you know, it was just a short answer.

409
00:27:03,020 --> 00:27:04,780
It wasn't nuanced enough, right?

410
00:27:04,780 --> 00:27:09,300
So it gave me partial credit because we right now in this version, we have five questions

411
00:27:09,300 --> 00:27:13,380
as mastery, instructor can set that to as many as they would like.

412
00:27:13,380 --> 00:27:17,140
And it really kind of pushes me to have a deeper understanding of that.

413
00:27:17,140 --> 00:27:18,140
Right.

414
00:27:18,140 --> 00:27:23,580
And let's go ahead and actually just look at another question here and answer incorrectly.

415
00:27:23,580 --> 00:27:35,940
I need help.

416
00:27:35,940 --> 00:27:39,260
And it will go ahead and just kind of meet me from where I'm at.

417
00:27:39,260 --> 00:27:43,380
And basically what happened is as you work through one lesson, the system will kind of

418
00:27:43,380 --> 00:27:46,900
keep monitoring as you get towards the end of a lesson, it will go ahead and nudge you

419
00:27:46,900 --> 00:27:47,900
to the next one.

420
00:27:47,900 --> 00:27:53,020
I don't want to spend too much time here teaching everybody about these two concepts, but I

421
00:27:53,020 --> 00:27:58,900
hope that you found this demonstration interesting and that have a wonderful day.

422
00:27:58,900 --> 00:28:02,260
So just a couple of points about these technical prototypes.

423
00:28:02,260 --> 00:28:05,740
So you could see Vince is using right now, Cloud or chat GPT four.

424
00:28:05,740 --> 00:28:09,700
He's also testing it with some of the models, which I would consider open source AI.

425
00:28:09,700 --> 00:28:14,500
And he's saying once the performance gets to that level of shift over all the work that

426
00:28:14,500 --> 00:28:19,820
he is doing, all the work that any technical prototype for that will come out of OFAI will

427
00:28:19,820 --> 00:28:20,940
be open source AI.

428
00:28:20,940 --> 00:28:24,540
So we're committed to making that decision.

429
00:28:24,540 --> 00:28:29,860
And I will note that one of the fellows for OFAI works at something called BGV Capital,

430
00:28:29,860 --> 00:28:31,340
a venture finance group.

431
00:28:31,340 --> 00:28:32,900
And I'm in active conversation with him.

432
00:28:32,900 --> 00:28:37,540
He's a managing partner about funding the open source AI prototypes that come out of

433
00:28:37,540 --> 00:28:42,580
OFAI and more generally, and they're very much interested in supporting responsible

434
00:28:42,580 --> 00:28:43,820
human centered AI.

435
00:28:43,820 --> 00:28:50,720
So it's important to try and get flows of money just to be blunt into this kind of open

436
00:28:50,720 --> 00:28:53,380
source AI and this type of approach.

437
00:28:53,380 --> 00:28:56,840
So I am one minute from the end.

438
00:28:56,840 --> 00:29:00,020
So I hope we can have at least one or two questions or discussion.

439
00:29:00,020 --> 00:29:01,020
These are some resources.

440
00:29:01,020 --> 00:29:08,420
I've placed them in the SCAD or SHED if you're from the UK website.

441
00:29:08,420 --> 00:29:11,060
And so you can have them there, but I'm sure the slides will be available at some point.

442
00:29:11,060 --> 00:29:14,620
I just want to acknowledge our funders again and open it up for any kinds of comments,

443
00:29:14,620 --> 00:29:16,220
questions, feedback you might have.

444
00:29:16,220 --> 00:29:24,300
Thank you.

445
00:29:24,300 --> 00:29:25,300
Interesting project.

446
00:29:25,300 --> 00:29:28,580
How do you think about sustaining this long term?

447
00:29:28,580 --> 00:29:34,420
I mean, there's obviously big money to build any of these models, but they take a lot to

448
00:29:34,420 --> 00:29:35,420
run as well.

449
00:29:35,420 --> 00:29:36,420
They do.

450
00:29:36,420 --> 00:29:37,420
You're right.

451
00:29:37,420 --> 00:29:39,760
So there's a couple of things on the sustainability dimension.

452
00:29:39,760 --> 00:29:44,580
One is to align it with work that's already happening and try to nudge that work toward

453
00:29:44,580 --> 00:29:48,380
the goals and the values and the approach we're taking for OFAI.

454
00:29:48,380 --> 00:29:52,140
So we've identified technical prototypes at UT Austin.

455
00:29:52,140 --> 00:29:56,100
That work is happening independent of whether we fund it or not.

456
00:29:56,100 --> 00:30:00,160
There's also the sustainability of OFAI as a concept.

457
00:30:00,160 --> 00:30:05,340
We have funding through July of 2025 from the initial funders, but I've heard from our

458
00:30:05,340 --> 00:30:10,420
current funders and other funders and from Carnegie Mellon itself that this is something

459
00:30:10,420 --> 00:30:12,120
we wish to sustain.

460
00:30:12,120 --> 00:30:16,540
As long as we can keep producing outputs, whether those are, you know, we have a position

461
00:30:16,540 --> 00:30:20,220
paper that I wish would have been done by today, but it isn't.

462
00:30:20,220 --> 00:30:25,940
But as soon as I'm happy to share with everybody the map, the prototypes and so on, that there's

463
00:30:25,940 --> 00:30:28,060
value in doing this work.

464
00:30:28,060 --> 00:30:31,420
And quite frankly, there's urgency in doing this work.

465
00:30:31,420 --> 00:30:35,180
And if people are interested and engaged in that, whether that's through financial support

466
00:30:35,180 --> 00:30:41,380
or in-kind contributions or work that you're already doing, we certainly welcome that.

467
00:30:41,380 --> 00:30:45,140
Just building off that question, what do you see as the like some of our options for long-term

468
00:30:45,140 --> 00:30:48,820
sustainability in the sense of like treating it like a utility or those sorts of things?

469
00:30:48,820 --> 00:30:53,380
How are you thinking about, you know, as we're bringing these different groups together,

470
00:30:53,380 --> 00:30:57,300
what do we start to seed in people's minds about how we think about the real long-term

471
00:30:57,300 --> 00:30:59,340
preservation and sustainability?

472
00:30:59,340 --> 00:31:02,940
And I'm Casey Wright at the Association of Public and Land-Grant Universities.

473
00:31:02,940 --> 00:31:04,720
Right.

474
00:31:04,720 --> 00:31:07,820
So this is just my own opinion.

475
00:31:07,820 --> 00:31:15,140
I think the large AI foundational model deployers have gotten themselves into a bind.

476
00:31:15,140 --> 00:31:17,880
I think they received enormous amounts of money.

477
00:31:17,880 --> 00:31:21,260
They're not getting the return on investment that they thought they would get.

478
00:31:21,260 --> 00:31:24,500
But what they have essentially done is built infrastructure for the rest of us to build

479
00:31:24,500 --> 00:31:26,060
on top of.

480
00:31:26,060 --> 00:31:29,660
I have serious concerns about infrastructure being opaque.

481
00:31:29,660 --> 00:31:32,060
So that just to be very clear about that.

482
00:31:32,060 --> 00:31:36,380
But all the work that Vince has done and many, many other people are doing are building on

483
00:31:36,380 --> 00:31:38,420
that infrastructure.

484
00:31:38,420 --> 00:31:45,460
And if you have an open source AI system and something goes wrong and you can evaluate

485
00:31:45,460 --> 00:31:50,260
it rigorously because it's open, then there must be something at that infrastructure layer

486
00:31:50,260 --> 00:31:52,180
that's causing the issues.

487
00:31:52,180 --> 00:31:56,100
And I think that's the way we open up those kinds of infrastructure layers.

488
00:31:56,100 --> 00:32:00,780
I don't think they wanted to create infrastructure for all of us to build a whole set of new

489
00:32:00,780 --> 00:32:01,780
models.

490
00:32:01,780 --> 00:32:04,660
They sort of said, look, we're very supportive and inclusive and participatory.

491
00:32:04,660 --> 00:32:06,100
You can do whatever you want.

492
00:32:06,100 --> 00:32:08,520
People have gone to town on that.

493
00:32:08,520 --> 00:32:14,600
So I think we just need to keep building where we can in open ways and in essence basically

494
00:32:14,600 --> 00:32:18,820
connect to their infrastructure and say, you need to open this up.

495
00:32:18,820 --> 00:32:24,340
If somebody says this is not safe and this is having a reproducibility issue or so on.

496
00:32:24,340 --> 00:32:27,820
So that's a technical type of response to that.

497
00:32:27,820 --> 00:32:32,900
In terms of the preservation of AI systems, I don't have an answer for that.

498
00:32:32,900 --> 00:32:34,500
These things are so dynamic.

499
00:32:34,500 --> 00:32:35,620
They're so statistical.

500
00:32:35,620 --> 00:32:37,340
They're so complex.

501
00:32:37,340 --> 00:32:39,180
You start with a set of training data.

502
00:32:39,180 --> 00:32:43,020
You iterate that data over each of the training runs and so on and so on.

503
00:32:43,020 --> 00:32:46,340
These are very complex issues that I think we need to address, but I don't think we have

504
00:32:46,340 --> 00:32:51,020
a good handle on them right now.

505
00:32:51,020 --> 00:32:53,260
So it is 918, let's call it.

506
00:32:53,260 --> 00:32:54,260
Thank you for your attention.

507
00:32:54,260 --> 00:32:54,260
[APPLAUSE]

508
00:32:54,460 --> 00:32:58,460
[APPLAUSE]

