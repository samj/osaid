### Start of next town hall held on 2024-01-12 ###
--- Presentation for 2024-01-12 ---
OPEN SOURCE AI DEFINITION
Online public townhall
Jan 12, 2024

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

The objective for 2024

Open Source AI Deﬁnition
version 1.0

Deﬁnition of AI system

Preamble
Out of scope issues
4 freedoms
License checklist

How Open Source came to be
1: Legal framework

2: Principles

3: Licenses

Copyright applied to
software, ﬁrst.

The GNU Manifesto
lays the ground to
oppose privatization.

Copyleft is a hack on
copyright.

This new artifact
became privatized
work.
Researchers
complained.

A community forms
around these
principles.

Incorporating the
principles, serving as
the Constitution of a
forming community.

5

Golden Rule applied to AI
If I like an AI system I must be free to share it with other people.

What we’ve learned so far
● We need to deﬁne Open Source AI, in general, not just
machine learning
● OECD’s deﬁnition of AI is well accepted (caveat: decisions)
“An AI system is a machine-based system that, for explicit or
implicit objectives, infers, from the input it receives, how to
generate outputs such as predictions, content,
recommendations, or decisions that can inﬂuence physical or
virtual environments. Different AI systems vary in their levels of
autonomy and adaptiveness after deployment.” (2023)

Matching expectations

AI deserves to enjoy the
beneﬁts of Open Source
● autonomy
● transparency
● collaborative
improvement
● ensuring the agency of
the user

Policy makers, academia and
industry are focusing on
● transparency
● trustworthiness
● reliability
● transparency
● explainability
● fairness
● safety etc

What basic freedoms do we need?
What is the preferred form to make modiﬁcations to an AI system?

What is Open Source AI
To be Open Source, an AI system needs to be available under
legal terms that grant the freedoms to:
● Use the system for any purpose and without having to ask
for permission.
● Study how the system works and inspect its components.
● Modify the system to change its recommendations,
predictions or decisions to adapt to your needs.
● Share the system with or without modiﬁcations, for any
purpose.

What is the preferred form to make
modiﬁcations to an AI system?

Getting the speciﬁcations
AI systems

List of
components

Legal
frameworks

Legal
documents

Checklist

What elements are
necessary to:
- use
- study
- modify
- share
an AI system?

What elements are
necessary to:
- use
- study
- modify
- share
an AI system?

For each artifact,
evaluate which
laws apply. Some
will be under
“Intellectual
Property” regimes,
some will be under
other regimes.

We’ll match the
components and
the identiﬁed legal
frameworks with
the terms of the
legal documents
already in use,
where available.

After repeating
this exercise
enough times,
we’ll be able to
generalize the
outcomes and
write the specs to
evaluate the
freedoms granted.

Small working groups to analyze systems
For each in:
- Pythia
- Llama2
- BLOOM
- Mistral
- Phi2
- …

● What do you need to give an input and
get an output? (use)
● What do you need to give an input and
get a different output? (modify)
● What do you need to understand why
given an input, you get that output?
(study)
● What do you need to let others give an
input and get an output? (share)
What’s the preferred form to make
modiﬁcations to an AI system?

Example: Pythia – INCOMPLETE! DON’T QUOTE!
Freedom to use:
- What do you need to
give an input and get an
output from Pythia?

❓

Example: Pythia – INCOMPLETE! DON’T QUOTE!
Freedom to study:
- What do you need to
understand why Pythia,
given an input, gives one
output?

❓

Example: Pythia – INCOMPLETE! DON’T QUOTE!
Freedom to modify:
- What do you need to
give an input and get a
different output from
Pythia?

❓

Example: Pythia – INCOMPLETE! DON’T QUOTE!
Freedom to share:
- What do you need to let
others give an input and
get an output from
Pythia or a version you
modiﬁed?

❓

Then the rest
-

get the legal framework for each component
get the legal documents
analyze the documents
write up a summary

Repeat for at least 4-5 AI systems, ideally not just LLMs
and “Generative AI”

2024 timeline

System testing work stream
Stakeholder consultation work stream
Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

Draft 0.0.8

Townhall +

… October
Monthly Virtual
Meetings

Release
version 1.0

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

v. 1.0

Criteria for RC1 and v. 1.0
RC1

version 1

- Expected outcome of
in-person meeting end
May/early June!
- The draft is completed
in all its parts
- The draft is supported
by at least 2
representatives for
each of the 6
stakeholder groups

- Expected outcome of
in-person and online
meetings through the
summer/early autumn
- The draft is endorsed by at
least 5 reps for each of the
stakeholder groups
- Announced in late October

Help us ﬁnd stakeholders
System Creator

License Creator

Regulator

Licensee

End User

Subject

Makes AI system
and/or
component that
will be studied,
used, modified,
or shared
through an open
source license
(e.g., ML
researcher in
academia or
industry)

Writes or edits
the open source
license to be
applied to the AI
system or
component;
includes
compliance
(e.g., IP lawyer)

Writes or edits
rules governing
licenses and
systems (e.g.
government
policy-maker)

Seeks to study,
use modify, or
share an open
source AI
system (e.g. AI
engineer, health
researcher,
education
researcher)

Consumes a
system output,
but does not
seek to study,
use, modify, or
share the
system (e.g.,
student using a
chatbot to write
a report, artist
creating an
image)

Affected
upstream or
downstream by
a system output
without
interacting with it
intentionally;
includes
advocates for
this group (e.g.
people with loan
denied, or
content creators)

✅

✅

⚠

✅

⚠

⚠

Enough to start

Enough to start

Leads to US, EU,
Singapore, no
commitment yet

Enough to start

Which org is squarely in
this space?

ACLU, Algorithmic
Justice League

It doesn’t end with v. 1.0
We’ll need to deﬁne rules for maintenance and review of the
Deﬁnition

OSI’s immediate next steps
- more publicity to the process
- public discussion forum
- bi-weekly townhalls
- more opportunities to volunteer
- update project landing page
- reach out to more stakeholders
- raise funds for 2024 meetings
- setup the board for review and approval of v. 1.0

Draft v. 0.0.4 of the Open Source AI Deﬁnition
Open to public comments

https://opensource.org/deepdive/drafts

24

❓
Become a member of OSI
https://members.opensource.org/join
Support more workshops in 2024
@ed@opensource.org
stefano@opensource.org
25

The Open Source Deﬁnition

Three building blocks of AI

Hardware

Knowledge

Data

Money

Money

Money

Time

Time
Gazillions of laws

A typical $FOO is made of
Software for
training and
testing, inference
and analysis

Model architecture
and its weights and
training
parameters

Raw data and
prepared datasets,
for training and
testing

All that is written by
a human and
copyrightable must
be Open Source.

No restrictions on:

It’s not the preferred
form for making
modiﬁcations to
model/weights.

●

Who can use them

●

How they’re used

●

Retraining

●

Redistribution

Does that mean the
Deﬁnition can ignore
the original data?
28

A typical $FOO is made of
Software for
training and
testing, inference
and analysis

Model architecture
and its weights and
training
parameters

Raw data and
prepared datasets,
for training and
testing

All that is written by
a human and
copyrightable must
be Open Source.

No restrictions on:

It’s not the preferred
form for making
modiﬁcations to
model/weights.

●

Who can use them

●

How they’re used

●

Retraining

●

●

Redistribution
Unless
it kills you

?

Does that mean the
Deﬁnition can ignore
the original data?
29

“Thou shall not use
[my code | my art]
in your dataset.”
Golden Rule violated!
If I like an AI system I must share it with other people who like it

EFF: How We Think About Copyright and AI Art
https://www.eff.org/deeplinks/2023/04/how-we-think-about-c
opyright-and-ai-art-0

Accumulating data is already highly regulated

Privacy
Anti-discrimination
Accessibility
Disability protection
National security
Human rights
FTC Lina Kahn on regulating AI
https://www.nytimes.com/2023/05/03/opinion/ai-lina-khan-ftc-technology.html

Fun with names

AI, Artiﬁcial Intelligence
● It’s a science with 70 years of history
● Ignore the hype
● It has no “intelligence”
Machine Learning, drop AI?
Open Source AI? No!
What then?
32

What are Open Source AI Components
For the purposes of today’s work, we are deﬁning the
components* of an AI system broadly as:
● Code Instruction for a computer to complete a task.
● Model Abstracted representation of what an AI system
has learned from the training data.
● Data Information converted into a form efficient for
processing and transfer.

*sources: Digital Public Goods Alliance, Bunch, Appen, ComputerScience.org

What Makes an AI System Open Source

How should the freedoms apply to each component for the AI system to be licensed as
open? Study
Use
Modify
Share
Code
For someone to study how the AI
system's code works and inspect it
requires that the license for that
system....

For someone to use the AI system's
code for any purpose and without
having to ask for permission
requires that the license for that
system....

For someone to modify the code to
change its recommendations,
predictions or decisions to adapt to
their needs requires that the license
for the AI system...

For someone to share the code with
or without modiﬁcations, for any
purpose requires that the license for
the AI system...

Model
For someone to study how the AI
system's model works and inspect it
requires that the license for that
system....

For someone to use the AI system's
model for any purpose and without
having to ask for permission
requires that the license for that
system....

For someone to modify the model
to change its recommendations,
predictions or decisions to adapt to
their needs requires that the license
for the AI system....

For someone to share the model
with or without modiﬁcations, for
any purpose requires that the
license for the AI system....

Data
For someone to study how the AI
system's data works and inspect it
requires that the license for that
system....

For someone to use the AI system's
data for any purpose and without
having to ask for permission
requires that the license for that
system...

For someone to modify the data to
change its recommendations,
predictions or decisions to adapt to
their needs requires that the license
for the AI system....

For someone to share the data with
or without modiﬁcations, for any
purpose requires that the license for
the AI system....


--- Subtitles for 2024-01-12 --- ###
1
00:00:00,001 --> 00:00:04,740
All right, here I am.

2
00:00:04,740 --> 00:00:11,340
Let's start the recording and welcome everyone.

3
00:00:11,340 --> 00:00:23,340
Maybe we can wait a couple of minutes and give a little bit of time for others to join.

4
00:00:23,340 --> 00:00:31,340
Okay.

5
00:00:31,340 --> 00:00:41,340
Okay.

6
00:00:51,340 --> 00:01:06,340
All right.

7
00:01:06,340 --> 00:01:16,340
All right.

8
00:01:16,340 --> 00:01:24,620
Let's get it started so we can use most of the time and we can also collect feedback.

9
00:01:24,620 --> 00:01:32,020
So I'll try to get enough time for people to ask me questions and I'm here available.

10
00:01:32,020 --> 00:01:35,940
Let's get started with some very quick community agreements.

11
00:01:35,940 --> 00:01:41,900
Let's make sure that we have give time for people to speak and if you're the kind of

12
00:01:41,900 --> 00:01:50,020
person who usually stays quiet, speak up, feel free to really grab time and write down.

13
00:01:50,020 --> 00:01:56,180
Use the hand button if you want to get attention or write down your comment, but please, we

14
00:01:56,180 --> 00:02:01,300
want to be able to, we want to hear from you.

15
00:02:01,300 --> 00:02:08,860
Be nice and don't have to remind people that this is a safe space and we don't have to

16
00:02:08,860 --> 00:02:14,660
we have to be gentle and we need to keep on moving.

17
00:02:14,660 --> 00:02:19,780
Also if we face obstacles, we move around and we will get back to it and we want to

18
00:02:19,780 --> 00:02:22,540
focus on solutions.

19
00:02:22,540 --> 00:02:31,700
This is a process that is complicated and it's also quite pioneer if you want.

20
00:02:31,700 --> 00:02:38,060
We don't have that history outside the organization of running this co-design process with multiple

21
00:02:38,060 --> 00:02:43,540
stakeholders across the world and there are a lot of things that we know don't work and

22
00:02:43,540 --> 00:02:53,100
can be done better, but we also need to keep on working because time is on us.

23
00:02:53,100 --> 00:03:02,340
Is there anything else that you think we need to cover in this agreement?

24
00:03:02,340 --> 00:03:09,060
All right.

25
00:03:09,060 --> 00:03:11,380
Okay.

26
00:03:11,380 --> 00:03:19,980
So give a recap to the people who have just started following this work.

27
00:03:19,980 --> 00:03:27,500
We started a discussion to define the definition of open source AI and this is a definition

28
00:03:27,500 --> 00:03:34,600
that is coming from a wide conversation with stakeholders from different sides of society

29
00:03:34,600 --> 00:03:42,700
for a very wide different groups and the objective that we have is to talk to multiple experts

30
00:03:42,700 --> 00:03:46,660
in various fields and disciplines around the world.

31
00:03:46,660 --> 00:03:53,660
We will not be able to have some genius coming out of the basement with a definition.

32
00:03:53,660 --> 00:03:57,420
It's unlikely for this to happen, so this needs to be a global conversation and we're

33
00:03:57,420 --> 00:03:58,580
helping.

34
00:03:58,580 --> 00:04:07,220
We hear as the open source initiative as facilitator, convener of conversations for an open source

35
00:04:07,220 --> 00:04:13,940
AI definition to come out of consensus from different stakeholders and the document that

36
00:04:13,940 --> 00:04:20,740
we're trying to draft looks is made of three parts.

37
00:04:20,740 --> 00:04:25,700
My computer is getting really slow.

38
00:04:25,700 --> 00:04:26,700
Okay.

39
00:04:26,700 --> 00:04:29,340
It's made of three parts.

40
00:04:29,340 --> 00:04:36,540
There is a definition of AI system and a preamble at the top and the preamble contains the basic

41
00:04:36,540 --> 00:04:43,180
principles of why we need open source AI.

42
00:04:43,180 --> 00:04:49,260
And there is a section also with issues that are out of scope to clarify what's not covered

43
00:04:49,260 --> 00:04:52,900
in this AI definition.

44
00:04:52,900 --> 00:04:59,020
Then we have the shortest possible answer to the question, what is open source AI?

45
00:04:59,020 --> 00:05:05,620
And they look a lot like the four freedoms for software that many of us have been accustomed

46
00:05:05,620 --> 00:05:07,260
to see.

47
00:05:07,260 --> 00:05:13,220
And then the rest of it is a checklist to evaluate legal documents that are used to

48
00:05:13,220 --> 00:05:18,180
grant the four freedoms above to the AI system.

49
00:05:18,180 --> 00:05:23,980
And we got to the point where we had plenty of conversation in the second half of last

50
00:05:23,980 --> 00:05:34,060
year, 2023, with a variety of people to have the bones, the bare bones of the definition

51
00:05:34,060 --> 00:05:38,380
of AI systems, the preamble, the out of scope and the four freedoms.

52
00:05:38,380 --> 00:05:44,180
And we're missing this checklist of legal documents at the end.

53
00:05:44,180 --> 00:05:50,980
Yes, I will explain the checklist in more details.

54
00:05:50,980 --> 00:05:58,860
So probably it's worth having a very quick overview of how we are proceeding.

55
00:05:58,860 --> 00:06:06,100
We're basically retracing the history of open source, compressing those 25, 30 years of

56
00:06:06,100 --> 00:06:11,140
history in a few months so that we can get to the open source AI definition.

57
00:06:11,140 --> 00:06:15,180
We're tracing the steps following this sequence.

58
00:06:15,180 --> 00:06:23,980
We're going from a software when it came out, there was a legal framework that was applied

59
00:06:23,980 --> 00:06:25,420
to it.

60
00:06:25,420 --> 00:06:33,380
And a community established itself around the principles of the GNU manifesto.

61
00:06:33,380 --> 00:06:39,580
And it started writing new software, the GNU operating system and sharing that was shared

62
00:06:39,580 --> 00:06:45,940
with legal agreements, legal documents that were granting rights rather than removing

63
00:06:45,940 --> 00:06:46,940
them.

64
00:06:46,940 --> 00:06:53,380
So that's the sequence that we're trying to re-trace.

65
00:06:53,380 --> 00:07:00,180
From understanding the new artifacts, these new AI systems, how they're working and which

66
00:07:00,180 --> 00:07:08,020
legal frameworks apply to them, what are the principles that we want to have applied for

67
00:07:08,020 --> 00:07:14,900
granting freedoms, and then we're going to look at legal documents to grant the rights

68
00:07:14,900 --> 00:07:18,380
rather than remove them.

69
00:07:18,380 --> 00:07:25,060
There is one interesting principle that is written inside the GNU manifesto, and that's

70
00:07:25,060 --> 00:07:33,740
the golden rule that is, it's written, we can reuse it easily to apply to the AI system.

71
00:07:33,740 --> 00:07:38,860
So if I like an AI system, I must be free to share it with other people.

72
00:07:38,860 --> 00:07:45,500
That's the basic principle that we want to embed that we're looking for now.

73
00:07:45,500 --> 00:07:53,700
And so far, what we've learned is that we really need to define open source AI in general,

74
00:07:53,700 --> 00:07:59,200
not just focus on machine learning or whatever is new and exciting in this generative AI

75
00:07:59,200 --> 00:08:03,220
space of the past year and a half.

76
00:08:03,220 --> 00:08:10,020
So the other thing that we have learned is we need a definition of AI system.

77
00:08:10,020 --> 00:08:19,060
And we found that the one provided by the Organization for Economic Development is quite

78
00:08:19,060 --> 00:08:20,060
well accepted.

79
00:08:20,060 --> 00:08:27,900
It's embedded in many legislations around the world and so far can be a valid starting

80
00:08:27,900 --> 00:08:28,900
point.

81
00:08:28,900 --> 00:08:33,700
And at least to get the conversation started, we can improve it later.

82
00:08:33,700 --> 00:08:43,940
And this system definition has this concept of it basically, you can read it, it was very

83
00:08:43,940 --> 00:08:51,260
in November, it was updated by the OECD.

84
00:08:51,260 --> 00:08:57,620
And the other thing that is important that we have learned is that we've established

85
00:08:57,620 --> 00:09:06,460
that for AI developers, we want practitioners that academia users of AI, we want them to

86
00:09:06,460 --> 00:09:11,340
have the same benefits of open source, which is the autonomy, the transparency, the fact

87
00:09:11,340 --> 00:09:16,500
that they have an agency, there is agency for the user.

88
00:09:16,500 --> 00:09:23,740
But we also noticed that policymakers and academia and maybe the developers themselves,

89
00:09:23,740 --> 00:09:28,980
developers of AI systems, they seem to be focusing more or concerned about transparency,

90
00:09:28,980 --> 00:09:31,060
explainability and other objectives.

91
00:09:31,060 --> 00:09:35,900
They're not thinking about open as a value.

92
00:09:35,900 --> 00:09:42,260
So we need to work to match the expectations of these two groups and make sure that open

93
00:09:42,260 --> 00:09:52,140
source AI helps ease the concerns of policymakers and academia.

94
00:09:52,140 --> 00:10:00,380
In other words, it does not block for example, transparent or trustworthy.

95
00:10:00,380 --> 00:10:05,580
We cannot have an open source AI that will never be transparent, will never be explainable

96
00:10:05,580 --> 00:10:11,420
or fair, because otherwise, there will never be an open source AI that can be adopted or

97
00:10:11,420 --> 00:10:17,540
that can be even legal if we look at some of the draft legislation that is flying around

98
00:10:17,540 --> 00:10:18,540
the world.

99
00:10:18,540 --> 00:10:28,700
So the next question that we asked ourselves in a small group is, what basic freedoms

100
00:10:28,700 --> 00:10:32,020
do we need in order to share AI systems?

101
00:10:32,020 --> 00:10:37,500
And the next, the sub question is, what is the preferred form to make modifications to

102
00:10:37,500 --> 00:10:41,480
an AI system?

103
00:10:41,480 --> 00:10:49,480
So the basic freedoms we have started by looking at the definition, the free software

104
00:10:49,480 --> 00:10:58,980
definition and tweaked the language during a few meetings in person.

105
00:10:58,980 --> 00:11:03,280
And we took the language to a point where it seems fair that we can have a more public

106
00:11:03,280 --> 00:11:05,520
conversation.

107
00:11:05,520 --> 00:11:15,960
And this is the current draft of the open source AI definition written down and you

108
00:11:15,960 --> 00:11:23,200
can see it's nothing too controversial or too complicated.

109
00:11:23,200 --> 00:11:29,520
We need to be able to use the system for any purpose without having to ask for permission.

110
00:11:29,520 --> 00:11:37,400
And it's quite important because that permissionless is what enabled open source, the open source

111
00:11:37,400 --> 00:11:40,200
world, the open source ecosystem to thrive.

112
00:11:40,200 --> 00:11:44,760
We need to be able to study, we need to be able to modify and give it to others for any

113
00:11:44,760 --> 00:11:48,720
purpose and without having to ask for permission again.

114
00:11:48,720 --> 00:11:56,760
Now the next big question in order to get a complete draft is, what is the preferred

115
00:11:56,760 --> 00:12:01,240
form to make modifications to an AI system?

116
00:12:01,240 --> 00:12:03,400
And that's what we need to do.

117
00:12:03,400 --> 00:12:05,800
This is the next big exercise that we need to do.

118
00:12:05,800 --> 00:12:07,560
We need to get the specification.

119
00:12:07,560 --> 00:12:13,020
So how are we going to be proceeding on this?

120
00:12:13,020 --> 00:12:20,160
We need to start by identifying the technical legal specifications of what is made, what

121
00:12:20,160 --> 00:12:22,160
an AI system is made of.

122
00:12:22,160 --> 00:12:27,280
What are the components that go into it?

123
00:12:27,280 --> 00:12:33,960
And what are these, the components that are, which of these components are necessary to

124
00:12:33,960 --> 00:12:41,200
use, to study, to share, modify such systems?

125
00:12:41,200 --> 00:12:48,280
Once we have that list of components that we need for each of those different four verbs,

126
00:12:48,280 --> 00:12:55,240
for freedoms, then we look at the legal frameworks that are for those.

127
00:12:55,240 --> 00:13:00,360
And from the legal frameworks, we can evaluate the legal elements, the legal documents that

128
00:13:00,360 --> 00:13:01,880
accompany them.

129
00:13:01,880 --> 00:13:09,320
Matching, for example, if one component is under copyright or intellectual property in

130
00:13:09,320 --> 00:13:16,720
general regime, then we can say the license, we can evaluate the license and see if that

131
00:13:16,720 --> 00:13:18,480
grants the freedoms.

132
00:13:18,480 --> 00:13:27,640
So after we repeat this exercise for more groups, then we'll have a better understanding.

133
00:13:27,640 --> 00:13:33,300
We can create that checklist.

134
00:13:33,300 --> 00:13:39,120
So here's the, how we're going to proceed.

135
00:13:39,120 --> 00:13:48,360
And we're going to proceed by evaluating a few examples, very specific elements, very

136
00:13:48,360 --> 00:13:56,080
specific systems like BTR or Lama2 or Bloom, and we'll split into small groups.

137
00:13:56,080 --> 00:13:59,960
And we're going to ask the questions one by one.

138
00:13:59,960 --> 00:14:03,480
What do I need to give input and get an output?

139
00:14:03,480 --> 00:14:07,040
So that's the use or and modify, et cetera.

140
00:14:07,040 --> 00:14:12,920
So for example, if we want to give one by one, let's look at PTA.

141
00:14:12,920 --> 00:14:18,240
What do I need in order to get an output from PTA?

142
00:14:18,240 --> 00:14:24,280
Then probably we'll need weights, inference code, for example, in this one as elements

143
00:14:24,280 --> 00:14:25,280
and components.

144
00:14:25,280 --> 00:14:30,560
Like then why is PTA giving an input gives one output?

145
00:14:30,560 --> 00:14:32,240
This is what we need to know.

146
00:14:32,240 --> 00:14:37,680
We probably need to know the architecture, what went into building the dataset, maybe

147
00:14:37,680 --> 00:14:43,440
access to the dataset itself and calculate the biases, et cetera.

148
00:14:43,440 --> 00:14:50,600
Then moving on, how do we modify and get a different output from PTA?

149
00:14:50,600 --> 00:14:54,240
They're big question.

150
00:14:54,240 --> 00:15:00,560
And finally, what we need in order to share it, you know, what share the original version

151
00:15:00,560 --> 00:15:02,840
or the modified version?

152
00:15:02,840 --> 00:15:13,880
So we'll need to run this exercise for more than one of these systems and write, get those

153
00:15:13,880 --> 00:15:20,000
components in general, analyze the legal frameworks, analyze the legal documents, and write up

154
00:15:20,000 --> 00:15:21,000
a summary.

155
00:15:21,000 --> 00:15:27,920
And that's going to be our-- most likely, it's going to be our basic components of the

156
00:15:27,920 --> 00:15:36,080
checklist that we have at the end of the document draft definition.

157
00:15:36,080 --> 00:15:46,680
In terms of timeline, we need to work-- we need to activate, like, at least-- we need

158
00:15:46,680 --> 00:15:53,320
to move very fast because everyone is-- there is already enough confusion on the market

159
00:15:53,320 --> 00:15:58,880
and many groups that are talking about open source AI without having a big-- without having

160
00:15:58,880 --> 00:16:01,480
a shared understanding of what that means.

161
00:16:01,480 --> 00:16:07,040
And we want to get with version 1 in October of this year.

162
00:16:07,040 --> 00:16:09,440
So towards the end of the year.

163
00:16:09,440 --> 00:16:14,880
And this means that we should be really having a release candidate around the beginning of

164
00:16:14,880 --> 00:16:15,880
the summer.

165
00:16:15,880 --> 00:16:25,040
In order to get to that, we need to have monthly release cadence of drafts and a constant

166
00:16:25,040 --> 00:16:31,320
public review of our work with these town halls that we're going to be running-- that

167
00:16:31,320 --> 00:16:38,320
are going to be running every two weeks at different time zones.

168
00:16:38,320 --> 00:16:43,480
And the important piece here is that-- so we're going to create working groups and we're

169
00:16:43,480 --> 00:16:54,680
going to create working groups to analyze these AI components.

170
00:16:54,680 --> 00:17:00,480
And we're going to be releasing new drafts as we go.

171
00:17:00,480 --> 00:17:02,920
Monthly with a monthly cadence.

172
00:17:02,920 --> 00:17:08,040
Hopefully by the end of May, early June, we'll have an in-presence meeting.

173
00:17:08,040 --> 00:17:16,320
We want to have enough support from different stakeholders.

174
00:17:16,320 --> 00:17:19,120
And I'm going to talk about that.

175
00:17:19,120 --> 00:17:23,560
What do we expect for the release candidate?

176
00:17:23,560 --> 00:17:29,960
Is to have at least a draft that is completed in all its parts.

177
00:17:29,960 --> 00:17:37,760
And support from at least two organizations, two groups for each of the stakeholders in

178
00:17:37,760 --> 00:17:43,280
that we have-- in the groups that we have identified that I will show in a second.

179
00:17:43,280 --> 00:17:49,240
And for version one, it's basically a larger group of support.

180
00:17:49,240 --> 00:17:58,680
So more stakeholders that support and endorse the definition.

181
00:17:58,680 --> 00:18:08,600
And we have identified six categories of stakeholders.

182
00:18:08,600 --> 00:18:13,480
The system creators, the ones who are going to be creating AI systems and they will need

183
00:18:13,480 --> 00:18:23,760
to-- so the ones that will create the AI systems.

184
00:18:23,760 --> 00:18:29,960
And the license creators, the ones who write the legal documents to apply to the AI system

185
00:18:29,960 --> 00:18:31,480
of components.

186
00:18:31,480 --> 00:18:37,320
The regulators, we want to have at least-- going to want to have conversations with regulators

187
00:18:37,320 --> 00:18:39,480
to get their feedback.

188
00:18:39,480 --> 00:18:46,560
If they may not be able to give us endorsements, but at least we want to hear what their thoughts.

189
00:18:46,560 --> 00:18:51,000
We're going to have early exposure to that.

190
00:18:51,000 --> 00:18:57,240
Then we have licensees, the ones who seek to study, modify, share an open source AI

191
00:18:57,240 --> 00:18:58,240
system.

192
00:18:58,240 --> 00:19:02,800
So it's engineers or developers, researchers.

193
00:19:02,800 --> 00:19:11,960
On the last two categories, it's where we probably need most help is end users.

194
00:19:11,960 --> 00:19:18,960
So the ones who, one, need to consume the system output, but are not necessarily interested

195
00:19:18,960 --> 00:19:24,280
in studying or modifying or sharing the system.

196
00:19:24,280 --> 00:19:32,960
And then the final group is the subjects, those who are affected by the effects of the

197
00:19:32,960 --> 00:19:37,120
system outputs, whether they are upstream or downstream.

198
00:19:37,120 --> 00:19:45,240
We use this to indicate, for example, prospective homeowners whose mortgage application is evaluated

199
00:19:45,240 --> 00:19:48,920
by a bank through an AI.

200
00:19:48,920 --> 00:19:51,040
That's a downstream subject.

201
00:19:51,040 --> 00:19:57,560
Or for example, photographers who find their image in a training data set.

202
00:19:57,560 --> 00:19:59,760
That's like content creators.

203
00:19:59,760 --> 00:20:02,440
Those are upstream subjects.

204
00:20:02,440 --> 00:20:06,880
We want to be talking to these organizations, too.

205
00:20:06,880 --> 00:20:08,880
And we want to have their feedback.

206
00:20:08,880 --> 00:20:18,120
And maybe and hopefully also they're endorsing the definition at the end.

207
00:20:18,120 --> 00:20:24,400
One thing also that I want to say is that this doesn't end, this process will probably

208
00:20:24,400 --> 00:20:30,360
not end with version 1, because this is going to be the first definition of open source

209
00:20:30,360 --> 00:20:33,200
that has a version number attached.

210
00:20:33,200 --> 00:20:39,680
So we'll need to have by the end of the year, once we announce version 1, we'll need to

211
00:20:39,680 --> 00:20:45,200
have in place rules for maintenance and review of this definition.

212
00:20:45,200 --> 00:20:52,000
We're probably going to need to maintain and update it, given how quickly the technical

213
00:20:52,000 --> 00:20:58,760
landscape changes, we will have to adapt.

214
00:20:58,760 --> 00:21:09,080
So our immediate next step is we want to have the process make it more public.

215
00:21:09,080 --> 00:21:16,280
Until now, we worked with a private drafting group that has been helping.

216
00:21:16,280 --> 00:21:23,920
And now we got to the point where we feel there is plenty of momentum on one hand and

217
00:21:23,920 --> 00:21:30,320
plenty of shared understanding of where we stand and what are the roadblocks, the biggest

218
00:21:30,320 --> 00:21:31,320
ones.

219
00:21:31,320 --> 00:21:33,720
So we want to have public discussion forums.

220
00:21:33,720 --> 00:21:39,120
We're starting today with this public biweekly town halls.

221
00:21:39,120 --> 00:21:44,560
And this will open up also to more opportunity for more opportunities to volunteer and help

222
00:21:44,560 --> 00:21:45,560
out.

223
00:21:45,560 --> 00:21:53,240
We'll be updating our project landing page, the opensource.org/deepdive.

224
00:21:53,240 --> 00:21:58,000
We need more stakeholders to get involved and we're raising funds.

225
00:21:58,000 --> 00:22:05,920
And we're also setting up the OSI board to review and approve version 1 once it comes

226
00:22:05,920 --> 00:22:08,480
out.

227
00:22:08,480 --> 00:22:13,800
And the drafts are being published.

228
00:22:13,800 --> 00:22:19,100
They're already public and they're already public also the comments.

229
00:22:19,100 --> 00:22:27,920
You can go to deepdive/drafts and you'll find a list of the published drafts and you can

230
00:22:27,920 --> 00:22:32,240
join the conversation in there.

231
00:22:32,240 --> 00:22:38,200
And with that, I want to open up to questions from you.

232
00:22:38,200 --> 00:22:48,520
I see that there is a little bit of a discussion here already.

233
00:22:48,520 --> 00:22:50,080
Do I have domain experts?

234
00:22:50,080 --> 00:22:51,080
Yes.

235
00:22:51,080 --> 00:22:54,800
So some domain experts have already volunteered.

236
00:22:54,800 --> 00:22:58,160
I'm talking to basically friends.

237
00:22:58,160 --> 00:23:04,080
Like there is one of the developer advocates and outreach advocates at Intel.

238
00:23:04,080 --> 00:23:05,720
He's a personal friend.

239
00:23:05,720 --> 00:23:07,920
I'm going to talk to him next week.

240
00:23:07,920 --> 00:23:16,480
The people at Luther AI have made themselves available to help out to analyze Pythia.

241
00:23:16,480 --> 00:23:27,320
And I'm reaching out to developers at Meta to get an explanation of Lama, Lama 2.

242
00:23:27,320 --> 00:23:29,680
And I'm happy to talk to more people.

243
00:23:29,680 --> 00:23:34,600
I have other people who have also volunteered to help.

244
00:23:34,600 --> 00:23:41,280
And we got expertise also inside the board to help out.

245
00:23:41,280 --> 00:23:49,520
To review, to run those reviews.

246
00:23:49,520 --> 00:23:54,600
The topic of content creators and the New York Times with their lawsuit.

247
00:23:54,600 --> 00:24:04,440
Yes, the legal team, the legal experts inside the board are already aware of that.

248
00:24:04,440 --> 00:24:05,440
They're following it.

249
00:24:05,440 --> 00:24:09,400
And I'm pushing the board and the board itself.

250
00:24:09,400 --> 00:24:18,880
People inside the board are getting more expertise and getting ready to even write opinions eventually

251
00:24:18,880 --> 00:24:21,000
on those topics.

252
00:24:21,000 --> 00:24:29,160
It's really interesting to see what's happening in the content on the content creators front

253
00:24:29,160 --> 00:24:35,400
and those legal issues.

254
00:24:35,400 --> 00:24:55,400
Any more curiosities?

255
00:24:55,400 --> 00:25:15,480
Maybe I can spend more time to talk about that checklist idea.

256
00:25:15,480 --> 00:25:22,840
The idea is to get it is to get the completion of that document of the open source AI definition

257
00:25:22,840 --> 00:25:30,760
needs to have something that resembles that can help that can help the license committee

258
00:25:30,760 --> 00:25:33,800
or the AI committee that will be formed.

259
00:25:33,800 --> 00:25:38,820
Some committee inside the open source initiative to evaluate the legal documents that are coming

260
00:25:38,820 --> 00:25:41,560
together with an AI system.

261
00:25:41,560 --> 00:25:44,800
So that an AI system can be judged whether it's open source or not.

262
00:25:44,800 --> 00:25:50,920
Whether it grants the four freedoms that it's supposed to grant.

263
00:25:50,920 --> 00:25:56,920
That is the checklist basically.

264
00:25:56,920 --> 00:25:59,920
Yes.

265
00:25:59,920 --> 00:26:02,920
Daniel.

266
00:26:02,920 --> 00:26:09,960
The question about the getting the discussion people from the global south and not just

267
00:26:09,960 --> 00:26:12,200
the United States and Europe.

268
00:26:12,200 --> 00:26:13,200
Absolutely.

269
00:26:13,200 --> 00:26:18,800
It was one of the eye opening conversations I've had with one of the meetings we had last

270
00:26:18,800 --> 00:26:26,840
year was in Africa, Ethiopia with the digital public goods alliance.

271
00:26:26,840 --> 00:26:34,000
Members meeting, members summit, private event with the DPGA members.

272
00:26:34,000 --> 00:26:35,280
It was really eye opening.

273
00:26:35,280 --> 00:26:38,160
So we are absolutely open to that.

274
00:26:38,160 --> 00:26:40,520
And more than happy.

275
00:26:40,520 --> 00:26:47,320
Like one of the call for actions, one of the things where you can help is to put us in

276
00:26:47,320 --> 00:26:53,360
touch with the people who can participate to these meetings.

277
00:26:53,360 --> 00:26:55,040
And workshops.

278
00:26:55,040 --> 00:27:02,840
So I'm happy to follow up with you, Daniel.

279
00:27:02,840 --> 00:27:05,840
Yes.

280
00:27:05,840 --> 00:27:06,840
Amanda.

281
00:27:06,840 --> 00:27:07,840
Good idea.

282
00:27:07,840 --> 00:27:08,840
Yes.

283
00:27:08,840 --> 00:27:14,840
We'll publish the agenda as we go forward.

284
00:27:14,840 --> 00:27:28,320
Ian or John, I don't know how to pronounce your name.

285
00:27:28,320 --> 00:27:29,320
Probably so.

286
00:27:29,320 --> 00:27:31,420
That's one of the things that we need to understand.

287
00:27:31,420 --> 00:27:39,860
What kind of so access to training data is probably unavoidable.

288
00:27:39,860 --> 00:27:43,480
The question is what kind of access?

289
00:27:43,480 --> 00:27:45,200
What level of access?

290
00:27:45,200 --> 00:27:48,480
The full on training data set?

291
00:27:48,480 --> 00:27:54,160
Or is it sufficient to have a detailed description of what went into it?

292
00:27:54,160 --> 00:27:56,320
Or something else?

293
00:27:56,320 --> 00:28:00,800
That's what we absolutely that's probably one of the most important, most difficult

294
00:28:00,800 --> 00:28:02,280
questions to ask.

295
00:28:02,280 --> 00:28:08,560
And most delicate conversation also.

296
00:28:08,560 --> 00:28:12,720
That's why we need to look at the individual specific.

297
00:28:12,720 --> 00:28:16,400
We try to have this conversation in generic terms.

298
00:28:16,400 --> 00:28:23,880
And having the conversation in generic terms ended up generating a lot of yes, but.

299
00:28:23,880 --> 00:28:27,400
Or yeah, but in this case, maybe if.

300
00:28:27,400 --> 00:28:29,800
You know, lots of uncertainties.

301
00:28:29,800 --> 00:28:31,080
We need to be specific.

302
00:28:31,080 --> 00:28:37,880
Let's look at specifically what's happening individually with PTA, with Lama 2, with FIDO,

303
00:28:37,880 --> 00:28:44,800
with one of these with OpenCV components, like OpenCV algorithms.

304
00:28:44,800 --> 00:28:48,480
Let's have a look at specifically what these need.

305
00:28:48,480 --> 00:28:56,680
And then we generalize.

306
00:28:56,680 --> 00:29:19,840
SBOMS with yeah, of course, like again, with the expert, with someone who has been modifying

307
00:29:19,840 --> 00:29:20,840
Lama 2.

308
00:29:21,000 --> 00:29:27,080
For example, this is one of the questions I asked one of the one of the friend of mine

309
00:29:27,080 --> 00:29:32,760
who's been working with and modifying Lama 2.

310
00:29:32,760 --> 00:29:34,920
Let's have a chat.

311
00:29:34,920 --> 00:29:40,640
What do you what do you actually need in order to modify the behavior of the Lama 2?

312
00:29:40,640 --> 00:29:46,440
For example, that's the exercise we need to do.

313
00:29:46,440 --> 00:29:47,440
Responsible data.

314
00:29:47,440 --> 00:29:48,440
Okay.

315
00:29:48,440 --> 00:29:52,360
Amanda, can you send me?

316
00:29:52,360 --> 00:29:54,360
Yeah, I'll save that.

317
00:29:54,360 --> 00:30:06,480
And I will if you have people who you know who will be I'd be happy to talk to them.

318
00:30:06,480 --> 00:30:36,000
I see other people typing.

319
00:30:36,000 --> 00:30:41,400
Did I answer your question?

320
00:30:41,400 --> 00:30:43,400
Okay.

321
00:30:43,400 --> 00:30:50,400
Thank you.

322
00:30:50,400 --> 00:31:05,800
Yes, links to slides.

323
00:31:05,800 --> 00:31:14,800
I got to publish them.

324
00:31:14,800 --> 00:31:24,160
So the next steps I mentioned, one is to we're going to be creating these these these forums,

325
00:31:24,160 --> 00:31:31,760
public forums where we can we can have conversations more more openly.

326
00:31:31,760 --> 00:31:36,600
And let me pull it back.

327
00:31:36,600 --> 00:31:42,960
And and run the exercise.

328
00:31:42,960 --> 00:31:46,840
I have scheduled meetings next week.

329
00:31:46,840 --> 00:31:55,520
As we as we get more as we run these exercises, analyzing PTA and analyzing Lama 2, building

330
00:31:55,520 --> 00:32:08,280
that list of elements of components and getting through the the the analysis of the legal

331
00:32:08,280 --> 00:32:16,600
frameworks and and and their legal things like let me get back to the slide where it

332
00:32:16,600 --> 00:32:19,600
shows.

333
00:32:19,600 --> 00:32:30,240
Yeah, this is the exercise that we need to do.

334
00:32:30,240 --> 00:32:31,240
This one.

335
00:32:31,240 --> 00:32:36,560
We need to go through this pipeline as quickly as possible.

336
00:32:36,560 --> 00:32:43,240
And once it's completed, we'll have version five, draft five.

337
00:32:43,240 --> 00:32:50,340
And from then on, we'll keep on iterating with with more online meetings and we'll try

338
00:32:50,340 --> 00:32:55,080
to be more more visible, more transparent from now on.

339
00:32:55,080 --> 00:32:56,360
Everything will be public.

340
00:32:56,360 --> 00:33:04,400
And if you follow us on Mastodon and and on LinkedIn, you get the updates.

341
00:33:04,400 --> 00:33:11,040
But as soon as hopefully end of next week, we'll have the public forum, if not the beginning

342
00:33:11,040 --> 00:33:13,120
of the week after.

343
00:33:13,120 --> 00:33:18,360
By the end of the month, for sure, we'll have the public forums that you can sign up and

344
00:33:18,360 --> 00:33:22,600
have discussions and conversations and get updates.

345
00:33:22,600 --> 00:33:27,800
All right.

346
00:33:27,800 --> 00:33:34,040
Are there any more questions or doubts?

347
00:33:34,040 --> 00:33:57,320
Yes, we should.

348
00:33:57,320 --> 00:34:01,880
We should be studying also applications not based on machine learning or deep learning.

349
00:34:01,880 --> 00:34:05,120
Yes, Jean Pierre.

350
00:34:05,120 --> 00:34:10,920
You have any specific suggestions, recommendations for one example that we or two examples that

351
00:34:10,920 --> 00:34:18,120
we we need to look at?

352
00:34:18,120 --> 00:34:32,720
Claire, the best link is open source.org/deepdive.

353
00:34:32,720 --> 00:34:38,640
But that page is in the process of being updated.

354
00:34:38,640 --> 00:34:49,760
Diane, yes, ML Commons is looped in with this initiative and as is the Linux Foundation,

355
00:34:49,760 --> 00:34:53,000
Linux Foundation groups.

356
00:34:53,000 --> 00:34:57,820
That's another thing that we should be we could be doing is to publish all the organizations

357
00:34:57,820 --> 00:35:16,000
that have been participating so far, following the development so far.

358
00:35:16,000 --> 00:35:39,600
I offer.

359
00:35:39,600 --> 00:35:48,600
Okay.

360
00:36:08,600 --> 00:36:22,920
I think we can also do voice.

361
00:36:25,760 --> 00:36:27,000
Voice questions.

362
00:36:31,800 --> 00:36:33,000
If anyone has.

363
00:36:37,320 --> 00:36:40,760
Yeah, we'll share it as a PDF, don't worry about it.

364
00:36:43,480 --> 00:36:52,680
All right, folks, if there are no more questions, I'm gonna close the recording and

365
00:36:52,680 --> 00:36:57,920
we'll make available the recording and a slide deck.

366
00:36:57,920 --> 00:37:04,520
And I'm gonna be available again in two weeks, similar content.

367
00:37:04,520 --> 00:37:10,200
We're gonna alternate the times so that instead of being in the afternoons in

368
00:37:10,200 --> 00:37:12,320
Europe time, we're gonna be in the morning in Europe time.

369
00:37:12,320 --> 00:37:17,400
So we're gonna try to follow to get the Asia Pacific

370
00:37:17,400 --> 00:37:21,480
audience to follow, an opportunity to follow.

371
00:37:21,480 --> 00:37:24,400
So thanks everyone, enjoy your weekend.

### End of last town hall held on 2024-01-12 ###

### Start of next town hall held on 2024-01-26 ###
--- Presentation for 2024-01-26 ---
OPEN SOURCE AI DEFINITION
Online public townhall
Jan 26, 2024

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

The objective for 2024

Open Source AI Deﬁnition
version 1.0

Deﬁnition of AI system

Preamble
Out of scope issues
4 freedoms
License checklist

What is Open Source AI
To be Open Source, an AI system needs to be available under
legal terms that grant the freedoms to:
● Use the system for any purpose and without having to ask
for permission.
● Study how the system works and inspect its components.
● Modify the system to change its recommendations,
predictions or decisions to adapt to your needs.
● Share the system with or without modiﬁcations, for any
purpose.

What is the preferred form to make
modiﬁcations to an AI system?

Getting the speciﬁcations
AI systems

As deﬁned by the
OECD.

List of
components

Legal
frameworks

Legal
documents

Checklist

What elements are
necessary to:
- use
- study
- modify
- share
an AI system?

For each artifact,
evaluate which
laws apply. Some
will be under
“Intellectual
Property” regimes,
some will be under
other regimes.

We’ll match the
components and
the identiﬁed legal
frameworks with
the terms of the
legal documents
already in use,
where available.

After repeating
this exercise
enough times,
we’ll be able to
generalize the
outcomes and
write the specs to
evaluate the
freedoms granted.

Report from the ﬁrst working group
session
Analyzing Llama2

8

Participants
●
●
●
●
●
●
●
●

✔ Stefano Maffulli -- Open Source Initiative (convener)
✔ Mer Joyce -- Do Big Good (facilitator)

✔ Bastien Guerry -- DINUM, French public

administration
✔ Ezequiel Lanza -- Intel
✔ Roman Shaposhnik -- Apache Software Foundation
✔ Davide Testuggine -- Meta
✔ Jonathan Torres -- Meta
Stefano Zacchiroli -- Polytechnic Institute of Paris

✔ = attended
All members participating in a personal capacity.

9

Purpose
● Process -- OSI has been convening a global
conversation to ﬁnd the deﬁnition of open source AI for
almost two years.
● Track -- The 2024 objective scope for Track 1: System
Testing is to discover what components need to be
available in each AI system for the whole system to be
studied, used, modiﬁed, and shared. We plan to
complete this track at the latest by May.
● Working group report -- objective is to talk through
initial points of difference on what components of
Llama 2 would need to be open for the whole AI system
to be studied, used, modiﬁed, and shared.
10

Framing
● Document – We’ll review the components table in the
Llama 2 specs doc and decide which exist in that AI
system, with a focus on resolving disagreement.
● Expectations – We’ll see how much of the table we get
through. (Insights on tempo and pace will be among
the learnings from this meeting.)
● Anything else? – Are there any other expectations or
framings we should put in place before we begin
working through the components table?
Go to Llama 2 document →

11

What do you need to give an input and get an
output from LLaMA2? (use)
Code

Which of these components is strictly necessary to use
Llama2?

Data preprocessing code

Not necessary BG EL
Nice for scientific reproducibility DT SM JT

Training code

Not necessary BG , EL, JT

Code used to perform inference for benchmark tests

Not necessary BG , DT, EL, SM, JT

Inference code

Necessary SM, EL, DT BG

Evaluation code
Any libraries or other code artifacts that are part of the
system, such as tokenizers and hyperparameter search
code, if used.

Necessary BG,EL SM
Should be split by “do you need it to run the model?”, so a
tokenizer is necessary while hyperparameter search code
is not (you do hyperparameter search at training time)

12

What do you need to give an input and get an
output from LLaMA2? (use)
Data - All data sets, including:
Training data sets

Not necessary - JT ,EL
Not necessary BG

Testing data sets

Necessary to check performance claims & compare
models, not necessary to run the model DT
Nice to have (for validation) - JT
Not necessary BG ,EL

Validation data sets

Nice to have (for validation) - JT DT SM
Not necessary BG

Benchmarking data sets

Nice to have (for validation)- JT ,EL
Not necessary BG

Data cards

Nice to have - JT
Not necessary BG

Evaluation metrics and results

Nice to have - JT
Not necessary BG

All other data documentation

Nice to have - Jt
Not necessary BG

13

What do you need to give an input and get an
output from LLaMA2? (use)
Model
[description TK]
Model architecture

Not necessary EL
Not necessary BG

Model parameters

Not necessary EL
Necessary SM

Model card

Necessary EL

Sample model outputs

Nice to have EL SM

Other documentation [or .…] produced, including
Thorough research papers

Nice to have EL SM

Usage documentation

Necessary EL SM

14

Next questions
What do you need
to understand how
LLaMA2 was built,
how can it be
ﬁne-tuned, what
biases, get a sense
of why an output to
an input … ? (study)
◦

how was it built,
explain its
performance,
etc.

What do you need
to give an input and
get a different
output from
LLaMA2? (modify)
◦

Techniques and
tools to
adapt/modify
for use including
ﬁne-tune and
optimize.

What do you need
to let others give an
input and get an
output from
LLaMA2? (share)
◦

Either
as-received or
after ﬁne-tuning
and other
modiﬁcations.

15

Next steps

16

Recruiting volunteers
- Review and validate the list of components
- Analyze other AI systems
(Pythia, BLOOM, Mistral, OpenCV …)

17

2024 timeline

System testing work stream
Stakeholder consultation work stream
Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

Draft 0.0.8

Townhall +

… October
Monthly Virtual
Meetings

Release
version 1.0

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

v. 1.0

Criteria for RC1 and v. 1.0
RC1

version 1

- Expected outcome of
in-person meeting end
May/early June!
- The draft is completed
in all its parts
- The draft is supported
by at least 2
representatives for
each of the 6
stakeholder groups

- Expected outcome of
in-person and online
meetings through the
summer/early autumn
- The draft is endorsed by at
least 5 reps for each of the
stakeholder groups
- Announced in late October

Help us ﬁnd stakeholders
System Creator

License Creator

Regulator

Licensee

End User

Subject

Makes AI system
and/or
component that
will be studied,
used, modified,
or shared
through an open
source license
(e.g., ML
researcher in
academia or
industry)

Writes or edits
the open source
license to be
applied to the AI
system or
component;
includes
compliance
(e.g., IP lawyer)

Writes or edits
rules governing
licenses and
systems (e.g.
government
policy-maker)

Seeks to study,
use modify, or
share an open
source AI
system (e.g. AI
engineer, health
researcher,
education
researcher)

Consumes a
system output,
but does not
seek to study,
use, modify, or
share the
system (e.g.,
student using a
chatbot to write
a report, artist
creating an
image)

Affected
upstream or
downstream by
a system output
without
interacting with it
intentionally;
includes
advocates for
this group (e.g.
people with loan
denied, or
content creators)

✅

✅

⚠

✅

⚠

⚠

Enough to start

Enough to start

Leads to US, EU,
Singapore, no
commitment yet

Enough to start

Which org is squarely in
this space?

ACLU, Algorithmic
Justice League

It doesn’t end with v. 1.0
We’ll need to deﬁne rules for maintenance and review of the
Deﬁnition

OSI’s immediate next steps
- more publicity to the process
- public discussion forum https://discuss.opensource.org
- bi-weekly townhalls
- more opportunities to volunteer
- update project landing page
- reach out to more stakeholders
- raise funds for 2024 meetings
- setup the board for review and approval of v. 1.0

Join the conversation
● Public forum
● Join as OSI member
○ Free or full
○ SSO with other
OSI websites

23

Draft v. 0.0.4 of the Open Source AI Deﬁnition
Open to public comments

https://opensource.org/deepdive/drafts

24


--- Subtitles for 2024-01-26 --- ###
1
00:00:00,001 --> 00:00:08,120
All right, and welcome.

2
00:00:08,120 --> 00:00:09,120
Welcome everyone.

3
00:00:09,120 --> 00:00:14,920
This is our second public town hall on the open source AI definition.

4
00:00:14,920 --> 00:00:23,400
The process that the open source initiative has started more than a year ago now.

5
00:00:23,400 --> 00:00:30,800
So a little ground rules before we start, we have these community agreements and I love

6
00:00:30,800 --> 00:00:37,000
to have your comments on these one person at a time.

7
00:00:37,000 --> 00:00:40,000
Meaning make space for others.

8
00:00:40,000 --> 00:00:45,040
If you have the tendency to speak a lot, try yourself.

9
00:00:45,040 --> 00:00:49,560
Think about being quiet and allow others to speak up.

10
00:00:49,560 --> 00:00:54,560
And if you're the kind of person who usually doesn't speak, I highly encourage you to raise

11
00:00:54,560 --> 00:01:01,520
your hand, write down your comments on the chat or take the mic.

12
00:01:01,520 --> 00:01:03,080
We want to hear your voice.

13
00:01:03,080 --> 00:01:04,760
We want to hear your comments.

14
00:01:04,760 --> 00:01:05,760
Everyone is welcome.

15
00:01:05,760 --> 00:01:06,760
Be nice.

16
00:01:06,760 --> 00:01:11,920
I don't think that we need to be talking too much about this.

17
00:01:11,920 --> 00:01:19,000
We expect everyone to go through this hard work without being hard themselves.

18
00:01:19,000 --> 00:01:24,520
And another couple of points, like we need to keep moving.

19
00:01:24,520 --> 00:01:31,600
And this means that if we face an obstacle during our conversations and as we are drafting

20
00:01:31,600 --> 00:01:38,600
the open source AI definition, we may want to face recognize that we have an obstacle

21
00:01:38,600 --> 00:01:40,840
or something hard to deal with.

22
00:01:40,840 --> 00:01:45,720
But during the meetings, we try to keep on moving.

23
00:01:45,720 --> 00:01:47,880
And we'll get back to the hard part later.

24
00:01:47,880 --> 00:01:51,280
So then we go towards the destination.

25
00:01:51,280 --> 00:01:53,680
And focus on solutions.

26
00:01:53,680 --> 00:01:56,840
That means this is pioneer work.

27
00:01:56,840 --> 00:01:59,680
It's a multi-stakeholder process.

28
00:01:59,680 --> 00:02:05,040
And we know that there are a lot of things that don't work that can be done better.

29
00:02:05,040 --> 00:02:13,960
But we need to focus on what works and keep marching towards the end goal.

30
00:02:13,960 --> 00:02:20,680
And the end goal is to have the open source AI definition version 1 by the end of the

31
00:02:20,680 --> 00:02:21,680
year.

32
00:02:21,680 --> 00:02:25,000
By ideally October.

33
00:02:25,000 --> 00:02:28,200
We started more than a year ago.

34
00:02:28,200 --> 00:02:34,000
And we need to have a shared understanding among multiple experts in various disciplines

35
00:02:34,000 --> 00:02:36,320
around the world.

36
00:02:36,320 --> 00:02:44,760
This definition is not coming from a sacred text given by a genius or a saint or some

37
00:02:44,760 --> 00:02:48,040
other form of venerable entity.

38
00:02:48,040 --> 00:02:50,160
It needs to be built by us.

39
00:02:50,160 --> 00:02:54,680
And we need to build an understanding together as we move forward before we can call this

40
00:02:54,680 --> 00:02:58,480
version 1.0.

41
00:02:58,480 --> 00:03:04,400
And I said we started some time ago now.

42
00:03:04,400 --> 00:03:09,800
And this is what we have achieved so far.

43
00:03:09,800 --> 00:03:14,640
We have a document that we're calling this definition.

44
00:03:14,640 --> 00:03:17,720
And it's made of a few parts.

45
00:03:17,720 --> 00:03:23,600
There is a definition of AI system at the beginning which is the same as the definition

46
00:03:23,600 --> 00:03:30,160
used by the organization for economic cooperation and development.

47
00:03:30,160 --> 00:03:31,160
OECD.

48
00:03:31,160 --> 00:03:38,640
And there's a preamble that contains the basic principles of why we need open source AI.

49
00:03:38,640 --> 00:03:43,560
And also mention of what is out of scope.

50
00:03:43,560 --> 00:03:50,800
Meaning the things that we're not covering in the definition.

51
00:03:50,800 --> 00:03:56,520
Then for freedoms which is the shortest possible answer to the question what is open source

52
00:03:56,520 --> 00:04:07,680
AI and followed by a checklist to evaluate legal documents, not just licenses that are

53
00:04:07,680 --> 00:04:14,320
used to grant the four freedoms to the components and elements of an AI system.

54
00:04:14,320 --> 00:04:20,080
So we have done quite a bit of work on the part above the checklist.

55
00:04:20,080 --> 00:04:21,080
And that's what we're missing.

56
00:04:21,080 --> 00:04:22,280
We're missing the checklist.

57
00:04:22,280 --> 00:04:24,160
That's what we're focusing on.

58
00:04:24,160 --> 00:04:28,520
That's what we focused on the last couple of weeks.

59
00:04:28,520 --> 00:04:30,640
Since our last meeting.

60
00:04:30,640 --> 00:04:33,920
So what is open source AI in terms of the four freedoms?

61
00:04:33,920 --> 00:04:38,160
This is the text that we have now in draft four.

62
00:04:38,160 --> 00:04:44,240
And after a few reviews with a few experts, I don't think that this is up for discussion.

63
00:04:44,240 --> 00:04:48,720
But I don't think that there are a lot of controversies or controversial statements

64
00:04:48,720 --> 00:04:49,720
in here.

65
00:04:49,720 --> 00:04:55,760
I think it should be quite fairly understood and fairly well supported.

66
00:04:55,760 --> 00:05:03,600
Now what we need to do in terms of question that we need to find an answer for, the next

67
00:05:03,600 --> 00:05:09,680
big thing is to understand what is the preferred form to make modifications to an AI system.

68
00:05:09,680 --> 00:05:14,000
Because if you those of you who are familiar with the open source definition and the free

69
00:05:14,000 --> 00:05:23,080
software definition, the preferred form to make modification is quite crucial to understand

70
00:05:23,080 --> 00:05:27,160
how you can make how you can exercise your freedom on software.

71
00:05:27,160 --> 00:05:31,000
To exercise your freedoms on software, for example, the freedom to study or the freedom

72
00:05:31,000 --> 00:05:37,480
to modify the software, you need access to the source code of that program.

73
00:05:37,480 --> 00:05:44,640
And we need to find an equivalent of source code of the program for the AI system.

74
00:05:44,640 --> 00:05:53,160
And the proposed path towards getting there is to use this multiple steps.

75
00:05:53,160 --> 00:05:58,000
First start from the concept of AI system.

76
00:05:58,000 --> 00:06:07,080
Now AI system as defined by the OECD means to simplify a system, a digital system that

77
00:06:07,080 --> 00:06:15,200
is capable given an input to generate an output with various degrees of independence from

78
00:06:15,200 --> 00:06:16,440
human interactions.

79
00:06:16,440 --> 00:06:23,640
So input, magic collaboration, output.

80
00:06:23,640 --> 00:06:29,640
Now for the first step that we're working on now is to find the list of components that

81
00:06:29,640 --> 00:06:35,360
are necessary to use, study, modify, share an AI system.

82
00:06:35,360 --> 00:06:41,360
And depending on the component, then from the list of components, we're going to look

83
00:06:41,360 --> 00:06:50,840
at each of these components to see if in which in each legal, which legal framework, legal

84
00:06:50,840 --> 00:06:53,220
regimes they operate under.

85
00:06:53,220 --> 00:07:00,720
So if it's, for example, the inference code or the training code that is software, it's

86
00:07:00,720 --> 00:07:05,400
under the what we already know that is covered by software.

87
00:07:05,400 --> 00:07:11,120
So it's going to be what people refer to as usually intellectual property or things like

88
00:07:11,120 --> 00:07:12,120
that.

89
00:07:12,120 --> 00:07:16,880
If it's data, then we have different regime.

90
00:07:16,880 --> 00:07:20,880
And this is also going to be an exercise where we're going to be identifying potential gaps.

91
00:07:20,880 --> 00:07:28,080
But we already know, for example, that large language models, the weights, for example,

92
00:07:28,080 --> 00:07:36,480
are not clearly, are not yet clearly labeled either as data or software, and they may fall

93
00:07:36,480 --> 00:07:39,560
into some other regime.

94
00:07:39,560 --> 00:07:44,120
So there's going to be conversations around there.

95
00:07:44,120 --> 00:07:47,080
And then we're going to look at existing frameworks.

96
00:07:47,080 --> 00:07:51,960
I mean, for each component, we're going to look at the legal documents that go with them

97
00:07:51,960 --> 00:07:55,680
and see if there is any gaps.

98
00:07:55,680 --> 00:08:00,680
And reading them and analyzing them, identify common traits.

99
00:08:00,680 --> 00:08:07,600
And finally, we should have, after repeating this exercise for many different AI systems,

100
00:08:07,600 --> 00:08:15,000
we're going to have something that looks like a valuable checklist that we can use for many

101
00:08:15,000 --> 00:08:19,040
years probably, hopefully.

102
00:08:19,040 --> 00:08:27,320
So last week I said the past weeks we have started working to analyze Lama 2 as an AI

103
00:08:27,320 --> 00:08:30,120
system as an example.

104
00:08:30,120 --> 00:08:35,400
And this is what we discovered in the meetings and online conversations that we had last

105
00:08:35,400 --> 00:08:36,400
week.

106
00:08:36,400 --> 00:08:42,080
We assembled a working group made of these people from different organizations, different

107
00:08:42,080 --> 00:08:46,440
expertise, all of them participating in the personal capacity.

108
00:08:46,440 --> 00:08:53,320
This is one of the -- none of them is authorized to speak for the company, but they are working

109
00:08:53,320 --> 00:08:59,880
-- they're collaborating with us.

110
00:08:59,880 --> 00:09:11,240
And the -- so the purpose of the meeting is -- I mean, we show the purpose of the meeting.

111
00:09:11,240 --> 00:09:19,080
It's part of the -- part of a track of work that is testing the system to discover the

112
00:09:19,080 --> 00:09:31,040
components that are available, like the -- and so make that list and identify which ones

113
00:09:31,040 --> 00:09:36,920
of them are absolutely necessary for -- to exercise each of the individual freedoms.

114
00:09:36,920 --> 00:09:43,040
Like the study -- use, study, modify, and share.

115
00:09:43,040 --> 00:09:55,160
And so we have reviewed the table of components that we have -- that we have borrowed.

116
00:09:55,160 --> 00:10:04,440
We've been following the work of the Linux Foundation, AI and data foundation, and data

117
00:10:04,440 --> 00:10:09,640
group, they've been working on a document on their own.

118
00:10:09,640 --> 00:10:16,560
And that contains a pretty long, pretty detailed list of components.

119
00:10:16,560 --> 00:10:24,160
And we've used that as a conversation starter for our exercise.

120
00:10:24,160 --> 00:10:28,920
We have -- so this is what we've done.

121
00:10:28,920 --> 00:10:34,080
We have separated the list of components into four main blocks.

122
00:10:34,080 --> 00:10:37,600
Well, three main blocks and one smaller block.

123
00:10:37,600 --> 00:10:40,280
One for things that we call code.

124
00:10:40,280 --> 00:10:42,320
That is software.

125
00:10:42,320 --> 00:10:47,680
And we have, for example, looking at the code, list of components that we call code.

126
00:10:47,680 --> 00:10:53,280
And apply to the exercise -- I mean, the function of using an AI system.

127
00:10:53,280 --> 00:10:55,840
So getting an output from an AI system.

128
00:10:55,840 --> 00:11:07,040
Asked the group to write down if that component was necessary, strictly necessary or not.

129
00:11:07,040 --> 00:11:11,360
For running or using the system.

130
00:11:11,360 --> 00:11:16,080
And this is the first result for the code.

131
00:11:16,080 --> 00:11:22,080
And then the next question is looking at the data.

132
00:11:22,080 --> 00:11:29,640
Separated into many -- you see many different types of datasets in here.

133
00:11:29,640 --> 00:11:35,880
And we asked what is necessary to use Llama2 in terms of data.

134
00:11:35,880 --> 00:11:42,520
And as you can see, most of the answers are in the not necessary.

135
00:11:42,520 --> 00:11:51,320
And we also captured in the document the comments on nuanced approaches.

136
00:11:51,320 --> 00:11:54,360
It's not necessarily it would be nice to have.

137
00:11:54,360 --> 00:11:56,640
And this is something that we want to debate further.

138
00:11:56,640 --> 00:12:02,480
I'm gonna ask a question to you afterwards.

139
00:12:02,480 --> 00:12:12,120
And finally, on the model itself, so the model weights, parameters, architecture, model card,

140
00:12:12,120 --> 00:12:19,080
et cetera, we asked people here to describe what is needed to use.

141
00:12:19,080 --> 00:12:24,720
I gotta say there was a little bit of a lot of actually a lot of conversations during

142
00:12:24,720 --> 00:12:30,720
the meeting around the meaning of these words.

143
00:12:30,720 --> 00:12:35,440
And there was a major misunderstanding on the word model parameters.

144
00:12:35,440 --> 00:12:46,280
Because in the intention of the paper from the LFAI data, which is a very early draft,

145
00:12:46,280 --> 00:12:55,480
so it's not really meant to be quoted yet, model parameters contains both model weights

146
00:12:55,480 --> 00:13:05,600
and model biases and parameters, hyperparameters and other elements.

147
00:13:05,600 --> 00:13:07,800
So there was a little bit of confusion.

148
00:13:07,800 --> 00:13:12,520
But there was -- you know, the group seemed to agree that, of course, you need the model

149
00:13:12,520 --> 00:13:15,800
weights to run, to use Llama2.

150
00:13:15,800 --> 00:13:22,880
And other components like model card, some people interpreted the definition of model

151
00:13:22,880 --> 00:13:26,040
card as something that is necessary for use.

152
00:13:26,040 --> 00:13:33,040
And then finally, the last group of elements or components is on the documentation, the

153
00:13:33,040 --> 00:13:34,040
supporting documentation.

154
00:13:34,040 --> 00:13:40,360
Like, the availability of a thorough research paper for the execution, you know, for running

155
00:13:40,360 --> 00:13:42,560
Llama2 is nice to have.

156
00:13:42,560 --> 00:13:46,320
I guess, you know, you can understand a lot of things.

157
00:13:46,320 --> 00:13:53,200
But it's actually not strictly -- what is strictly or much more necessary is have documentation

158
00:13:53,200 --> 00:13:55,720
on the usage.

159
00:13:55,720 --> 00:13:57,520
And this is the current status.

160
00:13:57,520 --> 00:14:02,360
We didn't get through -- we didn't get through the first meeting, during the first meeting

161
00:14:02,360 --> 00:14:06,240
through the other freedoms, Modify and others.

162
00:14:06,240 --> 00:14:11,720
But we're gonna keep on iterating with the others.

163
00:14:11,720 --> 00:14:12,720
So yeah.

164
00:14:12,720 --> 00:14:13,960
This is what we need to do next.

165
00:14:13,960 --> 00:14:19,200
It's to ask these questions on what you need to study.

166
00:14:19,200 --> 00:14:26,880
So understand how the Llama2 was built, fine-tuned, how it can be fine-tuned, what biases are

167
00:14:26,880 --> 00:14:31,560
in the dataset.

168
00:14:31,560 --> 00:14:34,920
And things like that.

169
00:14:34,920 --> 00:14:38,240
Explain its performance.

170
00:14:38,240 --> 00:14:45,480
And on the Modify questions are more on the -- what are the tools, techniques that we

171
00:14:45,480 --> 00:14:55,360
can use to fine-tune, optimize, get a different output from -- or faster outputs from the

172
00:14:55,360 --> 00:14:58,320
model or more accurate.

173
00:14:58,320 --> 00:15:01,800
And finally, in order to share it, what are we gonna do?

174
00:15:01,800 --> 00:15:07,680
You know, what is necessary, what is needed?

175
00:15:07,680 --> 00:15:18,240
So for us, the next steps are continuing on this process of running these meetings and

176
00:15:18,240 --> 00:15:21,680
starting new AI systems.

177
00:15:21,680 --> 00:15:28,720
We've already -- I've already asked a few people who volunteered to analyze Pythia.

178
00:15:28,720 --> 00:15:33,360
But we need to start parallel the process for Bloom and Mistral.

179
00:15:33,360 --> 00:15:38,240
And also we want to look at AI systems that are not generative AI, that are not large

180
00:15:38,240 --> 00:15:40,760
language models.

181
00:15:40,760 --> 00:15:45,480
And like inside OpenCV, the Open Computer Vision Project, there are a lot of neural

182
00:15:45,480 --> 00:15:51,240
networks and other kind of AI that are not generative AI.

183
00:15:51,240 --> 00:15:55,600
So they pose a slightly different -- slightly different questions if we want to look into

184
00:15:55,600 --> 00:15:56,600
those.

185
00:15:56,600 --> 00:16:00,880
So if you know anyone in those areas, point them our way.

186
00:16:00,880 --> 00:16:06,280
And I'll give you also more information about how to engage later.

187
00:16:06,280 --> 00:16:09,760
And we also need to validate this list of components.

188
00:16:09,760 --> 00:16:16,680
As I mentioned, we worked with the AI and data from Linux Foundation.

189
00:16:16,680 --> 00:16:21,680
But we know that the -- that their paper is not peer-reviewed.

190
00:16:21,680 --> 00:16:24,400
And their working paper, they're still working on it.

191
00:16:24,400 --> 00:16:27,800
So we need to provide feedback to them.

192
00:16:27,800 --> 00:16:36,720
But also we need to see whether that list of components is enough or if we need to keep

193
00:16:36,720 --> 00:16:40,960
on improving it.

194
00:16:40,960 --> 00:16:45,680
So to reiterate our timeline, it still looks like this.

195
00:16:45,680 --> 00:16:50,720
We want to have a new draft of the definition in February.

196
00:16:50,720 --> 00:16:54,200
And then a regular cadence every month.

197
00:16:54,200 --> 00:16:56,520
Have a new draft of the definition.

198
00:16:56,520 --> 00:17:00,120
Refining at every step.

199
00:17:00,120 --> 00:17:06,040
And most importantly, the most important part is as we refine this step, we also have more

200
00:17:06,040 --> 00:17:10,000
publicity to it, more people supporting it, endorsing it.

201
00:17:10,000 --> 00:17:18,040
And we need it to -- we want to get to a point in between the end of May, early June, where

202
00:17:18,040 --> 00:17:23,920
we have enough support and enough endorsements collected from a variety of different stakeholders

203
00:17:23,920 --> 00:17:32,720
to be able to call the definition feature complete and issue a release candidate, first

204
00:17:32,720 --> 00:17:33,720
release candidate.

205
00:17:33,720 --> 00:17:39,520
Then between June and October, we want to get into a series of conferences, a series

206
00:17:39,520 --> 00:17:44,000
of meetings around the world with me and other volunteers who have participated in the drafting

207
00:17:44,000 --> 00:17:45,000
process.

208
00:17:45,000 --> 00:17:51,480
Maybe some of the original endorsers that participated to release candidate work.

209
00:17:51,480 --> 00:18:00,000
And push it through a big exposure and a round of larger feedback.

210
00:18:00,000 --> 00:18:07,240
And by October, gain like double the amount of endorsers and be able to call it version

211
00:18:07,240 --> 00:18:09,040
1.0.

212
00:18:09,040 --> 00:18:15,680
And because version 1.0 will be basically feature complete license that enough organizations

213
00:18:15,680 --> 00:18:22,680
from a variety of interests will be supporting it and be able to endorse it.

214
00:18:22,680 --> 00:18:25,120
And say we're going to be using it.

215
00:18:25,120 --> 00:18:32,860
But then after version 1, we know that there's going to be more work to be done.

216
00:18:32,860 --> 00:18:43,400
So in terms of stakeholders that we want to have in the rooms to work with us on the definition,

217
00:18:43,400 --> 00:18:47,760
we need to find a way to engage with policymakers.

218
00:18:47,760 --> 00:18:59,120
That is we have some contacts and we need to have a little bit more of conversations

219
00:18:59,120 --> 00:19:04,760
with people who are working in the government space, in the policymaking space.

220
00:19:04,760 --> 00:19:13,360
Even though of course regulators will not be giving comments to us.

221
00:19:13,360 --> 00:19:14,360
People who write the legislation.

222
00:19:14,360 --> 00:19:19,240
And we will not engage with people who write legislation directly because we cannot as

223
00:19:19,240 --> 00:19:23,080
an American.

224
00:19:23,080 --> 00:19:32,120
But we want to hear from the people who are in this space.

225
00:19:32,120 --> 00:19:39,120
Because we notice that there is legislators are concerned about abuse of AI.

226
00:19:39,120 --> 00:19:45,880
And there is already starting to emerge a vision that open source AI or open AI widely

227
00:19:45,880 --> 00:19:58,680
available models and AI are capable of influencing elections or creating havoc in the society

228
00:19:58,680 --> 00:19:59,680
in general.

229
00:19:59,680 --> 00:20:08,200
So we need to make sure that whatever definition comes out of this process is not seen as a

230
00:20:08,200 --> 00:20:11,360
threat to society by regulators.

231
00:20:11,360 --> 00:20:15,860
That's something we need to be very careful about.

232
00:20:15,860 --> 00:20:20,280
And we need to explain this as we go.

233
00:20:20,280 --> 00:20:24,440
Understanding the problematics and solve them as soon as possible.

234
00:20:24,440 --> 00:20:27,560
And we need also to engage a lot with end users.

235
00:20:27,560 --> 00:20:36,400
So people who are like interacting with a bot at a bank.

236
00:20:36,400 --> 00:20:37,400
Or subjects.

237
00:20:37,400 --> 00:20:44,640
These are people who don't know that they're talking to an AI and they are affected by

238
00:20:44,640 --> 00:20:49,000
the automatic decisions, for example.

239
00:20:49,000 --> 00:20:50,600
We need to engage with them.

240
00:20:50,600 --> 00:20:58,720
So how a reminder, this is doesn't end with 1.0.

241
00:20:58,720 --> 00:21:04,640
We already started to define to think about what's going to be the future of 1.0.

242
00:21:04,640 --> 00:21:10,960
Like the open source initiative and its board has set up is setting up a new committee to

243
00:21:10,960 --> 00:21:19,240
brainstorm to think about the maintenance of this definition that is coming very quickly

244
00:21:19,240 --> 00:21:24,680
in a space that is evolving even faster, even more rapidly.

245
00:21:24,680 --> 00:21:32,160
So we need to prepare to catch up and to have a process to maintain the definition to keep

246
00:21:32,160 --> 00:21:34,800
it valid over time.

247
00:21:34,800 --> 00:21:40,960
And so what we're launching today is a new forum.

248
00:21:40,960 --> 00:21:47,040
We'll keep on doing these biweekly town halls and we'll keep on adding opportunities to

249
00:21:47,040 --> 00:21:50,000
volunteer to help out.

250
00:21:50,000 --> 00:21:55,520
We're working on a new version of the landing page to have the information all about this

251
00:21:55,520 --> 00:21:57,880
process all in one place.

252
00:21:57,880 --> 00:22:01,520
And we've done that.

253
00:22:01,520 --> 00:22:02,520
We're working on this.

254
00:22:02,520 --> 00:22:08,480
And as I said, setting up the board for managing the future.

255
00:22:08,480 --> 00:22:17,920
So as a big announcement for you is to the opening of the forums to have this conversation

256
00:22:17,920 --> 00:22:23,080
publicly online in order to join the forum.

257
00:22:23,080 --> 00:22:27,140
The forum uses the single sign on with other OSIs website.

258
00:22:27,140 --> 00:22:31,840
So you can you need to become a member of OSI.

259
00:22:31,840 --> 00:22:33,880
That is a free member.

260
00:22:33,880 --> 00:22:34,880
We have three tiers.

261
00:22:34,880 --> 00:22:35,880
There is a free membership.

262
00:22:35,880 --> 00:22:39,560
So you don't have to worry about having to pay.

263
00:22:39,560 --> 00:22:46,600
Or if this is an opportunity for you to support this work, which is very important, you can

264
00:22:46,600 --> 00:22:54,800
become a full member and donate to us from $50 a year and up.

265
00:22:54,800 --> 00:22:59,600
And on the forums you will find the links to the latest drafts.

266
00:22:59,600 --> 00:23:07,200
We will keep on asking questions on the forums.

267
00:23:07,200 --> 00:23:13,240
We'll keep on having the conversations we're having here constantly on the forums.

268
00:23:13,240 --> 00:23:20,960
And with that, I will I see that there are some questions on the chat.

269
00:23:20,960 --> 00:23:24,920
If you prefer, I can unmute all of you.

270
00:23:24,920 --> 00:23:30,840
You can also take the mic and speak.

271
00:23:30,840 --> 00:23:34,040
So I'll answer the question from Dirk.

272
00:23:34,040 --> 00:23:41,000
What licenses are required for the data and software components in the AI system?

273
00:23:41,000 --> 00:23:44,200
So we'll be talking about it.

274
00:23:44,200 --> 00:23:51,360
From the software components, it's I think that the answer is going to be quite easy.

275
00:23:51,360 --> 00:23:58,240
Anything that is recognizably software, we need to use licenses that have been approved

276
00:23:58,240 --> 00:24:01,480
by the open source definition.

277
00:24:01,480 --> 00:24:08,760
For the data component, it's we'll have to have conversations around that.

278
00:24:08,760 --> 00:24:15,800
I think we are working with organizations like Creative Commons and I mentioned the

279
00:24:15,800 --> 00:24:18,080
Linux Foundation AI Data.

280
00:24:18,080 --> 00:24:21,560
They have their licenses.

281
00:24:21,560 --> 00:24:27,960
There are different licenses that qualify as open data.

282
00:24:27,960 --> 00:24:35,000
The open data world has a different culture than the open source movement and the open

283
00:24:35,000 --> 00:24:37,400
source world.

284
00:24:37,400 --> 00:24:43,680
The data people, I mean, the people who have been dealing with open data that I met, I'm

285
00:24:43,680 --> 00:24:51,560
not sure how much they have been thinking about the fact that their data is actionable.

286
00:24:51,560 --> 00:24:56,160
We need to understand with them, we need to work with them to understand exactly what

287
00:24:56,160 --> 00:25:04,440
they think of their space once the space of open data, once it becomes actionable just

288
00:25:04,440 --> 00:25:05,780
like software.

289
00:25:05,780 --> 00:25:15,660
In other words, to make a pretty simple example, not many data sets out there are maintained

290
00:25:15,660 --> 00:25:20,860
in a way that they can be modified and fixed.

291
00:25:20,860 --> 00:25:28,380
One of the latest examples that I noticed is that in an open data set that has been

292
00:25:28,380 --> 00:25:38,540
used to train a lot of the large language models had reported, I mean, someone analyzed

293
00:25:38,540 --> 00:25:47,860
it years ago and found that it contained a lot of images that were illegal in many parts

294
00:25:47,860 --> 00:25:58,660
of the world and absolutely abhorrent, including child porn and atrocious material.

295
00:25:58,660 --> 00:26:08,540
Now the project that the people, the group that maintains this data set, which is open,

296
00:26:08,540 --> 00:26:13,300
never received a public issue.

297
00:26:13,300 --> 00:26:16,940
This is on GitHub, this data set.

298
00:26:16,940 --> 00:26:23,540
It's a GitHub project and there was no issue filed, but there was a paper, a research paper

299
00:26:23,540 --> 00:26:28,540
filed that described the bad material in the data set.

300
00:26:28,540 --> 00:26:34,100
So I think that there is work that needs to be done into this community because they're

301
00:26:34,100 --> 00:26:40,700
completely different and they're probably inside a similar conundrum that we as open

302
00:26:40,700 --> 00:26:46,420
source groups are when it comes to AI.

303
00:26:46,420 --> 00:26:51,140
Our world is being disrupted, let's say, modified.

304
00:26:51,140 --> 00:27:00,420
I don't know if that explains it.

305
00:27:00,420 --> 00:27:03,420
Any other questions?

306
00:27:03,420 --> 00:27:05,420
Curiosities?

307
00:27:05,420 --> 00:27:07,420
Thanks, Kellen.

308
00:27:07,420 --> 00:27:12,420
Yeah, glad to see you.

309
00:27:12,420 --> 00:27:34,420
And I hope that we can, yeah, we can work together soon.

310
00:27:34,420 --> 00:27:41,420
Any other concerns, questions?

311
00:27:41,420 --> 00:28:04,140
I'll give you time to start playing with the forums and maybe if you have any technical

312
00:28:04,140 --> 00:28:23,340
issues, you can, we can do, play with it.

313
00:28:23,340 --> 00:28:24,340
All right.

314
00:28:24,340 --> 00:28:25,340
No more questions.

315
00:28:25,340 --> 00:28:26,340
I'm going to stop the recording.

316
00:28:26,340 --> 00:28:27,180
Thanks.

317
00:28:27,180 --> 00:28:28,180
Bye.

### End of last town hall held on 2024-01-26 ###

### Start of next town hall held on 2024-02-09 ###
--- Presentation for 2024-02-09 ---
OPEN SOURCE AI DEFINITION
Online public townhall
Feb 9, 2024

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

The objective for 2024

Open Source AI Deﬁnition
version 1.0

Deﬁnition of AI system

Preamble
Out of scope issues
4 freedoms
License checklist

What is Open Source AI
To be Open Source, an AI system needs to be available under
legal terms that grant the freedoms to:
● Use the system for any purpose and without having to ask
for permission.
● Study how the system works and inspect its components.
● Modify the system to change its recommendations,
predictions or decisions to adapt to your needs.
● Share the system with or without modiﬁcations, for any
purpose.

What is the preferred form to make
modiﬁcations to an AI system?

Getting the speciﬁcations
AI systems

As deﬁned by the
OECD.

List of
components

Legal
frameworks

Legal
documents

Checklist

What elements are
necessary to:
- use
- study
- modify
- share
an AI system?

For each artifact,
evaluate which
laws apply. Some
will be under
“Intellectual
Property” regimes,
some will be under
other regimes.

We’ll match the
components and
the identiﬁed legal
frameworks with
the terms of the
legal documents
already in use,
where available.

After repeating
this exercise
enough times,
we’ll be able to
generalize the
outcomes and
write the specs to
evaluate the
freedoms granted.

Report from the working groups
Analyzing Llama2 and Pythia

8

Participants (Llama2 WG)
●
●
●
●
●
●
●
●

✔ Stefano Maffulli -- Open Source Initiative (convener)
✔ Mer Joyce -- Do Big Good (facilitator)

✔ Bastien Guerry -- DINUM, French public

administration
✔ Ezequiel Lanza -- Intel
✔ Roman Shaposhnik -- Apache Software Foundation
✔ Davide Testuggine -- Meta
✔ Jonathan Torres -- Meta
Stefano Zacchiroli -- Polytechnic Institute of Paris

✔ = attended
All members participating in a personal capacity.

9

Participants (Pythia wg)
●
●
●
●
●
●

Stefano Maffulli -- Open Source Initiative (convener)
Seo-Young Isabelle Hwang (Samsung)
Cailean Osborne (Researcher, Linux Foundation)
Stella Biderman (Eleuther AI)
Justin Colannino (Microsoft)
Aviya Skowron (Eleuther AI)

All members participating in a personal capacity.

10

Purpose
● Process -- OSI has been convening a global
conversation to ﬁnd the deﬁnition of open source AI for
almost two years.
● Track -- The 2024 objective scope for Track 1: System
Testing is to discover what components need to be
available in each AI system for the whole system to be
studied, used, modiﬁed, and shared. We plan to
complete this track at the latest by May.
● Working group report -- objective is to talk through
initial points of difference on what components of
Llama2, Pythia would need to be open for the whole AI
system to be studied, used, modiﬁed, and shared.
11

Framing
● Document – We’ll review the components table in the
Llama 2 specs doc and decide which exist in that AI
system, with a focus on resolving disagreement.
● Expectations – We’ll see how much of the table we get
through. (Insights on tempo and pace will be among
the learnings from this meeting.)
● Anything else? – Are there any other expectations or
framings we should put in place before we begin
working through the components table?
● Deadline - Feb 16 publish Llama2 and Pythia
12

Analysis of LLaMA2
Code All code used to parse and process
data, including:

Required to
Use?

Required
to Study?

Required to
Modify?

Data preprocessing code

SZ

SZ EL

Training code

SZ

SZ

Required to
Share?

Test code
Code used to perform inference for benchmark
tests
Validation code
Inference code

SZ
SM EL DT
SM JT SZ

SZ

SZ

SZ

SZ

Evaluation code
Other libraries or code artifacts that are part of
the system, such as tokenizers and
hyperparameter search code, if used.

BG,EL, SM,
SZ

SZ

13

Analysis of LLaMA2

Data All data sets, including:
Training data sets

Required to
Use?

Required
to Study?

Required to
Modify?

SZ

SZ

Testing data sets

SZ

Validation data sets

SZ

Required to
Share?

Benchmarking data sets
Data card
Evaluation data
Evaluation metrics and results
All other data documentation

SZ

SZ

14

Analysis of LLaMA2
Model All model elements, including:

Required to
Use?

Model architecture
Model parameters

SM, SZ, JT

Model card

EL

Required to
Study?

Required to Modify? Require
d to
Share?

SZ

SZ

SZ

SZ

Required to
Study?

Required to Modify? Require
d to
Share?

SZ

Sample model outputs
Other Any other documentation or tools produced or
used, including:

Required to
Use?

Research paper
Usage documentation

SZ

Technical report
Supporting tools

15

Analysis of Pythia
Code All code used to parse and process
data, including:

Required to
Use?

Required
to Study?

Required to
Modify?

Required to
Share?

Data preprocessing code

SH

SB SH CO

SB SH CO

SH

Training code

SH

SB SH CO

SB SH CO

SH

Test code

SH

SB SH CO

Code used to perform inference for benchmark
tests

SB SH CO

Validation code

SB SH CO

Inference code

SB SH

Evaluation code
Other libraries or code artifacts that are part of
the system, such as tokenizers and
hyperparameter search code, if used.

SH

SH

SH

SB SH CO
SB CO

SB SH CO

SB CO

16

Analysis of Pythia

Data All data sets, including:

Required to
Use?

Required
to Study?

Training data sets

SH

SB SH CO

Testing data sets

SH

SB SH CO

Validation data sets

SB SH CO

Benchmarking data sets

SB SH CO

Data card

SB SH ?

Evaluation data

SB SH CO

Evaluation metrics and results

SB SH CO

All other data documentation

SB SH CO

Required to
Modify?

Required to
Share?

17

Analysis of Pythia
Model All model elements, including:

Required to
Use?

Required to
Study?

Required to Modify? Require
d to
Share?

Model architecture

SB SH CO

SB SH CO

SB SH CO

SB SH
CO

Model parameters

SB SH CO

SB SH CO

SB SH CO

SB SH
CO

SB

SB

Model card
Sample model outputs

SH

Other Any other documentation or tools produced or
used, including:
Research paper
Usage documentation
Technical report
Supporting tools

SB

SB

18

Important questions on the forums
◦
◦
◦

The question of data
Is the OECD deﬁnition too broad?
Does the “Share” verb need clariﬁcation?

19

Next steps

20

Recruiting volunteers
- Review and validate the list of components
- Analyze other AI systems
(BLOOM, OpenCV …)

21

2024 timeline

System testing work stream
Stakeholder consultation work stream
Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

Draft 0.0.8

Townhall +

… October
Monthly Virtual
Meetings

Release
version 1.0

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

v. 1.0

Criteria for RC1 and v. 1.0
RC1

version 1

- Expected outcome of
in-person meeting end
May/early June!
- The draft is completed
in all its parts
- The draft is supported
by at least 2
representatives for
each of the 6
stakeholder groups

- Expected outcome of
in-person and online
meetings through the
summer/early autumn
- The draft is endorsed by at
least 5 reps for each of the
stakeholder groups
- Announced in late October

Help us ﬁnd stakeholders
System Creator

License Creator

Regulator

Licensee

End User

Subject

Makes AI system
and/or
component that
will be studied,
used, modified,
or shared
through an open
source license
(e.g., ML
researcher in
academia or
industry)

Writes or edits
the open source
license to be
applied to the AI
system or
component;
includes
compliance
(e.g., IP lawyer)

Writes or edits
rules governing
licenses and
systems (e.g.
government
policy-maker)

Seeks to study,
use modify, or
share an open
source AI
system (e.g. AI
engineer, health
researcher,
education
researcher)

Consumes a
system output,
but does not
seek to study,
use, modify, or
share the
system (e.g.,
student using a
chatbot to write
a report, artist
creating an
image)

Affected
upstream or
downstream by
a system output
without
interacting with it
intentionally;
includes
advocates for
this group (e.g.
people with loan
denied, or
content creators)

✅

✅

⚠

✅

⚠

⚠

Enough to start

Enough to start

Leads to US, EU,
Singapore, no
commitment yet

Enough to start

Which org is squarely in
this space?

ACLU, Algorithmic
Justice League

It doesn’t end with v. 1.0
We’ll need to deﬁne rules for maintenance and review of the
Deﬁnition

OSI’s immediate next steps
- more publicity to the process
- public discussion forum https://discuss.opensource.org
- bi-weekly townhalls
- more opportunities to volunteer
- update project landing page
- reach out to more stakeholders
- raise funds for 2024 meetings
- setup the board for review and approval of v. 1.0

Join the conversation
● Public forum
● Join as OSI member
○ Free or full
○ SSO with other
OSI websites

27

Draft v. 0.0.5 of the Open Source AI Deﬁnition
Open to public comments

https://opensource.org/deepdive/drafts

28

Closing

37

Debrief
● Reﬂection – How did that discussion go? Were we
able to address areas of disagreement in a
meaningful way? If so, how? If not, why not?
● Adaptation – How might we change the structure of
this meeting? How can we improve our review method
for other AI systems?
● Next Steps – How to continue to resolve
disagreements? Another synchronous meeting?
Asynchronous commenting or other method? How
would you personally like to be involved?
38

Thank you
We realize this is difficult work and we appreciate
your help and openness, both in analyzing this
system and improving the deﬁnitional process.

39


--- Subtitles for 2024-02-09 --- ###
1
00:00:00,001 --> 00:00:05,120
All right.

2
00:00:05,120 --> 00:00:09,760
Thanks for joining this panel meeting again.

3
00:00:09,760 --> 00:00:10,760
Sorry for my voice.

4
00:00:10,760 --> 00:00:21,120
This is the result of a week of conversations in Brussels in very loud environments.

5
00:00:21,120 --> 00:00:25,520
I think I stressed out my vocal cords a little bit too much.

6
00:00:25,520 --> 00:00:32,520
And I caught the false debug, which is not COVID.

7
00:00:32,520 --> 00:00:34,820
Just a little cold.

8
00:00:34,820 --> 00:00:43,040
So the purpose of these meetings is to keep the tempo and get live conversations and live

9
00:00:43,040 --> 00:00:51,040
updates on the most important things that have happened in the past couple of weeks.

10
00:00:51,040 --> 00:01:00,640
And let's remind everyone our principles under which we operate.

11
00:01:00,640 --> 00:01:04,360
We try to have -- make sure that one person speaks at a time.

12
00:01:04,360 --> 00:01:07,000
There's no crowds around Mike.

13
00:01:07,000 --> 00:01:11,040
Try to make space for others.

14
00:01:11,040 --> 00:01:13,840
If you tend to be quiet, speak up.

15
00:01:13,840 --> 00:01:19,600
You can use the buttons to raise your hands in this meeting, but you can also type if

16
00:01:19,600 --> 00:01:21,680
you prefer not to speak up.

17
00:01:21,680 --> 00:01:23,880
But please give feedback.

18
00:01:23,880 --> 00:01:28,640
This is the best place to have quick interactions.

19
00:01:28,640 --> 00:01:29,640
Let's use them.

20
00:01:29,640 --> 00:01:36,160
And I don't think we need to be stressing the fact that we want everyone to be nice.

21
00:01:36,160 --> 00:01:39,320
And keep in mind, we need to keep moving.

22
00:01:39,320 --> 00:01:41,480
We need to finish this process.

23
00:01:41,480 --> 00:01:47,120
And if we face an obstacle, we move around it and we should be getting back to it later

24
00:01:47,120 --> 00:01:52,120
rather than stop it and admiring how big and insurmountable it is.

25
00:01:52,120 --> 00:01:53,840
And we need to focus on solutions.

26
00:01:53,840 --> 00:01:57,400
It's a multi-stakeholder, co-design process.

27
00:01:57,400 --> 00:02:01,560
It's basically pioneer work for us.

28
00:02:01,560 --> 00:02:05,600
And we know there are a lot of things that don't work that can be done better.

29
00:02:05,600 --> 00:02:11,080
But we need to focus on what actually works and keep on moving.

30
00:02:11,080 --> 00:02:18,080
Are there anything else that we need to take care of?

31
00:02:18,080 --> 00:02:21,280
All right.

32
00:02:21,280 --> 00:02:23,800
Reminder.

33
00:02:23,800 --> 00:02:26,440
I was wondering whether to keep this slide or not.

34
00:02:26,440 --> 00:02:32,280
But I think I want to remind everyone that our objective is to have an open source AI

35
00:02:32,280 --> 00:02:37,280
definition that is workable, that is good enough by the end of the year.

36
00:02:37,280 --> 00:02:38,280
It's really important.

37
00:02:38,280 --> 00:02:45,560
And everyone is asking, not only is asking for one, but I think that we really have this

38
00:02:45,560 --> 00:02:52,120
responsibility to create, to come to an agreement, to agree on something.

39
00:02:52,120 --> 00:02:54,640
And notice the version number 1.0.

40
00:02:54,640 --> 00:02:58,760
It's not necessarily going to be the most perfect one.

41
00:02:58,760 --> 00:03:03,240
We will always be able to fix it.

42
00:03:03,240 --> 00:03:06,640
And a reminder for what we have so far.

43
00:03:06,640 --> 00:03:13,960
We have a definition of AI systems in a document, version 5.05.

44
00:03:13,960 --> 00:03:21,760
And we have basic preambles for the basic principles of why we need an open source AI.

45
00:03:21,760 --> 00:03:30,040
And we may want to have this wording also reviewed and straight up as quickly as possible.

46
00:03:30,040 --> 00:03:33,760
Because that's another question that I get asked often.

47
00:03:33,760 --> 00:03:36,280
Why do we need open source AI?

48
00:03:36,280 --> 00:03:37,280
Why is it important?

49
00:03:37,280 --> 00:03:46,160
I refer to this preamble, but I want to make sure that we are quoting that.

50
00:03:46,160 --> 00:03:49,840
The other piece is the what's Atascope?

51
00:03:49,840 --> 00:03:58,080
And there has been a little bit of discussion around what is Atascope.

52
00:03:58,080 --> 00:04:03,320
I encourage you to give feedback on this text, too.

53
00:04:03,320 --> 00:04:07,400
Because I don't want to, you know, I want to have it finalized as quickly as possible.

54
00:04:07,400 --> 00:04:10,520
I don't think that it's bad, necessarily.

55
00:04:10,520 --> 00:04:17,440
But we want to have a conclusion very quickly.

56
00:04:17,440 --> 00:04:18,440
Then we have the four freedoms.

57
00:04:18,440 --> 00:04:21,040
There isn't much debate around this right now.

58
00:04:21,040 --> 00:04:24,760
Although there are a couple of questions that I will highlight later.

59
00:04:24,760 --> 00:04:30,560
And what we're working on right now that is missing is this checklist of legal documents.

60
00:04:30,560 --> 00:04:38,280
And I give you an update now on this, on the work that we've been doing.

61
00:04:38,280 --> 00:04:44,640
So the four freedoms as described here are at the core of the open source definition

62
00:04:44,640 --> 00:04:45,640
of AI.

63
00:04:45,640 --> 00:04:52,360
And we're at the stage where we need to identify what are the preferred forms to make modifications

64
00:04:52,360 --> 00:04:54,640
to an AI system.

65
00:04:54,640 --> 00:04:59,800
And in the process, understand what we're going to be is highlighted here.

66
00:04:59,800 --> 00:05:02,320
And we're at the stage two.

67
00:05:02,320 --> 00:05:11,960
We have a list of components that we have identified thanks to the work done by Linux

68
00:05:11,960 --> 00:05:21,000
Foundation AI and Data Commons, Generative AI Commons Working Group.

69
00:05:21,000 --> 00:05:25,600
They have provided a list of components for machine learning systems.

70
00:05:25,600 --> 00:05:33,720
And we have been using that list of components to identify which of those components are

71
00:05:33,720 --> 00:05:41,440
required, they are a must have, in order to be able to use a system, to study a system,

72
00:05:41,440 --> 00:05:44,040
modify and share it.

73
00:05:44,040 --> 00:05:50,000
And then once we have this list with these matching points, we're going to be progressing

74
00:05:50,000 --> 00:05:58,240
on this, on this, on this line, on this timeline, where we're going to be checking whether the

75
00:05:58,240 --> 00:06:07,880
components have or fall under which legal frameworks, whether that's exclusive rights,

76
00:06:07,880 --> 00:06:16,960
exclusive rights like intellectual property, broad term, copyright, patents, secrets, or

77
00:06:16,960 --> 00:06:18,920
what have you.

78
00:06:18,920 --> 00:06:24,040
Or if they don't, what kind of legal frameworks they fall under.

79
00:06:24,040 --> 00:06:29,120
And then we're going to be looking at the licenses as a next step.

80
00:06:29,120 --> 00:06:35,880
And the licenses are legal documents, legal terms, with which they are distributed.

81
00:06:35,880 --> 00:06:37,800
We're going to be identifying gaps.

82
00:06:37,800 --> 00:06:44,840
And from those gaps, and from the list of legal documents, we're going to make a checklist

83
00:06:44,840 --> 00:06:52,040
to evaluate the freedoms in these legal documents.

84
00:06:52,040 --> 00:07:00,240
Now we started working with two groups, analyzing specifically Lama 2 and PTI as examples, two

85
00:07:00,240 --> 00:07:05,720
examples of generative AI machine learning systems.

86
00:07:05,720 --> 00:07:14,200
And the members of these groups are in this list, me and Mer, are almost basically observers

87
00:07:14,200 --> 00:07:16,360
and facilitators of the meeting.

88
00:07:16,360 --> 00:07:20,680
And we have experts of different, different capabilities.

89
00:07:20,680 --> 00:07:26,840
All of these people are working for a company, one way, shape, or form.

90
00:07:26,840 --> 00:07:31,160
But they're participating, not representing their company's views.

91
00:07:31,160 --> 00:07:35,240
They're representing us, experts.

92
00:07:35,240 --> 00:07:39,760
But of course, for transparency, we list their affiliation.

93
00:07:39,760 --> 00:07:44,240
So this is the Lama Working Group and PTI Working Group.

94
00:07:44,240 --> 00:07:50,720
It's a little bit smaller, but it includes people here from different parts of the world

95
00:07:50,720 --> 00:07:53,120
also.

96
00:07:53,120 --> 00:08:04,400
So with them, we have gone through, the process has been, we're at the point where we are

97
00:08:04,400 --> 00:08:06,480
at this working group report.

98
00:08:06,480 --> 00:08:09,840
They need to go through these documents.

99
00:08:09,840 --> 00:08:10,840
They're going through these documents.

100
00:08:10,840 --> 00:08:21,760
Let me share with you what's happening, what's happening in here.

101
00:08:21,760 --> 00:08:28,720
So for, we built this table, you may have seen it in draft five that I published at

102
00:08:28,720 --> 00:08:30,120
the beginning of last week.

103
00:08:30,120 --> 00:08:39,200
The draft five of the definition of open source CI has this table at the bottom, in which

104
00:08:39,200 --> 00:08:47,520
you can see the list of components, and then on each, on the first column.

105
00:08:47,520 --> 00:08:49,360
And then there are other four columns.

106
00:08:49,360 --> 00:08:57,040
Those four columns have, basically, they're going to be X marks, whether the component

107
00:08:57,040 --> 00:09:01,240
on the row is required, so it's mandatory.

108
00:09:01,240 --> 00:09:09,440
It must be available to use the system, to use LamaTo, or to study LamaTo, or to modify

109
00:09:09,440 --> 00:09:12,880
and share it, LamaTo, so individually.

110
00:09:12,880 --> 00:09:15,560
And the components are split into four categories.

111
00:09:15,560 --> 00:09:22,680
There is code, and this is what happens, what are the analysis done by the experts in the

112
00:09:22,680 --> 00:09:25,640
working group for LamaTo.

113
00:09:25,640 --> 00:09:32,600
And it looks like, in order to, the kind of code that we need to, there is pretty much

114
00:09:32,600 --> 00:09:41,840
consensus on the kind of code that needs to be available for using LamaTo is the inference

115
00:09:41,840 --> 00:09:50,400
code and the libraries, such as the tokenizer and hyperparameter search code, et cetera.

116
00:09:50,400 --> 00:09:53,760
So those are required to use, seem to be.

117
00:09:53,760 --> 00:09:58,320
In order to study, there is a little bit less participation of this group, like only one

118
00:09:58,320 --> 00:10:02,920
person so far has filled in the table.

119
00:10:02,920 --> 00:10:07,920
And it looks like training code and data pre-processing code and the other libraries are required

120
00:10:07,920 --> 00:10:11,360
to study and to modify.

121
00:10:11,360 --> 00:10:18,800
Similarly, there is little participation, but there are some boxes in here.

122
00:10:18,800 --> 00:10:27,800
Moving forward on the data front, doesn't look like any of the data is required to use

123
00:10:27,800 --> 00:10:36,280
the system, but SC is the initials of StackFano Zacchiroli.

124
00:10:36,280 --> 00:10:42,160
He's looked at the, he's left these comments that training data set and other data documentation

125
00:10:42,160 --> 00:10:45,400
is required to study.

126
00:10:45,400 --> 00:10:51,120
And similarly, testing and validation data set is required to modify the system, but

127
00:10:51,120 --> 00:10:53,560
not to use and share.

128
00:10:53,560 --> 00:11:00,600
And finally, on the use front, looks like there is pretty much consensus that model

129
00:11:00,600 --> 00:11:09,840
parameters is necessary to use it and study, modify, and share.

130
00:11:09,840 --> 00:11:18,040
And maybe usage documentation, according to StackFano Zacchiroli, is required to modify.

131
00:11:18,040 --> 00:11:24,840
And moving forward, Pithia, this one working group has done a little bit more work and

132
00:11:24,840 --> 00:11:27,000
it's a little more comprehensive.

133
00:11:27,000 --> 00:11:34,920
You can see that in order to study the data, it looks like there is a lot of boxes checked

134
00:11:34,920 --> 00:11:40,440
here, which is very interesting to see.

135
00:11:40,440 --> 00:11:52,560
Some data is even necessary, according to Sarah Young, in order to run, to use the system.

136
00:11:52,560 --> 00:12:02,320
It's going to be interesting to see, to have for her, the rationale behind this decision.

137
00:12:02,320 --> 00:12:11,760
And on the data front, sorry, that was the code front, yeah, okay, so on the code piece.

138
00:12:11,760 --> 00:12:23,880
On the data front, some data required to use, but there is a unanimity that, again, a lot

139
00:12:23,880 --> 00:12:31,000
of data is required to study, which is also very interesting and needs to be investigated

140
00:12:31,000 --> 00:12:32,640
further.

141
00:12:32,640 --> 00:12:40,720
And when we build a model for execution and model architecture and parameters seem to

142
00:12:40,720 --> 00:12:46,600
be necessary, but also modify and share.

143
00:12:46,600 --> 00:12:50,400
So this work is making progress.

144
00:12:50,400 --> 00:12:57,520
We are, we have scheduled two more meetings with each of these groups next week, and we're

145
00:12:57,520 --> 00:13:06,440
going to drive for completing these cards by Friday, so that we can have two complete

146
00:13:06,440 --> 00:13:12,080
analyses and publish them for a wider conversation on the forums.

147
00:13:12,080 --> 00:13:20,840
This is going to be a very major milestone for us, and a very good, important result.

148
00:13:20,840 --> 00:13:28,520
Now, the forums that we launched, we launched them, was it last week?

149
00:13:28,520 --> 00:13:35,520
They already contain a lot of interesting conversations, but I wanted to highlight three

150
00:13:35,520 --> 00:13:45,640
of them that I think are very crucial to have some sort of, to have a, to drive towards

151
00:13:45,640 --> 00:13:52,480
a conclusion so we can release a T in three weeks, four weeks, we can release a new draft

152
00:13:52,480 --> 00:13:54,960
of the definition.

153
00:13:54,960 --> 00:14:02,080
And I think that the top and most important one is the conversation around data, and you

154
00:14:02,080 --> 00:14:03,320
will see it on the forum.

155
00:14:03,320 --> 00:14:10,600
This is one of the ones with the highest amount of comments on it, I think.

156
00:14:10,600 --> 00:14:19,800
It's worth keeping an eye out in there, because I think we need to come close to a conclusion

157
00:14:19,800 --> 00:14:26,880
very soon, or at least to try to understand what the consensus is, or if there is no consensus,

158
00:14:26,880 --> 00:14:32,640
we need to highlight why, and what are the reasons, the main reasons for that lack of

159
00:14:32,640 --> 00:14:36,360
consensus, the controversial part.

160
00:14:36,360 --> 00:14:40,800
It's very crucial in there.

161
00:14:40,800 --> 00:14:48,880
And the other thing, the other conversation we have ongoing is that I think is important,

162
00:14:48,880 --> 00:14:57,400
it is the one on the definition of AI system, that right now we've been using the one provided

163
00:14:57,400 --> 00:15:06,880
by the organization for economic cooperation and development, the OECD.

164
00:15:06,880 --> 00:15:16,640
There are a couple of comments, one by Richard Fontana, but also others, that, arguing that

165
00:15:16,640 --> 00:15:25,200
the definition by the OECD is too wide, and too, that covers pretty much everything.

166
00:15:25,200 --> 00:15:27,920
Everything digital.

167
00:15:27,920 --> 00:15:32,000
And we may want to revise it.

168
00:15:32,000 --> 00:15:37,200
So I don't have a strong attachment to that definition, or any other definition, but we

169
00:15:37,200 --> 00:15:45,240
need to have a definition of AI system, because the open source AI needs to refer to a system,

170
00:15:45,240 --> 00:15:50,160
and not to individual components, or pieces.

171
00:15:50,160 --> 00:15:54,240
We need to have a framework of reference that we can tie to.

172
00:15:54,240 --> 00:16:03,120
I go back to explaining that the open source definition for software refers to programs.

173
00:16:03,120 --> 00:16:10,760
And programs, even though they're not defined either, but pretty much everyone knows what

174
00:16:10,760 --> 00:16:18,000
they are, the discipline, and the software, the computer science is old enough that we

175
00:16:18,000 --> 00:16:20,760
know a program when we see it.

176
00:16:20,760 --> 00:16:25,560
For AI, I don't think we have that luxury yet, and we need to be able, we need to be

177
00:16:25,560 --> 00:16:28,120
a little bit more specific.

178
00:16:28,120 --> 00:16:36,920
So if anyone knows of different, better, well understood definitions of AI systems that

179
00:16:36,920 --> 00:16:45,080
we can use and reuse, so please go and make suggestions on that thread.

180
00:16:45,080 --> 00:16:51,160
There is another one that is very interesting to me, at least, and there's a conversation

181
00:16:51,160 --> 00:17:03,200
around the meaning of the verb share, because there is an argument being made that the sharing

182
00:17:03,200 --> 00:17:08,320
needs to be clarified, that we can share the systems with the same conditions under which

183
00:17:08,320 --> 00:17:13,080
we have, under the same legal conditions for which we have received it.

184
00:17:13,080 --> 00:17:19,720
I, you know, it's a very quite legally type of question.

185
00:17:19,720 --> 00:17:27,240
I'm not exactly sure I understand where that conversation, that question is coming from,

186
00:17:27,240 --> 00:17:37,480
but I see people involved in it, and I would recommend someone looks at it and tries to

187
00:17:37,480 --> 00:17:46,360
explain or tries to find a converging solution.

188
00:17:46,360 --> 00:17:48,440
So what are the next steps?

189
00:17:48,440 --> 00:17:53,000
And maybe, let me see, I see a question in here.

190
00:17:53,000 --> 00:17:57,160
Nick, the result for Lama2 and PTA, should they be similar?

191
00:17:57,160 --> 00:18:01,520
Eventually, yeah, I mean, probably they will not, because they're different people making

192
00:18:01,520 --> 00:18:02,520
different evaluations.

193
00:18:02,520 --> 00:18:09,880
They should be similar, because they're similar systems, similar architecture, similar things.

194
00:18:09,880 --> 00:18:14,680
They should be similar, but if they don't, then that's what the process is going to be

195
00:18:14,680 --> 00:18:15,680
like.

196
00:18:15,680 --> 00:18:19,880
We're going to have to have a conversation once we have also Bloom, for example, the

197
00:18:19,880 --> 00:18:27,840
three of them, we will have to find a way to identify the diversity and why, explain

198
00:18:27,840 --> 00:18:32,560
why things are different, and drive towards a conclusion.

199
00:18:32,560 --> 00:18:40,600
I think a lot of the work is going to be around explaining exactly what access means.

200
00:18:40,600 --> 00:18:50,120
I'm sensing from conversations I've had with multiple people that the concept of access

201
00:18:50,120 --> 00:18:58,240
to the data, access to the architecture, access to the documentation is different.

202
00:18:58,240 --> 00:19:00,240
Sorry.

203
00:19:00,240 --> 00:19:06,120
Sorry for that.

204
00:19:06,120 --> 00:19:17,200
All right, so what are we doing next?

205
00:19:17,200 --> 00:19:25,840
We started recruiting people, but we need to review this list of components and the

206
00:19:25,840 --> 00:19:36,560
checklists that the two working groups on PTA and Lama2 have started, and also we started

207
00:19:36,560 --> 00:19:46,920
recruiting people to analyze Bloom OpenCV, and we may even need probably will recruit

208
00:19:46,920 --> 00:19:52,880
for more other systems if necessary.

209
00:19:52,880 --> 00:19:58,200
But at least we want to look at OpenCV as a curiosity mostly because it's a non-generative

210
00:19:58,200 --> 00:20:07,000
AI just to validate that list of components down the line.

211
00:20:07,000 --> 00:20:12,560
That's for the next couple of weeks' work.

212
00:20:12,560 --> 00:20:17,640
And just reminding everyone, the timeline is here.

213
00:20:17,640 --> 00:20:21,520
We have draft five released in February as scheduled.

214
00:20:21,520 --> 00:20:24,320
We're going to be releasing 06.

215
00:20:24,320 --> 00:20:33,280
We keep on going with these virtual meetings, town halls, and work group activities, trying

216
00:20:33,280 --> 00:20:40,880
to speed up things so that we can get in end of May, early June with an in-person meeting

217
00:20:40,880 --> 00:20:46,480
that will eventually resolve the last controversies and issue a release candidate.

218
00:20:46,480 --> 00:20:48,840
This is a very aggressive timeline.

219
00:20:48,840 --> 00:20:51,240
I keep on stressing this out.

220
00:20:51,240 --> 00:20:58,520
We need to keep the tempo, and that's why I'm putting so much energy into this.

221
00:20:58,520 --> 00:21:05,440
Once we have the release candidate, the idea is to take it in a roadshow around the world.

222
00:21:05,440 --> 00:21:12,080
We have found already three partner conferences in different ways in different parts of the

223
00:21:12,080 --> 00:21:21,040
world where we can host a presentation and a review of the release candidate.

224
00:21:21,040 --> 00:21:27,840
And so we get to number version one in October.

225
00:21:27,840 --> 00:21:35,480
The criteria for respectively release candidate and version one is to have representatives

226
00:21:35,480 --> 00:21:42,000
from each of the stakeholders to support it, and all of this information is public now

227
00:21:42,000 --> 00:21:45,640
on our website.

228
00:21:45,640 --> 00:21:53,600
And these are the categories of stakeholders where we would be putting logos in here as

229
00:21:53,600 --> 00:21:56,600
we go.

230
00:21:56,600 --> 00:22:01,800
And that's the other reminder that I want everyone to keep in mind.

231
00:22:01,800 --> 00:22:03,360
It's not going to be perfect.

232
00:22:03,360 --> 00:22:12,560
And the board of the OSI is already working on setting up a committee that will be looking

233
00:22:12,560 --> 00:22:18,440
into the review and approval of version one when it comes out, because it's going to be

234
00:22:18,440 --> 00:22:27,560
the board's purview to review and finally approve the work of the community and take

235
00:22:27,560 --> 00:22:33,760
over the maintenance of this definition, because this is going to be the first version, the

236
00:22:33,760 --> 00:22:42,480
first definition maintained by the OSI with a version number in it.

237
00:22:42,480 --> 00:22:49,080
And yeah, we're working very hard to make sure that we have the funding to support all

238
00:22:49,080 --> 00:22:50,080
of this.

239
00:22:50,080 --> 00:22:51,800
So but I'm crossing fingers.

240
00:22:51,800 --> 00:22:54,800
I think we're going to be okay.

241
00:22:54,800 --> 00:22:59,800
And if I run out of funds, I'm going to let you know in advance.

242
00:22:59,800 --> 00:23:02,040
And we got the forums.

243
00:23:02,040 --> 00:23:03,880
I'm pretty excited about this.

244
00:23:03,880 --> 00:23:07,680
So they're easy to sign in.

245
00:23:07,680 --> 00:23:19,440
If you have already a member, log in for any of the open source initiatives, websites,

246
00:23:19,440 --> 00:23:22,880
and opensource.net, it's going to work seamlessly.

247
00:23:22,880 --> 00:23:28,840
If not, you can register, become a free member, or now is also a good time to sign up and

248
00:23:28,840 --> 00:23:34,800
become a full member so that you can vote also at the next election that are coming

249
00:23:34,800 --> 00:23:37,840
up for the board.

250
00:23:37,840 --> 00:23:40,600
And we're also draft 05.

251
00:23:40,600 --> 00:23:45,160
Since last time we spoke at this time, it's a new thing.

252
00:23:45,160 --> 00:23:49,840
So go and look at the latest draft also.

253
00:23:49,840 --> 00:23:54,640
And leave your comments on the you can leave comments directly on the draft, or you can

254
00:23:54,640 --> 00:23:57,760
leave comments on the forums if they're more generic.

255
00:23:57,760 --> 00:24:02,480
If it's specific for, you know, I want to change this word, I would recommend leave

256
00:24:02,480 --> 00:24:06,560
the conversation on the draft.

257
00:24:06,560 --> 00:24:13,000
But if it's more generic about and requires larger, more text and stuff like that, like

258
00:24:13,000 --> 00:24:16,240
leave it on the forum.

259
00:24:16,240 --> 00:24:20,480
And with that, I'm happy to take questions.

260
00:24:20,480 --> 00:24:25,440
Victor, public link to LamaTubePT session.

261
00:24:25,440 --> 00:24:29,560
No, well, actually not yet.

262
00:24:29,560 --> 00:24:44,160
Not yet, because we want to leave the groups to work a little bit more, you know, in peace

263
00:24:44,160 --> 00:24:51,240
and without having to be, you know, like under, what is it called?

264
00:24:51,240 --> 00:24:58,160
Under, you know, like a aquarium type of thing.

265
00:24:58,160 --> 00:25:06,720
We're going to make public everything as soon as the work is done.

266
00:25:06,720 --> 00:25:15,200
Matt is that correct as an answer?

267
00:25:15,200 --> 00:25:19,200
I cannot hear you.

268
00:25:19,200 --> 00:25:20,200
Sorry.

269
00:25:20,200 --> 00:25:27,520
I need to enable you.

270
00:25:27,520 --> 00:25:44,040
You should be able now to unblock your mic.

271
00:25:44,040 --> 00:25:45,040
And now you're muted.

272
00:25:45,040 --> 00:25:46,040
You need to unmute yourself.

273
00:25:46,040 --> 00:25:47,040
Thank you.

274
00:25:47,040 --> 00:25:50,720
It's only been three months, three years of pandemic.

275
00:25:50,720 --> 00:26:00,080
Yeah, so the groups themselves, they meet as small groups, but the membership is transparent.

276
00:26:00,080 --> 00:26:06,280
And then we report out all the documentation that's being created, which is the summary

277
00:26:06,280 --> 00:26:11,160
and also the tables that Stefano shared.

278
00:26:11,160 --> 00:26:13,840
And that really is the report.

279
00:26:13,840 --> 00:26:19,920
I think we're also planning to make the slides that the groups are working from public, which

280
00:26:19,920 --> 00:26:22,400
is effectively the agenda.

281
00:26:22,400 --> 00:26:30,520
So that's how we're balancing transparency and also people being able to have a meeting,

282
00:26:30,520 --> 00:26:31,960
which I think also has value.

283
00:26:31,960 --> 00:26:37,960
So yeah, open to any feedback.

284
00:26:37,960 --> 00:26:58,200
Thank you.

285
00:26:58,200 --> 00:27:00,260
you

286
00:28:00,220 --> 00:28:05,220
you

287
00:28:10,220 --> 00:28:13,220
you

288
00:28:13,220 --> 00:28:18,220
you

289
00:28:23,220 --> 00:28:25,280
you

290
00:28:25,280 --> 00:28:31,280
you

291
00:28:31,280 --> 00:28:33,340
you

### End of last town hall held on 2024-02-09 ###

### Start of next town hall held on 2024-02-23 ###
--- Presentation for 2024-02-23 ---
OPEN SOURCE AI DEFINITION
Online public townhall
Feb 23, 2024
last updated: Feb 22, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

The objective for 2024

Open Source AI Deﬁnition
version 1.0

3

Deﬁnition of AI system

Preamble
Out of scope issues
4 freedoms
License checklist

4

What is Open Source AI
To be Open Source, an AI system needs to be available under
legal terms that grant the freedoms to:
● Use the system for any purpose and without having to ask
for permission.
● Study how the system works and inspect its components.
● Modify the system to change its recommendations,
predictions or decisions to adapt to your needs.
● Share the system with or without modiﬁcations, for any
purpose.

5

Report from the workgroups

6

Workgroups
Llama 2
1.
2.
3.
4.
5.
6.

Bastien Guerry
DINUM, French
public administration
Ezequiel Lanza
Intel
Roman Shaposhnik
Apache Software
Foundation
Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute
of Paris

Pythia
BLOOM
1.
George C. G. Barbosa
1.
Seo-Young Isabelle
Fundação Oswaldo Cruz
Hwang Samsung
2.
Daniel Brumund GIZ
2.
Cailean Osborne
FAIR Forward - AI for all
University of Oxford,
3.
Danish Contractor
Linux Foundation
BLOOM Model Gov. WG
4.
Abdoulaye Diack
3.
Stella Biderman
Google
EleutherAI
5.
Deshni Govender GIZ
4.
Justin Colannino
FAIR Forward - AI for all
Microsoft
6.
Jaan Li University of
5.
Aviya Skowron
Tartu, Phare Health
EleutherAI
7.
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
8.
Ofentse Phuti WiMLDS
Gaborone
9.
Caleb Fianku Quao
Kwame Nkrumah
University of Science and
Technology, Kumasi

OpenCV
1.
2.
3.
4.
5.
6.
7.

8.
9.

Rahmat Akintola
Cubeseed Africa
Ignatius Ezeani
Lancaster University
Kevin Harerimana
CMU Africa
Satya Mallick
OpenCV
David Manset
ITU
Phil Nelson
OpenCV
Tlamelo Makati
WiMLDS Gaborone,
Technological
University Dublin
Minyechil Alehegn
Tefera Mizan Tepi
University
Akosua Twumasi
Ghana Health
Service

7

Recommendations summary

2/21/24

● Required
● Likely Not Required
○ Training, validation and
○ Evaluation code
testing code
○ Evaluation data
○ Inference code
○ Evaluation results
○ Model architecture
○ All other data
○ Model parameters
documentation
○ Supporting libraries and
○ Model metadata
tools
○ Model card
● Likely Required
○ Research paper
○ Data preprocessing code
○ Technical report
● Maybe Required
● Not Required
○ Datasets
○ Data card
○ Usage documentation
○ Sample model outputs

8

Methodology
○ Voting: by component (Llama 2 example) +
compilation overview
○ Emerging Results: recommendation rubric
■ Code: recommendations + detail
■ Data: recommendations + detail
■ Model: recommendations + detail
■ Other: recommendations + detail

Component voting (Llama 2 example)
Code All code used to parse and process
data, including:

Required to
Use?

Required
to Study?

Required to
Modify?

Data preprocessing code

SZ

SZ EL

Training code

SZ

SZ

Required to
Share?

Test code
Code used to perform inference for benchmark
tests
Validation code
Inference code

SZ
SM EL DT
SM JT SZ

SZ

SZ

SZ

SZ

Evaluation code
Other libraries or code artifacts that are part of
the system, such as tokenizers and
hyperparameter search code, if used.

BG,EL, SM,
SZ

SZ

(as of Feb. 9, 2024)
10

Vote compilation (overview)

As of 2/21/24 at 9:00 pm UTC

11

Recommendation rubric

*

As of 2/21/24 at 9:00 pm UTC

12

Code recommendations

As of 2/21/24 at 9:00 pm UTC

13

Code detail

As of 2/21/24 at 9:00 pm UTC

14

Data recommendations

As of 2/21/24 at 9:00 pm UTC

15

Data detail

As of 2/21/24 at 9:00 pm UTC

16

Model recommendations

As of 2/21/24 at 9:00 pm UTC

17

Model detail

As of 2/21/24 at 9:00 pm UTC

18

Other recommendations

As of 2/21/24 at 9:00 pm UTC

19

Other detail

As of 2/21/24 at 9:00 pm UTC

* Most votes come from a category titled "Other libraries
or code artifacts that are part of the system, such as
tokenizers and hyperparameter search code, if used."
20

Liesenfeld, A., Lopez, A. &
Dingemanse, M. 2023. “Opening up
ChatGPT: Tracking Openness,
Transparency, and Accountability in
Instruction-Tuned Text Generators.”
In CUI '23: Proceedings of the 5th
International Conference on
Conversational User Interfaces. July
19-21, Eindhoven. doi:
10.1145/3571884.3604316

21

Voting ends today @ 11:00pm UTC

22

Other updates

23

Focus narrowing on machine learning

● Narrowing the deﬁnitional scope from any AI
system to ML speciﬁcally
● Goal is to increase the accuracy and precision of
the deﬁnition we create
● Change will appear in version 0.0.6 this month
24

Questions from the forum
● In other words, the use of this “system”
terminology is a complication that may have the
effect of narrowing the perceived scope of what
the OSAID covers. Is the thought that the ordinary
OSD kicks in in cases where purportedly you don’t
have a “system”?
(Richard Fontana)
25

Next steps
● Final vote compilation next week
● Version 0.0.6 release early March

26

2024 timeline

System testing work stream
Stakeholder consultation work stream
Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

Draft 0.0.8

Townhall +

… October
Monthly Virtual
Meetings

Release
version 1.0

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

v. 1.0

27

Criteria for RC1 and v. 1.0
RC1

version 1

- Expected outcome of
in-person meeting end
May/early June!
- The draft is completed
in all its parts
- The draft is supported
by at least 2
representatives for
each of the 6
stakeholder groups

- Expected outcome of
in-person and online
meetings through the
summer/early autumn
- The draft is endorsed by at
least 5 reps for each of the
stakeholder groups
- Announced in late October

28

Help us ﬁnd stakeholders
System Creator

License Creator

Regulator

Licensee

End User

Subject

Makes AI system
and/or
component that
will be studied,
used, modified,
or shared
through an open
source license
(e.g., ML
researcher in
academia or
industry)

Writes or edits
the open source
license to be
applied to the AI
system or
component;
includes
compliance
(e.g., IP lawyer)

Writes or edits
rules governing
licenses and
systems (e.g.
government
policy-maker)

Seeks to study,
use modify, or
share an open
source AI
system (e.g. AI
engineer, health
researcher,
education
researcher)

Consumes a
system output,
but does not
seek to study,
use, modify, or
share the
system (e.g.,
student using a
chatbot to write
a report, artist
creating an
image)

Affected
upstream or
downstream by
a system output
without
interacting with it
intentionally;
includes
advocates for
this group (e.g.
people with loan
denied, or
content creators)

✅

✅

⚠

✅

⚠

⚠

Enough to start

Enough to start

Leads to US, EU,
Singapore, no
commitment yet

Enough to start

Which org is squarely in
this space?

ACLU, Algorithmic
Justice League

29

It doesn’t end with v. 1.0
We’ll need to deﬁne rules for maintenance and review of the
Deﬁnition

30

Join the conversation
● Public forum
● Join as OSI member
○ Free or full
○ SSO with other
OSI websites

31

Draft v. 0.0.5 of the Open Source AI Deﬁnition
Open to public comments

https://opensource.org/deepdive/drafts

32

Q&A

33

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the
deﬁnitional process.

34


--- Subtitles for 2024-02-23 --- ###
1
00:00:00,001 --> 00:00:26,120
Welcome to the fourth Town Hall. The scope of this Town Hall is to get people the chance

2
00:00:26,120 --> 00:00:36,240
to interact live with the process, ask questions, and get regular updates on what's happening

3
00:00:36,240 --> 00:00:45,880
and what's hot. So for the live meeting, just a few ground rules. We want to be giving space

4
00:00:45,880 --> 00:00:52,960
and being nice, listening to questions, but also make sure that people can ask questions

5
00:00:52,960 --> 00:00:58,000
and feel empowered to do so. But we also want to make sure that we're moving forward. We're

6
00:00:58,000 --> 00:01:06,520
not getting stuck debating forever. We need to make decisions and keep on going. And let's

7
00:01:06,520 --> 00:01:14,480
remind everyone that the objective for 2024 is to have a working open source AI definition.

8
00:01:14,480 --> 00:01:22,920
And by working, I mean it must be something that has endorsements from different groups

9
00:01:22,920 --> 00:01:32,320
and can be put in practice and allows for open source AI systems and components to exist.

10
00:01:32,320 --> 00:01:38,520
It's not something that is theoretical pie in the sky. We love it, but nobody can use

11
00:01:38,520 --> 00:01:47,320
it in practice. What we're working on is to have a definition that is made up of a few

12
00:01:47,320 --> 00:01:57,280
pieces. We have pretty decent understanding of all the pieces in here, minus some wordsmithing

13
00:01:57,280 --> 00:02:03,320
and clarifications, but the basic piece that we're still working on is the license or legal

14
00:02:03,320 --> 00:02:10,600
documents checklist at the bottom. So what happens, we have clarified that what is open

15
00:02:10,600 --> 00:02:18,560
source AI is basically something, it's referred to AI system. And the reason for it is that

16
00:02:18,560 --> 00:02:25,600
it gives us an anchor. It gives us a way to clarify what we're talking about, what freedoms

17
00:02:25,600 --> 00:02:31,680
we want to use. And it helps us run, drive the conversation around what do I need in

18
00:02:31,680 --> 00:02:39,360
order to use an AI system? What am I actually using? What is it that I want to study? What

19
00:02:39,360 --> 00:02:47,200
is it that I want to modify? What is it that I want to share? What kind of practical outputs

20
00:02:47,200 --> 00:02:58,080
or practical elements, examples I want to get out of this freedom, out of this verb?

21
00:02:58,080 --> 00:03:09,320
And so we've been running this exercise with four groups that have been split into analyzing

22
00:03:09,320 --> 00:03:22,240
four systems, Lama2, Bloom, BFIA, and OpenCV. Three of them, the first three are generative

23
00:03:22,240 --> 00:03:30,560
AI systems and OpenCV is non-generative. It's a computer vision program, computer vision

24
00:03:30,560 --> 00:03:39,040
framework, set of libraries with different algorithms. And some of the algorithms that

25
00:03:39,040 --> 00:03:47,120
OpenCV uses are neural networks and so machine learning based systems. And we wanted to see

26
00:03:47,120 --> 00:03:56,960
with a little bit of differentiation, if the requirements are different for non-generative

27
00:03:56,960 --> 00:04:05,760
AI systems so far. So you see the people who have volunteered to analyze the systems and

28
00:04:05,760 --> 00:04:11,300
there is a lot of diversity here, geographic representation from all over the place, from

29
00:04:11,300 --> 00:04:17,600
all places in the continent. There is academia represented, there are industry players, there

30
00:04:17,600 --> 00:04:30,200
is civil society interest in here too, and government organizations like the ITU. You

31
00:04:30,200 --> 00:04:35,400
can see a lot of diversity here. I'm really happy to see the involvement of the very wide

32
00:04:35,400 --> 00:04:42,720
breadth of people volunteering their time to contribute to this effort. I kept saying

33
00:04:42,720 --> 00:04:50,680
this from the beginning, this is not the effort defining something new like open source AI,

34
00:04:50,680 --> 00:04:56,880
having a new definition. The Open Source Initiative is not the work of a genius in a basement

35
00:04:56,880 --> 00:05:02,760
that comes out with a secret text. This is the work of a community that puts their brilliant

36
00:05:02,760 --> 00:05:08,640
minds together and drives towards consensus, drives towards a shared understanding of what

37
00:05:08,640 --> 00:05:16,320
open source AI means. That's what's going to make this valuable.

38
00:05:16,320 --> 00:05:29,760
So the summary, so these groups have been looking at the list of components that have

39
00:05:29,760 --> 00:05:40,080
been produced by a list of components, generically described in a working paper that is almost

40
00:05:40,080 --> 00:05:47,600
ready for publication by the Linux Foundation AI and Data Generative AI Commons Working

41
00:05:47,600 --> 00:05:54,880
Group or initiative inside the Linux Foundation AI and Data Foundation. They have produced

42
00:05:54,880 --> 00:06:01,040
a list of components that they want to use as a reference for judging the projects that

43
00:06:01,040 --> 00:06:06,920
are going to be hosted by the Linux Foundation. We've taken that same list of components,

44
00:06:06,920 --> 00:06:11,520
we passed it to the working groups, and each of them have been looking at the list of working

45
00:06:11,520 --> 00:06:17,160
groups and have been answering the question, what do I need in order to, which component

46
00:06:17,160 --> 00:06:28,920
is necessary, which component is required to train, to study, or to run a system or

47
00:06:28,920 --> 00:06:35,800
modify and share it? And a summary, very quick high-level view,

48
00:06:35,800 --> 00:06:42,280
is what are required are the training and evaluation and testing code, there is a requirement

49
00:06:42,280 --> 00:06:47,920
for inference code, model architecture, model parameters, and supporting libraries and tools.

50
00:06:47,920 --> 00:06:55,160
That looks like there's pretty good consensus. And then a few other things are on the likely

51
00:06:55,160 --> 00:07:03,120
required, maybe required, are data pre-processing code, data sets, and usage documentation.

52
00:07:03,120 --> 00:07:13,360
And then on the right-hand side, you see what's likely not to be required.

53
00:07:13,360 --> 00:07:23,360
We have asked the group to vote for each component. We compiled the votes into one document that

54
00:07:23,360 --> 00:07:31,760
I will show, and we split the results. This is an example of the table that the working

55
00:07:31,760 --> 00:07:38,680
group has been using as a reference. They've been filling in their initials in the cells.

56
00:07:38,680 --> 00:07:46,160
For each row, there is the component, and on the column side, there is yes or no. Is

57
00:07:46,160 --> 00:07:51,840
it required for using? Is it required to study, et cetera?

58
00:07:51,840 --> 00:08:01,440
Then we compiled, we're compiling the exercises. It's going to be done today, completed today.

59
00:08:01,440 --> 00:08:11,200
We're compiling this spreadsheet with the sum of the votes for each of the components,

60
00:08:11,200 --> 00:08:18,040
and we're going to grade it with a very simple algorithm. Basically, a yes if the median

61
00:08:18,040 --> 00:08:27,440
is higher than two votes, et cetera. I mean, yes, is the median higher? It's a yes. It's

62
00:08:27,440 --> 00:08:34,960
lower? It's a no. Pretty simple. This is what it's going to look like, and we're going to

63
00:08:34,960 --> 00:08:41,760
share this as the exercise completes today. I'm going to summarize and make it public

64
00:08:41,760 --> 00:08:45,600
next week on the forums.

65
00:08:45,600 --> 00:08:55,400
So, code, we can see what's required. As I mentioned, inference code is mostly likely

66
00:08:55,400 --> 00:09:01,960
on the data front. This is the interesting part. A lot of the groups are leaning on the

67
00:09:01,960 --> 00:09:12,760
maybe or maybe not. Maybe not. This is an important result and something that we'll

68
00:09:12,760 --> 00:09:19,160
have to spend a little bit of time debating as soon as the exercise finishes. My sense

69
00:09:19,160 --> 00:09:26,040
is that this is the crucial. We knew from the beginning that the conversations around

70
00:09:26,040 --> 00:09:32,320
data are crucial. Some of the live interactions that I've had with people in the working group

71
00:09:32,320 --> 00:09:44,840
have been along. I've been sort of describing what the highlighting, I've been highlighting

72
00:09:44,840 --> 00:10:00,360
to me the fact that definitely data is often, if not always, necessary, but the level of

73
00:10:00,360 --> 00:10:09,720
access is different in the mind of the practitioners. In other words, for example, during one of

74
00:10:09,720 --> 00:10:16,400
the conversations with one of the working groups, one of the questions from a volunteer

75
00:10:16,400 --> 00:10:28,040
was what level of depth do we want to organize or do we want to understand the verb study?

76
00:10:28,040 --> 00:10:34,800
So how deep do I want to be able to study a system? Because there is a very radical

77
00:10:34,800 --> 00:10:45,200
difference between studying a system because I need to write a PhD dissertation or if I

78
00:10:45,200 --> 00:10:51,480
need to study and understand it enough so that I can evaluate, for example, its transparency

79
00:10:51,480 --> 00:11:04,040
according to the regulation from the AI Act or a slightly different level of understanding.

80
00:11:04,040 --> 00:11:10,680
What level do I need to have if I want to only understand enough so that I can modify,

81
00:11:10,680 --> 00:11:21,000
retrain, improve, fine-tune a system? That, in my mind, means diving deeper with other

82
00:11:21,000 --> 00:11:30,760
conversations with some of these working groups has been highlighting how the level of access

83
00:11:30,760 --> 00:11:36,120
to the original dataset is also something that we need to clarify and we have an opportunity

84
00:11:36,120 --> 00:11:56,920
to clarify. How much do we need the dataset in full, like all the terabytes or petabytes

85
00:11:56,920 --> 00:12:03,600
of the original dataset or in some cases, some people have argued that it's enough to

86
00:12:03,600 --> 00:12:10,880
have a very good detailed description of what went into the dataset, maybe a randomized

87
00:12:10,880 --> 00:12:20,280
sample of the data inside the dataset. It could be sufficient to study, to modify, etc.

88
00:12:20,280 --> 00:12:27,440
This is a conversation that we'll have to dive deeper in the next iteration.

89
00:12:27,440 --> 00:12:35,360
The last thing that I want to say around the data issue is that a lot of the debates are

90
00:12:35,360 --> 00:12:44,760
also on the availability of data in general. Many of the practitioners that I talked to

91
00:12:44,760 --> 00:12:53,160
highlighted how not having access to data in general is a problem. The uncertainties

92
00:12:53,160 --> 00:13:01,240
around the regulations for text and data mining and the privacy laws and the very different

93
00:13:01,240 --> 00:13:05,760
implications of the copyright issues that have been raised in the United States and

94
00:13:05,760 --> 00:13:13,440
other places, they're all putting obstacles and this lack of clarity is blocking a lot

95
00:13:13,440 --> 00:13:18,120
of the freedoms that we want to unblock instead.

96
00:13:18,120 --> 00:13:28,520
I've been discussing with the people of the Open Future in Europe and we're starting to

97
00:13:28,520 --> 00:13:36,600
think about opening another separate conversation around data governance. This is something

98
00:13:36,600 --> 00:13:43,360
that might happen in the very near future where the data conversation will probably

99
00:13:43,360 --> 00:13:52,200
be spun off or another parallel conversation will start to talk about the availability

100
00:13:52,200 --> 00:13:56,920
of data.

101
00:13:56,920 --> 00:14:03,360
Moving on for the model recommendation, no surprise here, we know that we need to have

102
00:14:03,360 --> 00:14:09,840
the model architecture and the model parameters and unsure about the model metadata and model

103
00:14:09,840 --> 00:14:18,160
card. This is probably because there is not a lot of clarity over what those components

104
00:14:18,160 --> 00:14:30,440
actually mean because the paper from the Linux Foundation is fairly new still. The communities

105
00:14:30,440 --> 00:14:38,920
need more time to digest and understand what each individual component is.

106
00:14:38,920 --> 00:14:44,880
The other interesting thing is the groups have many, many people have voted yes on the

107
00:14:44,880 --> 00:14:57,520
supporting libraries and tools. That's because depending on how the supporting tools are

108
00:14:57,520 --> 00:15:04,360
described and the Linux Foundation has described them quite well in detail, they include in

109
00:15:04,360 --> 00:15:16,040
here things like hyperparameters, search code and tokenizers that most people consider necessary

110
00:15:16,040 --> 00:15:21,640
in order to interact, for example, with the system.

111
00:15:21,640 --> 00:15:29,280
This is the summary. This is where we are. It's interesting to see how we got very close

112
00:15:29,280 --> 00:15:43,000
to the framework from the research paper on openness in AI from this group and the group

113
00:15:43,000 --> 00:15:56,040
Liesenfeld and Dingelman. This is it. We're going to be ending the vote on the working

114
00:15:56,040 --> 00:16:04,960
group today. We're going to wrap it up and summarize the exercise.

115
00:16:04,960 --> 00:16:14,640
One thing that is quite becoming clear is that we're really focusing on machine learning

116
00:16:14,640 --> 00:16:21,320
right now. This is something that we expected when we started at the very beginning of the

117
00:16:21,320 --> 00:16:28,880
process that we were actually, this definition of open source AI is very close to, I mean

118
00:16:28,880 --> 00:16:37,200
very close. Right now, it's being driven by machine learning. We're going to be talking

119
00:16:37,200 --> 00:16:45,120
mostly about this. I'm not sure exactly how we're going to be dealing with this slight

120
00:16:45,120 --> 00:16:56,560
change, but it may be necessary to clarify this in the definitional documents. I still

121
00:16:56,560 --> 00:17:05,320
don't know exactly how to deal with this. Because there was a conversation months ago

122
00:17:05,320 --> 00:17:13,280
and so many people have argued that what we want to have is a definition for AI in general

123
00:17:13,280 --> 00:17:19,800
and not machine learning specifically. This is something that will be brought up again

124
00:17:19,800 --> 00:17:29,240
and we'll have that conversation. Then there is one highlight that there is an interesting

125
00:17:29,240 --> 00:17:36,600
question from the forum, very thought provoking from Richard Fontana. He's been driving the

126
00:17:36,600 --> 00:17:42,880
conversation around whether we need a definition that refers to open source, I mean to AI systems

127
00:17:42,880 --> 00:17:51,000
and not to its individual components. He makes a very compelling argument and I encourage

128
00:17:51,000 --> 00:18:01,200
you all to check on the forum and see the debate because one of my highlighting here

129
00:18:01,200 --> 00:18:10,480
is only one of the questions that Fontana raises. It is one of his latest messages.

130
00:18:10,480 --> 00:18:21,520
He's talking about whether the terminology may have the effect of narrowing or interacting

131
00:18:21,520 --> 00:18:35,400
in weird ways with the open source definition, the one that refers to software. My current

132
00:18:35,400 --> 00:18:44,920
line of thinking is that the definition of AI system is something that we have to introduce

133
00:18:44,920 --> 00:18:57,160
at the beginning because the conversation that we were having was going around in circles

134
00:18:57,160 --> 00:19:06,280
because we were talking about things like, "Oh, but what do I need to exercise the freedom

135
00:19:06,280 --> 00:19:12,600
to use a model?" People were thinking about data, were thinking about our components.

136
00:19:12,600 --> 00:19:19,240
It was really not clear. There was no clarity in there. As soon as we introduced the concept

137
00:19:19,240 --> 00:19:27,680
of AI system, then everything started to drive and to be more aligned. Now that we got into

138
00:19:27,680 --> 00:19:36,760
the deeper parts of the debate, we're starting to get a much clearer understanding of what

139
00:19:36,760 --> 00:19:43,200
is necessary to use and to study and run and what kind of components we're talking about.

140
00:19:43,200 --> 00:19:49,600
This is being formalized by the industry groups inside the Linux Foundation. There are other

141
00:19:49,600 --> 00:19:53,560
groups that are working in a similar fashion on listed components for machine learning

142
00:19:53,560 --> 00:20:02,800
systems. It's becoming more clear, that whole aspect. I'm not too married to any of these

143
00:20:02,800 --> 00:20:13,080
ideas. I do want to clarify and answer the question to Fontana.

144
00:20:13,080 --> 00:20:20,080
I believe that every single component – now that we have identified every single component,

145
00:20:20,080 --> 00:20:26,880
each of the components will have its own terms of use and terms of services. Some are going

146
00:20:26,880 --> 00:20:35,480
to be software code and those will be distributed and will be available with licenses like any

147
00:20:35,480 --> 00:20:43,040
other piece of software that we are very used to. Other components like model parameters,

148
00:20:43,040 --> 00:20:52,320
etc., they will be covered by other law and other legal frameworks. I don't think that

149
00:20:52,320 --> 00:20:56,160
there is going to be much of an interaction between the open source AI definition and

150
00:20:56,160 --> 00:21:00,040
the open source software definition. There's going to be interaction, but there's going

151
00:21:00,040 --> 00:21:05,200
to be a clear separation between one and the other. I don't think that there's going to

152
00:21:05,200 --> 00:21:11,640
be any complication in here. We'll have the list of components and we'll have the legal

153
00:21:11,640 --> 00:21:18,840
frameworks understood for each of the components. We will have a way to read and interpret each

154
00:21:18,840 --> 00:21:29,400
of the individual components' legal documents that go with them. We'll be able to generally

155
00:21:29,400 --> 00:21:35,960
understand the freedoms that we must have for each of the components.

156
00:21:35,960 --> 00:21:42,680
One thing that is interesting, though, is that we also are going to generate – we

157
00:21:42,680 --> 00:21:49,480
will need to have some sort of dependency graph. For example, a component that is like

158
00:21:49,480 --> 00:21:57,200
the model parameters. The model parameters will have a dependency on maybe on the tokenizer

159
00:21:57,200 --> 00:22:02,280
because otherwise you're not going to need it. So, model component, model parameter,

160
00:22:02,280 --> 00:22:07,760
and tokenizer will have to come and be shipped together in order to be used. That's why we

161
00:22:07,760 --> 00:22:15,040
need a definition for a system, in my mind. You cannot say the component itself is free

162
00:22:15,040 --> 00:22:21,200
or an open source like the model itself because the model itself is not going to be usable

163
00:22:21,200 --> 00:22:28,520
without this tokenizer, for example. That's where I think we're going to be heading

164
00:22:28,520 --> 00:22:37,200
towards, this dependency graph and the bundles that are necessary in order to have an open

165
00:22:37,200 --> 00:22:44,760
source AI. That's why we need a definition of a system because we need to have some way

166
00:22:44,760 --> 00:22:51,120
of anchoring that conversation. But we may not need it and just reference to the graph.

167
00:22:51,120 --> 00:22:55,200
We'll see where we end up with.

168
00:22:55,200 --> 00:23:04,280
That's for the next step. Today, the final vote. Then, next week, early March, we're

169
00:23:04,280 --> 00:23:18,600
going to be releasing a new version. We're still on time, in my mind, to get to June

170
00:23:18,600 --> 00:23:25,640
with a release candidate. The criteria, to repeat, the release candidate will need to

171
00:23:25,640 --> 00:23:29,800
have the support of at least two representatives for each of these stakeholder groups that

172
00:23:29,800 --> 00:23:40,320
we have invited to the conversation. Version 1 will have to be supported and endorsed by

173
00:23:40,320 --> 00:23:49,800
at least double as such or at least five representatives for each stakeholder group. We will be announcing

174
00:23:49,800 --> 00:23:54,000
it in late October. That's the deadline.

175
00:23:54,000 --> 00:24:01,560
Between the release candidate and version 1, we're planning a worldwide tour to show

176
00:24:01,560 --> 00:24:10,720
the release candidate and host small workshops to iterate with other stakeholders around

177
00:24:10,720 --> 00:24:18,680
the world. We're fundraising for this. Stay tuned and we'll show details probably next

178
00:24:18,680 --> 00:24:24,880
week or the week after.

179
00:24:24,880 --> 00:24:34,040
I think we're getting much closer to have a full list of stakeholders yet. It's always

180
00:24:34,040 --> 00:24:40,560
good if you have friends or contacts that you think should be involved and should be

181
00:24:40,560 --> 00:24:49,240
following this. Mostly, we're looking for end users and subject category and in some

182
00:24:49,240 --> 00:24:56,520
ways also regulators, although not necessarily government employees or policy makers because

183
00:24:56,520 --> 00:25:04,080
they're not going to be able to freely give feedback. But organizations that work very

184
00:25:04,080 --> 00:25:09,800
closely like lobbying organizations that work very closely with policy makers, they'd be

185
00:25:09,800 --> 00:25:20,480
very useful to have a read into the definition as it's being drafted so that they can help

186
00:25:20,480 --> 00:25:25,440
us understand what the regulators are concerned about.

187
00:25:25,440 --> 00:25:31,600
We create a definition or we come up with a definition that is not illegal out of the

188
00:25:31,600 --> 00:25:37,920
gate. Let's put it that way. Either illegal or impossible to implement in many parts of

189
00:25:37,920 --> 00:25:44,240
the world because it clashes with regulation.

190
00:25:44,240 --> 00:25:53,520
And a reminder that we'll probably have to keep an eye on the evolution of the world

191
00:25:53,520 --> 00:25:59,040
and keep an eye on the definition that we come up with so that we can adapt it in case

192
00:25:59,040 --> 00:26:06,200
there are changes in the technological landscape that will force us to create either another

193
00:26:06,200 --> 00:26:11,680
checklist separate. Like if we're now focusing on machine learning, maybe there's going to

194
00:26:11,680 --> 00:26:16,320
be some new technology or some variation of machine learning that we require to review

195
00:26:16,320 --> 00:26:22,800
the list of components. And at that point, we will have to adjust or add a new checklist

196
00:26:22,800 --> 00:26:29,600
to the bottom of our definition of AI. So we will need to keep on working.

197
00:26:29,600 --> 00:26:37,040
And with that, my encouragement for you is to join the conversation in the forum. We're

198
00:26:37,040 --> 00:26:42,680
making an effort to start publishing weekly also a summary of the forums on our blog to

199
00:26:42,680 --> 00:26:48,800
make sure that everyone has an opportunity to stay on top even though their inboxes get

200
00:26:48,800 --> 00:26:58,120
full. And a reminder to everyone that on opensource.org/deepdive, you will find a link to the drafts and the

201
00:26:58,120 --> 00:27:04,320
drafts themselves are – you can comment on them directly.

202
00:27:04,320 --> 00:27:11,640
So with that, I'm open to opening the floor for questions and answers from you. I can

203
00:27:11,640 --> 00:27:29,640
bring up your mic so you can speak up or you can type it however you want to talk.

204
00:27:29,640 --> 00:27:42,360
I believe Isabel is writing a question. Let's wait for that.

205
00:27:42,360 --> 00:27:49,800
In the meantime, it's very good that we're seeing these types of discussions. And I find

206
00:27:49,800 --> 00:27:55,000
it particularly interesting that we're actually discussing what's the meaning of each one

207
00:27:55,000 --> 00:28:02,160
of those verbs, right? What's use, what's study, what's modify, and what's share? I

208
00:28:02,160 --> 00:28:09,040
see a lot of discussions there in the forum around those verbs. And in particular for

209
00:28:09,040 --> 00:28:21,040
study, it really does depend on how deep it's study, what's the meaning of study. And I

210
00:28:21,040 --> 00:28:27,120
was looking back here at the free software definition and I'm just going to read through

211
00:28:27,120 --> 00:28:34,720
this, the freedom, what, right? So the freedom to study how the problem works and change

212
00:28:34,720 --> 00:28:42,160
it so it does not – it does your computing as you wish.

213
00:28:42,160 --> 00:28:54,080
So it's clear here that the idea behind this is just study how the problem works and to

214
00:28:54,080 --> 00:29:04,160
modify. If you really want something deep, like to study to really understand that, it

215
00:29:04,160 --> 00:29:11,680
becomes clear that you need more than just access to the code. You have to have a lot

216
00:29:11,680 --> 00:29:18,080
of documentation to understand the idea behind that. And the same applies to data. You have

217
00:29:18,080 --> 00:29:27,600
to have not just the data should be open, but also how the data was created. How was

218
00:29:27,600 --> 00:29:40,720
it filtered? So in a way, it becomes impractical. And the open source definition and open source

219
00:29:40,720 --> 00:29:46,880
software has always been pragmatic, right? That's one of the key ideas behind open source

220
00:29:46,880 --> 00:29:56,880
definition. And so having a study so deep like that, perhaps it might become impractical

221
00:29:56,880 --> 00:30:03,760
to actually apply that. So that's what I wanted to share here.

222
00:30:03,760 --> 00:30:12,320
Yeah, that's a good point. I've been rereading the classic documents myself and realizing

223
00:30:12,320 --> 00:30:21,760
that that's the spirit. It's very, very practical. The freedom to study, all the descriptions

224
00:30:21,760 --> 00:30:30,800
inside the Kino Manifesto, the benefits are for scientific progress. And it talks about

225
00:30:30,800 --> 00:30:40,480
the schools will not have to waste too much in order to acquire software and for teaching

226
00:30:40,480 --> 00:30:51,440
and let the students understand what's happening. So there are multiple facets that are useful

227
00:30:51,440 --> 00:30:52,960
to go back and read.

228
00:30:52,960 --> 00:30:57,360
All right. So if there are no more questions...

229
00:30:57,360 --> 00:30:58,360
Isabel.

230
00:30:58,360 --> 00:30:59,360
Okay, go ahead.

231
00:30:59,360 --> 00:31:06,560
Yeah, so Isabel doesn't have a question, but she actually shared exactly those discussions

232
00:31:06,560 --> 00:31:12,880
around the verbs use and share. And thank you, Isabel, for sharing that.

233
00:31:12,880 --> 00:31:25,280
Wonderful. Yes, thank you. So today we're going to start with publishing a forum wrap-up

234
00:31:25,280 --> 00:31:35,200
on our blog. And we're going to be publishing the recording as usual and the slide deck

235
00:31:35,200 --> 00:31:44,480
for this town hall. All right, everyone, thank you for joining. And we'll see you in two weeks

236
00:31:44,480 --> 00:31:52,720
at a later time. So that is compatible, more compatible with the United States and West

237
00:31:52,720 --> 00:31:59,680
Coast in general. And we'll keep on hammering on this. We'll have a new definition, new draft

238
00:31:59,680 --> 00:32:03,200
at the next meeting. Thank you.

### End of last town hall held on 2024-02-23 ###

### Start of next town hall held on 2024-03-08 ###
--- Presentation for 2024-03-08 ---
OPEN SOURCE AI DEFINITION
Online public townhall
March 8, 2024
last updated: March 5, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

The objective for 2024

Open Source AI Deﬁnition
version 1.0

3

hackmd.io/@opensourceinitiative/osaid-0-0-5

Deﬁnition of AI system

Preamble
Out of scope issues
4 freedoms
Legal checklist

4

What is Open Source AI
To be Open Source, an AI system needs to be available
under legal terms that grant the freedoms to:
● Use the system for any purpose and without having to
ask for permission.
● Study how the system works and inspect its
components.
● Modify the system to change its recommendations,
predictions or decisions to adapt to your needs.
● Share the system with or without modiﬁcations, for
any purpose.
5

Working group recommendations

6

Systems review plan
Planned phases and where we are now:
1. ✔ Analyze a sample of “AI systems” to identify precisely the
required components for study, use modiﬁcation, and sharing of
the entire system
2. For each component of these systems, check their availability
and the conditions for use/distribution (the legal documents)
3. Generalize the ﬁndings and complete a checklist for OSI license
committee to evaluate legal documents for AI systems (OSAID
“feature complete”)
4. Get endorsements from major stakeholders (RC1)
5. Keep reﬁning the OSAID, as it gains support from more
stakeholders (v. 1.0)

7

Systems
Selected to have diversity of approaches:
1. Pythia: open science project, with a permissive license
2. BLOOM: open science project, with lots of details
released but shared with a restrictive license
3. Llama 2: commercial project, accompanied by limited
amount of science and with a restrictive license
4. OpenCV: open source project, with ML components
outside of the generative AI space
8

Members
Llama 2
1.
2.
3.
4.
5.
6.

Bastien Guerry
DINUM, French
public administration
Ezequiel Lanza
Intel
Roman Shaposhnik
Apache Software
Foundation
Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute
of Paris

Pythia
BLOOM
1.
George C. G. Barbosa
1.
Seo-Young Isabelle
Fundação Oswaldo Cruz
Hwang Samsung
2.
Daniel Brumund GIZ
2.
Cailean Osborne
FAIR Forward - AI for all
University of Oxford,
3.
Danish Contractor
Linux Foundation
BLOOM Model Gov. WG
4.
Abdoulaye Diack
3.
Stella Biderman
Google
EleutherAI
5.
Deshni Govender GIZ
4.
Justin Colannino
FAIR Forward - AI for all
Microsoft
6.
Jaan Li University of
5.
Aviya Skowron
Tartu, Phare Health
EleutherAI
7.
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
8.
Ofentse Phuti WiMLDS
Gaborone
9.
Caleb Fianku Quao
Kwame Nkrumah
University of Science and
Technology, Kumasi

OpenCV
1.
2.
3.
4.
5.
6.
7.

8.
9.

Rahmat Akintola
Cubeseed Africa
Ignatius Ezeani
Lancaster University
Kevin Harerimana
CMU Africa
Satya Mallick
OpenCV
David Manset
ITU
Phil Nelson
OpenCV
Tlamelo Makati
WiMLDS Gaborone,
Technological
University Dublin
Minyechil Alehegn
Tefera Mizan Tepi
University
Akosua Twumasi
Ghana Health
Service

9

Voting
Code All code used to parse and process
data, including:

Required to
Use?

Required
to Study?

Required to
Modify?

Data preprocessing code

SZ

SZ EL

Training code

SZ

SZ

Required to
Share?

Test code
Code used to perform inference for benchmark
tests
Validation code
Inference code

SZ
SM EL DT
SM JT SZ

SZ

SZ

SZ

SZ

Evaluation code
Other libraries or code artifacts that are part of
the system, such as tokenizers and
hyperparameter search code, if used.

BG,EL, SM,
SZ

SZ

source: Llama 2 working group (Feb. 9, 2024)
10

Vote compilation

As of 2/21/24 at 9:00 pm UTC

11

Recommendations summary
● Required
○
○
○
○
○

Training, validation &
testing code
Inference code
Model architecture
Model parameters
Supporting libraries &
tools

● Likely Required
○ Data preprocessing code
● Maybe Required
○
○
○

Datasets
Usage documentation
Research paper

2/26/24

● Likely Not Required
○
○

Model card
Evaluation code

○
○
○
○
○
○

Data card
Evaluation data
Evaluation results
Model metadata
Sample model outputs
Technical report

● Not Required

go to results spreadsheet →
12

Deﬁnition v. 0.0.6

3/7/24

● Required components ● Optional (appreciated, not required)
○
○
○
○
○
○

Data preprocessing
code
Training, validation &
testing code
Inference code
Model architecture
Model parameters
Supporting libraries &
tools

○
○
○
○
○
○
○
○
○
○
○

Datasets
Usage documentation
Research paper
Model card
Evaluation code
Data card
Evaluation data
Evaluation results
Model metadata
Sample model outputs
Technical report

13

Required Components Detail

3/7/24

●

A sufficiently detailed information on how the system was trained,
including the training methodologies and techniques, the training
data sets used, information about the provenance of those data
sets, their scope and characteristics; how the data was obtained
and selected, the labeling procedures and data cleaning
methodologies.

●

The code used for pre-processing data, the code used for training,
validation and testing.

●

The model parameters, including weights. Where applicable, these
should include checkpoints from key intermediate stages of
training as well as the ﬁnal optimizer state.

●

The supporting libraries like tokenizers and hyperparameters
search code (if used), the inference code, and model architecture.

14

Generalized text in v. 0.0.6
Precondition to exercise these freedoms
is to have access to the preferred form
to make modiﬁcations to the system.
Release date: Mar 11, 2024

15

Next steps
● Version 0.0.6 release on Monday
● Start step 2: For each system, check the availability
of required components and analyze their
conditions for use/distribution (the legal
documents)
16

What phase 2 will look like
For each AI system, build a table like:
Required component

Link to resource

Legal framework

Data pre-processing code

URL

OSI-approved license

Training, validation and testing code

URL

…

Inference code

URL

…

Supporting libraries and tools

URL

…

Model architecture

URL

…

Model parameters

URL

???
17

2024 timeline

Track 1: System testing work stream
Track 2: Stakeholder consultation work stream
Track 3: Releases

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

Draft 0.0.8

Townhall +

… October
Monthly Virtual
Meetings

Release
version 1.0

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

v. 1.0

18

Criteria for RC1 and v. 1.0
RC1

version 1

- Expected outcome of
in-person meeting end
May/early June!
- The draft is completed
in all its parts
- The draft is supported
by at least 2
representatives for
each of the 6
stakeholder groups

- Expected outcome of
in-person and online
meetings through the
summer/early autumn
- The draft is endorsed by at
least 5 reps for each of the
stakeholder groups
- Announced in late October

19

Help us ﬁnd stakeholders
System Creator

License Creator

Regulator

Licensee

End User

Subject

Makes AI system
and/or
component that
will be studied,
used, modified,
or shared
through an open
source license
(e.g., ML
researcher in
academia or
industry)

Writes or edits
the open source
license to be
applied to the AI
system or
component;
includes
compliance
(e.g., IP lawyer)

Writes or edits
rules governing
licenses and
systems (e.g.
government
policy-maker)

Seeks to study,
use modify, or
share an open
source AI
system (e.g. AI
engineer, health
researcher,
education
researcher)

Consumes a
system output,
but does not
seek to study,
use, modify, or
share the
system (e.g.,
student using a
chatbot to write
a report, artist
creating an
image)

Affected
upstream or
downstream by
a system output
without
interacting with it
intentionally;
includes
advocates for
this group (e.g.
people with loan
denied, or
content creators)

✅

✅

⚠

✅

⚠

⚠

Enough to start

Enough to start

Leads to US, EU,
Singapore, no
commitment yet

Enough to start

Which org is squarely in
this space?

ACLU, Algorithmic
Justice League

20

It doesn’t end with v. 1.0
We’ll need to deﬁne rules for maintenance and review of the
Deﬁnition

21

Join the conversation
● discuss.opensource.org
● Public forum
● Join as OSI member
○ Free or full
○ SSO with other OSI
websites

22

Q&A

23

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the
deﬁnitional process.

24


--- Subtitles for 2024-03-08 --- ###
1
00:00:00,001 --> 00:00:05,000
Yeah, let's go with it.

2
00:00:05,000 --> 00:00:06,000
All right.

3
00:00:06,000 --> 00:00:07,640
Okay, welcome everyone.

4
00:00:07,640 --> 00:00:13,520
So you are attending the online public town hall for the open source AI definition on

5
00:00:13,520 --> 00:00:16,360
March 8th, 2024.

6
00:00:16,360 --> 00:00:17,360
My name is Mare Joyce.

7
00:00:17,360 --> 00:00:23,560
My pronouns are she and her, and I'm the process facilitator for creating this definition as

8
00:00:23,560 --> 00:00:27,400
a multi-stakeholder co-design process.

9
00:00:27,400 --> 00:00:32,660
Okay, so these are community agreements.

10
00:00:32,660 --> 00:00:34,360
Some of you have already seen them.

11
00:00:34,360 --> 00:00:36,260
I'll go over them briefly.

12
00:00:36,260 --> 00:00:43,440
So one mic, one speaker, allowing one person to speak at a time, no interrupting.

13
00:00:43,440 --> 00:00:47,160
Take space, make space if you tend to speak up.

14
00:00:47,160 --> 00:00:48,320
Allow space for others to speak.

15
00:00:48,320 --> 00:00:52,040
If you tend not to speak, we do invite you to share.

16
00:00:52,040 --> 00:00:58,000
Kindness, just remembering that the work is hard, but we don't have to be, and to just

17
00:00:58,000 --> 00:01:00,800
be gentle with each other and curious.

18
00:01:00,800 --> 00:01:07,680
And of course, hate speech and insults are not permitted in the spaces that we host.

19
00:01:07,680 --> 00:01:15,720
Forward motion just means that we focus on what's possible and that we note obstacles

20
00:01:15,720 --> 00:01:21,680
and then we route around them and move forward and come back when needed, but that we don't

21
00:01:21,680 --> 00:01:27,080
let complexity stop our project from moving forward.

22
00:01:27,080 --> 00:01:29,440
Solution seeking is connected to that.

23
00:01:29,440 --> 00:01:37,920
So we just, we know that suggesting new ideas and options is vulnerable, but it is crucial.

24
00:01:37,920 --> 00:01:43,040
And this is how we resolve those complexities that we just mentioned.

25
00:01:43,040 --> 00:01:47,840
And are there any other norms that you would like to see in this meeting?

26
00:01:47,840 --> 00:01:52,800
If you have any, you can type them in the chat.

27
00:01:52,800 --> 00:02:02,680
I also realized that there's a transparency norm that we should add here, just to say

28
00:02:02,680 --> 00:02:09,280
that this will be posted publicly, this video will be posted publicly.

29
00:02:09,280 --> 00:02:12,360
Okay.

30
00:02:12,360 --> 00:02:13,680
Looks like we don't have any other comments.

31
00:02:13,680 --> 00:02:16,200
I will continue.

32
00:02:16,200 --> 00:02:24,880
So yes, our objective for 2024 is to release version 1.0 of the open source AI definition.

33
00:02:24,880 --> 00:02:28,720
And this is what the definition looks like.

34
00:02:28,720 --> 00:02:30,960
We define AI system.

35
00:02:30,960 --> 00:02:36,040
There is a preamble that basically makes an argument for why this definition is necessary.

36
00:02:36,040 --> 00:02:37,880
There are certain out of scope issues.

37
00:02:37,880 --> 00:02:41,240
The URL is up at the top.

38
00:02:41,240 --> 00:02:44,960
If you'd like to see the document.

39
00:02:44,960 --> 00:02:50,240
And there are the four freedoms, which I will go into in greater detail.

40
00:02:50,240 --> 00:02:55,440
And then there's this license checklist, which is looking at specifically which components

41
00:02:55,440 --> 00:03:00,640
of an AI system must be open in order for the system to be called open according to

42
00:03:00,640 --> 00:03:04,600
this definition.

43
00:03:04,600 --> 00:03:06,960
These are the freedoms.

44
00:03:06,960 --> 00:03:14,720
These were coming from earlier documents in the open source movement, but were verified

45
00:03:14,720 --> 00:03:22,120
and drafted specifically for this purpose by co-design workshops at the end of 2023.

46
00:03:22,120 --> 00:03:27,000
So using the system for any purpose without having to ask for permission, studying how

47
00:03:27,000 --> 00:03:32,680
the system works and inspecting its components, modifying the system to change its recommendations

48
00:03:32,680 --> 00:03:37,400
and predictions and to adapt it to your needs and sharing the system with or without modification

49
00:03:37,400 --> 00:03:46,880
for any purpose are the overarching requirements of an open source AI system.

50
00:03:46,880 --> 00:03:50,360
And now what we're going to talk about is the most recent work we've been doing, which

51
00:03:50,360 --> 00:03:59,200
is recommendations of working groups that have been looking at AI systems to develop

52
00:03:59,200 --> 00:04:02,440
this list of required components.

53
00:04:02,440 --> 00:04:10,280
So this is what we're up to with systems review, which is also called track one of our project.

54
00:04:10,280 --> 00:04:14,000
We're not going too deeply into different tracks, but for those of you who have that

55
00:04:14,000 --> 00:04:17,080
context, this is track one of three.

56
00:04:17,080 --> 00:04:23,200
So what we have completed is that we analyzed a sample of AI systems to identify precisely

57
00:04:23,200 --> 00:04:27,160
required components for study, use, modification and sharing.

58
00:04:27,160 --> 00:04:31,520
And what we are going to do next is for each component of these systems, check their availability

59
00:04:31,520 --> 00:04:34,800
and the conditions for use and distribution.

60
00:04:34,800 --> 00:04:42,440
So legal documents and licenses, because that is the mechanism by which OSI verifies a piece

61
00:04:42,440 --> 00:04:47,880
of technology is open source or not is through licenses and legal documents.

62
00:04:47,880 --> 00:04:53,360
Then we will generalize the findings and complete a checklist for OSI license committee, get

63
00:04:53,360 --> 00:04:56,260
endorsement from major stakeholders.

64
00:04:56,260 --> 00:05:03,860
And that will be called that's release candidate one, which will proceed version 1.0, which

65
00:05:03,860 --> 00:05:12,420
of course we will refine and rough dates are RC1 is June and then version 1.0 is October.

66
00:05:12,420 --> 00:05:17,860
And anything to add to the slide, Stefano?

67
00:05:17,860 --> 00:05:23,540
No, this to me looks complete.

68
00:05:23,540 --> 00:05:30,260
It's more about if anyone has any comments so far, I mean, you can use the chat and keep

69
00:05:30,260 --> 00:05:31,260
on adding comments.

70
00:05:31,260 --> 00:05:32,260
Yeah.

71
00:05:32,260 --> 00:05:33,260
Okay.

72
00:05:33,260 --> 00:05:34,260
Sounds good.

73
00:05:34,260 --> 00:05:35,260
Okay.

74
00:05:35,260 --> 00:05:38,620
So these are the four systems that we developed work groups around and this is why we chose

75
00:05:38,620 --> 00:05:39,620
them.

76
00:05:39,620 --> 00:05:42,820
We wanted to have a diversity of approaches.

77
00:05:42,820 --> 00:05:48,740
So we looked at Pythia, an open science project with a permissive license, Bloom, another

78
00:05:48,740 --> 00:05:55,220
open science project with lots of details with release, but with a restrictive license.

79
00:05:55,220 --> 00:06:00,180
And I'm sure all these descriptions could be, might be different for different people,

80
00:06:00,180 --> 00:06:02,740
but this is how we were looking at the systems.

81
00:06:02,740 --> 00:06:09,340
Lama2, obviously a commercial project with a restrictive license, OpenCV, an open source

82
00:06:09,340 --> 00:06:15,940
project with ML components, but let's have generative AI.

83
00:06:15,940 --> 00:06:23,380
And so these were the work group members having public membership as part of the transparency

84
00:06:23,380 --> 00:06:28,300
commitment we have just so that you know, who are the people that are making these recommendations?

85
00:06:28,300 --> 00:06:34,700
Cause these are particularly empowered stakeholders in determining what the components of the

86
00:06:34,700 --> 00:06:36,740
definition will be.

87
00:06:36,740 --> 00:06:45,300
And we did additional outreach to have a better, particularly racial and geographic representation.

88
00:06:45,300 --> 00:06:50,420
So you can take a look at that later, I guess.

89
00:06:50,420 --> 00:06:55,780
We do have posted on the forum, all these, this information as well.

90
00:06:55,780 --> 00:07:01,460
And what the, what the members of the working groups did, this might be information that

91
00:07:01,460 --> 00:07:05,180
some of you have already heard, but I'll go through it for those who are new, is that

92
00:07:05,180 --> 00:07:11,860
they voted on, we took a list of components from a forthcoming paper.

93
00:07:11,860 --> 00:07:15,180
Can we, can we name the paper yet Stefano?

94
00:07:15,180 --> 00:07:16,940
Cause it is forthcoming.

95
00:07:16,940 --> 00:07:17,940
We can name it.

96
00:07:17,940 --> 00:07:18,940
Yes.

97
00:07:18,940 --> 00:07:19,940
Okay.

98
00:07:19,940 --> 00:07:20,940
Excellent.

99
00:07:20,940 --> 00:07:21,940
Okay.

100
00:07:22,940 --> 00:07:27,580
So it's called the Model Openness Framework.

101
00:07:27,580 --> 00:07:33,340
It's being, it was written primarily by researchers connected to the Linux Foundation, although

102
00:07:33,340 --> 00:07:39,640
with other collaborators as well, it's going to be released on ArchiveX, I think imminently,

103
00:07:39,640 --> 00:07:41,820
maybe even today or tomorrow.

104
00:07:41,820 --> 00:07:48,340
Anyway, they came up with this really nice generalized list of AI system components.

105
00:07:48,340 --> 00:07:53,380
And so we use their list and then we created this voting system.

106
00:07:53,380 --> 00:07:58,120
Do you think this component is necessary to use, study, modify, share, and then members

107
00:07:58,120 --> 00:08:00,780
of the working groups with their initials.

108
00:08:00,780 --> 00:08:04,620
So again, there's transparency who's saying that what is required voted.

109
00:08:04,620 --> 00:08:10,340
And this is just a screenshot of a slide from the LAMA working group.

110
00:08:10,340 --> 00:08:12,620
And yeah, if you have any questions, just put them in the chat.

111
00:08:12,620 --> 00:08:15,840
And if I don't see them, Stefano will see them.

112
00:08:15,840 --> 00:08:21,380
And then we compiled all the votes across all the working groups.

113
00:08:21,380 --> 00:08:35,140
And I developed a rubric for basically picking a vote total and turning it into a recommendation

114
00:08:35,140 --> 00:08:37,120
for a requirement.

115
00:08:37,120 --> 00:08:39,060
And I think, do I have a zoom in on that?

116
00:08:39,060 --> 00:08:40,440
I do not have a zoom in on that.

117
00:08:40,440 --> 00:08:45,900
So if you look at the upper, I don't know which corner it is, upper left or right, but

118
00:08:45,900 --> 00:08:50,500
you see this part of the slide, for me, it's the upper right, which says legend.

119
00:08:50,500 --> 00:08:55,580
And it's basically based on the mean number of votes, something that was more than two

120
00:08:55,580 --> 00:09:04,140
times the mean was an automatic yes, this is required on down to likely yes, maybe lean

121
00:09:04,140 --> 00:09:17,580
no and then no for less than, okay, between I think 0.5 and zero or zero less than 0.5

122
00:09:17,580 --> 00:09:18,580
and zero.

123
00:09:18,580 --> 00:09:19,580
Yeah.

124
00:09:21,580 --> 00:09:27,940
So you can kind of see through this color coding how each component ranked, you can

125
00:09:27,940 --> 00:09:33,300
see there's a few here that are yeses, like training validation and testing code and inference

126
00:09:33,300 --> 00:09:34,500
code.

127
00:09:34,500 --> 00:09:39,740
And then many lean no's, for example, data set.

128
00:09:39,740 --> 00:09:41,860
And then we looked at multiple different data sets.

129
00:09:41,860 --> 00:09:43,860
So that was different from the model openness framework.

130
00:09:43,860 --> 00:09:47,940
They just had data set, but we thought that's such an important part of the system that

131
00:09:47,940 --> 00:09:51,900
we broke it out into multiple different types of data set to really get as much input as

132
00:09:51,900 --> 00:09:56,220
possible on whether that should be required.

133
00:09:56,220 --> 00:10:02,500
And then this is the result of the voting of that applying the rubric to the votes was

134
00:10:02,500 --> 00:10:09,860
that the required elements were and are training validation and testing code, inference code,

135
00:10:09,860 --> 00:10:15,940
model architecture, model parameters, which includes weights, supporting libraries and

136
00:10:15,940 --> 00:10:23,380
tools which includes things like tokenizers and hyperparameter search if used.

137
00:10:23,380 --> 00:10:28,860
And then down through a likely required data processing code, maybe the data sets as you

138
00:10:28,860 --> 00:10:37,860
saw not likely elements like the model card, not required data card, sample model, technical

139
00:10:37,860 --> 00:10:38,860
report.

140
00:10:38,860 --> 00:10:44,860
And then what is new from I think some of you have been in previous meetings is that

141
00:10:44,860 --> 00:10:51,180
we have for version 6.0 made this distinction.

142
00:10:51,180 --> 00:10:55,820
So the required components, we took everything that was required plus likely required.

143
00:10:55,820 --> 00:11:01,100
So the data pre-processing code and those are the required components.

144
00:11:01,100 --> 00:11:03,060
And then everything else is optional.

145
00:11:03,060 --> 00:11:09,820
So there's nothing that we're saying don't include this, but everything else is listed

146
00:11:09,820 --> 00:11:13,500
as appreciated, but not required.

147
00:11:13,500 --> 00:11:14,660
And I think let's see what's next.

148
00:11:14,660 --> 00:11:18,300
I feel like this might be a good place to pause.

149
00:11:18,300 --> 00:11:19,300
Maybe we do this.

150
00:11:19,300 --> 00:11:25,900
Do you want to pause here for questions, Stefano, or do you want to go into this greater detail

151
00:11:25,900 --> 00:11:28,500
about version 6.0 and then pause?

152
00:11:28,500 --> 00:11:35,780
Yeah, let's wait and see if anyone has any questions so far.

153
00:11:35,780 --> 00:11:40,500
We can definitely take a break.

154
00:11:40,500 --> 00:11:41,500
Yeah.

155
00:11:41,500 --> 00:11:48,340
So any kind of questions or thoughts on how we got here and where we are?

156
00:11:48,340 --> 00:11:49,340
Okay.

157
00:11:49,340 --> 00:11:50,340
Thank you.

158
00:11:50,340 --> 00:11:51,340
I'm muted basically, everyone.

159
00:11:51,340 --> 00:11:59,020
If anyone wants to jump, they can use voice or if you feel more comfortable, you can type.

160
00:11:59,020 --> 00:12:00,020
Yeah.

161
00:12:00,020 --> 00:12:08,580
And there's a raise hand function at the bottom like in Zoom as well.

162
00:12:08,580 --> 00:12:12,060
Yep.

163
00:12:12,060 --> 00:12:20,300
I don't see many questions.

164
00:12:20,300 --> 00:12:21,300
Maybe we can just...

165
00:12:21,300 --> 00:12:22,300
Obastium has a question.

166
00:12:22,300 --> 00:12:23,300
Go ahead.

167
00:12:23,300 --> 00:12:24,300
Yeah.

168
00:12:24,300 --> 00:12:25,300
Hi.

169
00:12:25,300 --> 00:12:26,300
Thanks for the summary.

170
00:12:26,300 --> 00:12:36,420
I was just wondering if you had any priority in each column, like is data processing code

171
00:12:36,420 --> 00:12:38,980
more important than training validation?

172
00:12:38,980 --> 00:12:45,220
I guess on the left, everything is at the same level, but maybe on the right, some things

173
00:12:45,220 --> 00:12:50,580
are more optional than others or is it something that has not been discussed?

174
00:12:50,580 --> 00:12:54,620
Hasn't been discussed, frankly.

175
00:12:54,620 --> 00:12:57,500
And yes, they're all required.

176
00:12:57,500 --> 00:12:58,500
It's a yes or no.

177
00:12:58,500 --> 00:12:59,500
Yeah, it's a yes or no.

178
00:12:59,500 --> 00:13:05,500
Yeah, we can have a conversation about that.

179
00:13:05,500 --> 00:13:06,500
Yeah.

180
00:13:06,500 --> 00:13:07,500
Because...

181
00:13:07,500 --> 00:13:17,780
Oh, let me just before those double questions, let me get another question from the next

182
00:13:17,780 --> 00:13:20,140
person, Bastien, and come back to you.

183
00:13:20,140 --> 00:13:21,140
Unless...

184
00:13:21,140 --> 00:13:22,140
Is that okay?

185
00:13:22,140 --> 00:13:23,140
Sure.

186
00:13:23,140 --> 00:13:24,140
No problem.

187
00:13:24,140 --> 00:13:25,140
Yes.

188
00:13:25,140 --> 00:13:26,140
Okay.

189
00:13:26,140 --> 00:13:27,140
Thanks.

190
00:13:27,140 --> 00:13:28,140
All right.

191
00:13:28,140 --> 00:13:29,140
Jacob?

192
00:13:29,140 --> 00:13:30,140
Yeah.

193
00:13:30,140 --> 00:13:31,140
You guys can hear me, right?

194
00:13:31,140 --> 00:13:32,140
Correct.

195
00:13:32,140 --> 00:13:33,140
Yep.

196
00:13:33,140 --> 00:13:34,140
Okay, cool.

197
00:13:34,140 --> 00:13:36,900
Yeah, sure.

198
00:13:36,900 --> 00:13:48,120
So I guess one question I had is if some of the reasoning around decisions that were made

199
00:13:48,120 --> 00:13:50,180
within these groups is available.

200
00:13:50,180 --> 00:13:58,580
I'd just be curious, particularly on the datasets portion.

201
00:13:58,580 --> 00:14:12,040
I guess my intuition is that if something is open source, we should be able to verify

202
00:14:12,040 --> 00:14:18,580
its legality completely.

203
00:14:18,580 --> 00:14:27,660
And without access to the datasets or in some way, then that may be a lot more difficult.

204
00:14:27,660 --> 00:14:33,460
I'm not saying complete access to the datasets.

205
00:14:33,460 --> 00:14:35,980
But some access may be worthwhile.

206
00:14:35,980 --> 00:14:38,420
Go ahead, Stef.

207
00:14:38,420 --> 00:14:39,420
Yeah.

208
00:14:39,420 --> 00:14:43,820
I can give an argument.

209
00:14:43,820 --> 00:14:51,660
So the question of legality may not be appropriate for the open source definition.

210
00:14:51,660 --> 00:14:55,140
It's a separate conversation.

211
00:14:55,140 --> 00:15:02,980
But I do believe, and there were conversations inside the group about that understanding

212
00:15:02,980 --> 00:15:03,980
of the datasets.

213
00:15:03,980 --> 00:15:12,420
And in fact, one of the reasons why you see there the required components, the data pre-processing

214
00:15:12,420 --> 00:15:22,940
code is that many of the groups had debated about which of the -- what's necessary, what

215
00:15:22,940 --> 00:15:28,980
level of understanding do you need to have in order to be able to feel safe about using

216
00:15:28,980 --> 00:15:36,500
a model or have enough information, the transparency requirements that look like they're going

217
00:15:36,500 --> 00:15:42,740
to be mandated by law anyway, at least in Europe, and other information about provenance

218
00:15:42,740 --> 00:15:50,680
and assessing risk in deployments, bias calculation, and all of those things.

219
00:15:50,680 --> 00:15:59,260
So the conversation went -- this is the -- it's the torn issue, access to the datasets, what

220
00:15:59,260 --> 00:16:06,660
is necessary, what is required, what level of depth do you need to have access to it.

221
00:16:06,660 --> 00:16:11,580
And it's something that we're probably going to keep continuing debating.

222
00:16:11,580 --> 00:16:17,500
What seemed clear to me is that the original dataset, having access to it completely and

223
00:16:17,500 --> 00:16:22,860
fully in a way that you can download it, et cetera, and retrain with the purpose of retraining

224
00:16:22,860 --> 00:16:30,020
or rebuilding from scratch a model that you have received, it's a requirement that is

225
00:16:30,020 --> 00:16:31,660
not strictly necessary.

226
00:16:31,660 --> 00:16:38,780
At least there was pretty much -- pretty good agreement on that front.

227
00:16:38,780 --> 00:16:39,780
>> Thank you.

228
00:16:39,780 --> 00:16:41,340
And I see you have a raised hand again, Jacob.

229
00:16:41,340 --> 00:16:45,700
I'm just going to go to another question, and we can come back to you.

230
00:16:45,700 --> 00:16:48,900
So Shala asks -- yeah, exactly.

231
00:16:48,900 --> 00:16:51,580
No, thank you.

232
00:16:51,580 --> 00:16:58,300
So Shala asks, did any of the systems reviewed meet the required components?

233
00:16:58,300 --> 00:17:04,680
So let me go back to the -- yeah.

234
00:17:04,680 --> 00:17:11,020
So it wasn't that we were trying to evaluate whether any system had all of its components

235
00:17:11,020 --> 00:17:12,300
required.

236
00:17:12,300 --> 00:17:14,700
It was more of a comparison.

237
00:17:14,700 --> 00:17:22,620
And no, there was no system where all the components were considered to be required.

238
00:17:22,620 --> 00:17:28,820
But the ones that are required are simply the ones on this list under the lime green

239
00:17:28,820 --> 00:17:32,180
required title.

240
00:17:32,180 --> 00:17:37,980
And those are the ones that then we moved into this binary distinction of required and

241
00:17:37,980 --> 00:17:41,940
not, which will appear in definition 0.0.6.

242
00:17:41,940 --> 00:17:47,340
And feel free to raise your hand or ask a follow-up question in case I didn't ask.

243
00:17:47,340 --> 00:17:48,340
Okay.

244
00:17:48,340 --> 00:17:50,220
Bastien needs to go.

245
00:17:50,220 --> 00:17:51,220
Okay.

246
00:17:51,220 --> 00:17:52,220
Thank you for coming, Bastien.

247
00:17:52,220 --> 00:17:55,780
I see -- Jacob, did you want to ask another question?

248
00:17:55,780 --> 00:17:58,420
>> I want to complete the answer.

249
00:17:58,420 --> 00:18:05,540
I want to add something else for Shala.

250
00:18:05,540 --> 00:18:10,580
This analysis, this -- you know, the response also will come after the next phase.

251
00:18:10,580 --> 00:18:17,140
We're going to be reviewing the systems and see if they have available the required components

252
00:18:17,140 --> 00:18:19,740
and what conditions they're available.

253
00:18:19,740 --> 00:18:23,140
What can we do with those components?

254
00:18:23,140 --> 00:18:24,140
>> Okay.

255
00:18:24,140 --> 00:18:27,580
Yeah, I see what you're asking, Shala.

256
00:18:27,580 --> 00:18:32,540
Thank you for clarifying that, Stefano.

257
00:18:32,540 --> 00:18:35,620
I see Justin is typing.

258
00:18:35,620 --> 00:18:37,340
But Jacob already has his hand up.

259
00:18:37,340 --> 00:18:40,340
So yes, ask your question, Jacob.

260
00:18:40,340 --> 00:18:48,620
>> I should have written it down.

261
00:18:48,620 --> 00:18:49,620
I apologize.

262
00:18:49,620 --> 00:18:52,220
I had a thought.

263
00:18:52,220 --> 00:18:53,220
It's gone now.

264
00:18:53,220 --> 00:18:55,220
I'll bring it up if I remember it.

265
00:18:55,220 --> 00:18:56,220
>> No problem.

266
00:18:56,220 --> 00:18:57,220
No problem.

267
00:18:57,220 --> 00:19:01,200
I'm going to -- let's move on to the next slide.

268
00:19:01,200 --> 00:19:07,660
Because that one adds this extra information about requirements around the training method.

269
00:19:07,660 --> 00:19:10,220
And then we can continue to take questions.

270
00:19:10,220 --> 00:19:15,300
What do you think, Stefano?

271
00:19:15,300 --> 00:19:23,100
So I think this is where you will start presenting, I think.

272
00:19:23,100 --> 00:19:33,460
>> I was looking at the -- we can answer Justin.

273
00:19:33,460 --> 00:19:34,460
>> Okay.

274
00:19:34,460 --> 00:19:35,460
Let's do that.

275
00:19:35,460 --> 00:19:38,260
>> He admitted -- no, no.

276
00:19:38,260 --> 00:19:41,100
He admitted that it's outside of this.

277
00:19:41,100 --> 00:19:42,100
>> Okay.

278
00:19:42,100 --> 00:19:43,100
Great.

279
00:19:44,100 --> 00:19:53,700
>> So this is the text that is now in the draft of -- in the draft version 6.

280
00:19:53,700 --> 00:19:54,700
006.

281
00:19:54,700 --> 00:19:57,020
Which is going to go live on Monday.

282
00:19:57,020 --> 00:19:59,780
So it's basically completed.

283
00:19:59,780 --> 00:20:09,420
And the narrative in here is to have a piece of text inside the definition of that section

284
00:20:09,420 --> 00:20:11,540
of what is open source AI.

285
00:20:11,540 --> 00:20:16,260
Where we describe what we actually need in order to exercise those four freedoms.

286
00:20:16,260 --> 00:20:19,260
To use study, share, modify.

287
00:20:19,260 --> 00:20:25,500
And that's where the text that I was talking about before, the sufficiently detailed information

288
00:20:25,500 --> 00:20:36,580
of how the system was trained, and the components basically that the groups have said that they

289
00:20:36,580 --> 00:20:37,580
require.

290
00:20:37,580 --> 00:20:43,300
Here they are described in a more verbose way, more of a narrative rather than bullet

291
00:20:43,300 --> 00:20:52,900
points talking about quoting, mentioning specifically components that are referenced in a separate

292
00:20:52,900 --> 00:20:56,740
paper by -- or maintained by another organization.

293
00:20:56,740 --> 00:21:05,620
So this is more about describing in a more -- in a fairly precise fashion the list of

294
00:21:05,620 --> 00:21:10,100
components that require components that came out of the working group.

295
00:21:10,100 --> 00:21:14,140
And here you can see, like, you need to have detailed information of how the system was

296
00:21:14,140 --> 00:21:15,140
trained.

297
00:21:15,140 --> 00:21:19,420
Providence of the data, the scope and characteristics.

298
00:21:19,420 --> 00:21:25,980
Some of these wording also comes from the EU AI Act in terms of requirements of transparency.

299
00:21:25,980 --> 00:21:34,500
So I tried to use the words included in that legislation so that thinking that it's sufficiently

300
00:21:34,500 --> 00:21:36,860
-- it's been reviewed.

301
00:21:36,860 --> 00:21:37,860
It's been debated.

302
00:21:37,860 --> 00:21:43,340
It's been agreed upon by the -- at least by the European regulators.

303
00:21:43,340 --> 00:21:48,980
And negotiated heavily.

304
00:21:48,980 --> 00:21:56,380
So I'm assuming it's quite fine-tuned to represent transparency requirements in a fairly

305
00:21:56,380 --> 00:21:57,380
detailed way.

306
00:21:57,380 --> 00:22:01,300
So at least it's a good way to start the conversation for us.

307
00:22:01,300 --> 00:22:08,380
And then in terms of code, pieces that are about the preprocessing data, the code used

308
00:22:08,380 --> 00:22:15,300
for the training validation, and the supporting libraries.

309
00:22:15,300 --> 00:22:21,260
And the model parameters, the weights, which should also include the checkpoints for the

310
00:22:21,260 --> 00:22:23,660
intermediate stages of the training.

311
00:22:23,660 --> 00:22:25,180
As well as the finalized one.

312
00:22:25,180 --> 00:22:34,420
So this is what's in -- you will see on Monday on draft six as it gets published.

313
00:22:34,420 --> 00:22:40,100
You want to go on to the next one?

314
00:22:40,100 --> 00:22:46,140
Because this is the other thing that will be clear.

315
00:22:46,140 --> 00:22:47,140
Spelled out.

316
00:22:47,140 --> 00:22:56,140
Like the precondition that we need to focus on, like a very, very high-level necessary

317
00:22:56,140 --> 00:23:02,220
thing, necessary feature that needs to be made available is to have the preferred form

318
00:23:02,220 --> 00:23:05,100
to make modifications to the system.

319
00:23:05,100 --> 00:23:12,540
And that, for machine learning, the examples that we have studied, they give us the list

320
00:23:12,540 --> 00:23:13,540
of components.

321
00:23:13,540 --> 00:23:22,020
This sentence here is a form to make the open source AI definition a little bit more flexible

322
00:23:22,020 --> 00:23:28,380
and adaptable to other technologies should they change next year or two years from now.

323
00:23:28,380 --> 00:23:30,900
At least in the short term.

324
00:23:30,900 --> 00:23:32,500
Give us a little bit more flexibility.

325
00:23:32,500 --> 00:23:39,700
And that's what we're going to do.

326
00:23:39,700 --> 00:23:45,620
So the next step is -- the very immediate next step is to finalize the -- hit the release

327
00:23:45,620 --> 00:23:52,100
button and go live with version six of the draft on Monday.

328
00:23:52,100 --> 00:24:00,100
And then we're going to start the second step in track one, which is to review each of the

329
00:24:00,100 --> 00:24:02,380
systems that we have already analyzed.

330
00:24:02,380 --> 00:24:03,380
Maybe add some more.

331
00:24:03,380 --> 00:24:06,700
I'm not against adding more at this stage.

332
00:24:06,700 --> 00:24:11,260
Because we need to look at the required components that we think are necessary.

333
00:24:11,260 --> 00:24:15,260
The working groups have identified as necessary.

334
00:24:15,260 --> 00:24:16,460
Make a list.

335
00:24:16,460 --> 00:24:21,420
And for each of the components, put the URLs, you know, where can I get them?

336
00:24:21,420 --> 00:24:23,300
Where can I get it?

337
00:24:23,300 --> 00:24:27,180
If it's not available, then mark it as unavailable.

338
00:24:27,180 --> 00:24:30,180
And if we can get them, under what conditions?

339
00:24:30,180 --> 00:24:35,380
So we're going to find out -- I can already say we know.

340
00:24:35,380 --> 00:24:41,380
For Lama 2, we know that in order to download the model weights, you will have to -- you

341
00:24:41,380 --> 00:24:45,100
will have to sign up on a website.

342
00:24:45,100 --> 00:24:47,820
You need to give in your details.

343
00:24:47,820 --> 00:24:51,700
You need to ask for permission.

344
00:24:51,700 --> 00:24:55,620
And you need to sign an agreement by doing so.

345
00:24:55,620 --> 00:25:00,300
And that agreement has specifications of what you can and cannot do.

346
00:25:00,300 --> 00:25:04,340
And we'll log all of that information in a place.

347
00:25:04,340 --> 00:25:07,340
And we'll analyze them in step three.

348
00:25:07,340 --> 00:25:11,460
PTR will have different things, et cetera, et cetera.

349
00:25:11,460 --> 00:25:12,460
Oh, in fact, yes.

350
00:25:12,460 --> 00:25:16,260
I put together -- there is a slide with -- go ahead.

351
00:25:16,260 --> 00:25:17,780
Go to the next one.

352
00:25:17,780 --> 00:25:18,780
Yes.

353
00:25:18,780 --> 00:25:23,460
That's the -- that's what the next table, the next working groups will do.

354
00:25:23,460 --> 00:25:30,420
It's basically go through the list of required components, put the URL, and match the legal

355
00:25:30,420 --> 00:25:31,420
framework.

356
00:25:31,420 --> 00:25:34,500
Now, you will see that here we're talking mostly about code.

357
00:25:34,500 --> 00:25:36,500
And there is one line.

358
00:25:36,500 --> 00:25:41,460
So for the code parts, it's going to be easy to say that all the code made available needs

359
00:25:41,460 --> 00:25:46,780
to be formally using an OSI-approved license.

360
00:25:46,780 --> 00:25:49,740
And I don't think it's going to be that complicated, that part.

361
00:25:49,740 --> 00:25:52,740
The model parameters instead.

362
00:25:52,740 --> 00:26:00,100
That part is most likely going to raise a conversation around what legal frameworks

363
00:26:00,100 --> 00:26:02,700
go around the parameters.

364
00:26:02,700 --> 00:26:09,300
Those are -- there is still some, from what I hear from the legal communities, the lawyers

365
00:26:09,300 --> 00:26:17,500
have diverging opinions whether parameters are copyrightable.

366
00:26:17,500 --> 00:26:21,940
And if they're not copyrightable, what kind of other -- if there is any other exclusive

367
00:26:21,940 --> 00:26:26,700
in property regimes or not.

368
00:26:26,700 --> 00:26:34,220
And therefore, what kind of contracts -- the validity of the contracts or terms of use

369
00:26:34,220 --> 00:26:38,700
and other legal tools.

370
00:26:38,700 --> 00:26:45,340
Whether -- you know, which ones are better or valid, et cetera.

371
00:26:45,340 --> 00:26:51,340
So it's going to spin off an interesting legal conversation.

372
00:26:51,340 --> 00:26:53,340
>> Sorry.

373
00:26:53,340 --> 00:26:57,900
I clicked early.

374
00:26:57,900 --> 00:27:03,180
I didn't want to raise my hand before you were done.

375
00:27:03,180 --> 00:27:06,180
>> No, you're good.

376
00:27:06,180 --> 00:27:07,180
Go.

377
00:27:07,180 --> 00:27:08,180
>> Okay.

378
00:27:08,180 --> 00:27:15,260
So the first question I have, which is what I meant to ask earlier, was what constitutes

379
00:27:15,260 --> 00:27:21,540
data preprocessing versus a new dataset?

380
00:27:21,540 --> 00:27:25,980
Like is it -- if it's done internally to the company that is making the model, that's when

381
00:27:25,980 --> 00:27:28,180
it's data preprocessing?

382
00:27:28,180 --> 00:27:35,180
Or -- yeah, I guess basically how are we differentiating between those?

383
00:27:35,180 --> 00:27:40,900
>> No, let me look at the paper.

384
00:27:40,900 --> 00:27:47,300
The paper, it will go live later today.

385
00:27:47,300 --> 00:27:48,980
And definitely on Monday.

386
00:27:48,980 --> 00:27:52,940
But my memory serves me right.

387
00:27:52,940 --> 00:28:00,420
The data preprocessing is the tooling that is used to do things like cleaning up, formatting

388
00:28:00,420 --> 00:28:10,580
the data, and tokenizing, you know, doing the -- all the preparation work that goes

389
00:28:10,580 --> 00:28:17,540
into prepare the data to be fed into -- for data ingestion, for example.

390
00:28:17,540 --> 00:28:18,540
>> Right.

391
00:28:18,540 --> 00:28:24,180
>> Feature engineering, you know, data, yeah, all of that code.

392
00:28:24,180 --> 00:28:25,180
>> Right.

393
00:28:25,180 --> 00:28:26,580
Oh, I guess there's a follow-up.

394
00:28:26,580 --> 00:28:31,460
But I'll pause if you want to go to someone else.

395
00:28:31,460 --> 00:28:32,460
>> What do you mean?

396
00:28:32,460 --> 00:28:33,460
There's overlap?

397
00:28:33,460 --> 00:28:37,700
>> Sorry, Mer, I couldn't hear you.

398
00:28:37,700 --> 00:28:43,900
>> Yeah, you can -- yeah, let's -- yeah, good facilitation practice.

399
00:28:43,900 --> 00:28:46,500
Let me go to Mo and then we can come back to this.

400
00:28:46,500 --> 00:28:53,460
Mo says model architecture is defined in the code.

401
00:28:53,460 --> 00:28:56,860
It overlaps with the training and validation and testing code.

402
00:28:56,860 --> 00:29:02,700
Is model architecture really an independent component that can be analyzed individually?

403
00:29:02,700 --> 00:29:09,940
Do you want to -- >> Once we can -- yeah, I think that once

404
00:29:09,940 --> 00:29:22,380
we can point at the paper that so far we've been using generously through pre-preview,

405
00:29:22,380 --> 00:29:29,820
we can have a better understanding of these technical overlaps, et cetera.

406
00:29:29,820 --> 00:29:32,540
>> Yeah, the delineation between different components.

407
00:29:32,540 --> 00:29:38,100
So yeah, part of this paper is that there's often multi-paragraph definitions of each

408
00:29:38,100 --> 00:29:39,340
of these components.

409
00:29:39,340 --> 00:29:43,140
So yeah, I think that's the best place to go for that.

410
00:29:43,140 --> 00:29:46,860
Jacob, did you want to ask a follow-up?

411
00:29:46,860 --> 00:29:49,660
>> Yeah.

412
00:29:49,660 --> 00:29:57,860
So I guess then at least how I'm thinking about this data pre-processing and I will

413
00:29:57,860 --> 00:30:03,740
be like be open -- I'm coming at this from an adversarial perspective intentionally because

414
00:30:03,740 --> 00:30:08,900
I feel like that will be done, I guess.

415
00:30:08,900 --> 00:30:19,820
And so basically my thought is if we don't have to know -- like if a third party can

416
00:30:19,820 --> 00:30:27,100
do the pre-processing, not in terms of tokenization necessarily, but more of like sifting the

417
00:30:27,100 --> 00:30:40,620
data and can create a new dataset that is proprietary and then you use that, that's

418
00:30:40,620 --> 00:30:46,540
a way to obfuscate your data usage.

419
00:30:46,540 --> 00:30:50,980
And so maybe that's not relevant here.

420
00:30:50,980 --> 00:30:57,900
Maybe that goes too much into legality, but that's just sort of how I'm thinking about

421
00:30:57,900 --> 00:31:02,340
it and I'm curious to hear what your response is.

422
00:31:02,340 --> 00:31:05,660
>> It seems like it's covered by the first bullet.

423
00:31:05,660 --> 00:31:08,340
Isn't a lot of that concern covered by the first bullet?

424
00:31:08,340 --> 00:31:11,900
There's just so much documentation about data that's required?

425
00:31:11,900 --> 00:31:14,900
Or what do you think, Stefano?

426
00:31:14,900 --> 00:31:22,340
>> If I understand correctly, the question or the concern, it's about replicating the

427
00:31:22,340 --> 00:31:34,020
dataset with different setups, different things in it.

428
00:31:34,020 --> 00:31:39,900
Or different way of reorganizing, reshuffling the same -- using the pre-processing code

429
00:31:39,900 --> 00:31:45,740
means that you have a way to rebuild -- it doesn't mean that you have access to the original

430
00:31:45,740 --> 00:31:47,260
data.

431
00:31:47,260 --> 00:31:53,540
You can -- you have a way to extrapolate, you have those transparency requirements that

432
00:31:53,540 --> 00:32:00,340
make you -- that give you some sort of better understanding for how you would have to build

433
00:32:00,340 --> 00:32:07,300
your own dataset for your own training if you want to do -- if you want to rebuild from

434
00:32:07,300 --> 00:32:12,580
scratch or if you want to build from -- not rebuild from scratch, build from scratch something

435
00:32:12,580 --> 00:32:20,260
else that looks or behaves similar to what you have received.

436
00:32:20,260 --> 00:32:27,180
In other words, I don't think that the scenario that you're describing is a scenario that

437
00:32:27,180 --> 00:32:30,260
necessarily is part of this conversation.

438
00:32:30,260 --> 00:32:31,700
>> Gotcha.

439
00:32:31,700 --> 00:32:32,700
Okay.

440
00:32:32,700 --> 00:32:39,020
Let's go on and see what else comes up.

441
00:32:39,020 --> 00:32:47,260
>> Yeah, because I think some of these questions can be -- yeah, can be described also in what

442
00:32:47,260 --> 00:32:48,260
we're doing.

443
00:32:48,260 --> 00:32:53,180
Another thing -- another of the things that we're doing next and partially I can talk

444
00:32:53,180 --> 00:32:59,380
about here the roadshow that now it's not detailed here, but you can see on the timeline

445
00:32:59,380 --> 00:33:05,980
we have these three tracks, the green, the white, and the light blue that we're following

446
00:33:05,980 --> 00:33:06,980
in parallel.

447
00:33:06,980 --> 00:33:09,820
And in June we're going to have the release candidate.

448
00:33:09,820 --> 00:33:16,140
Once that release candidate happens, we'll have meetings in different parts of the world

449
00:33:16,140 --> 00:33:23,780
and we're organizing them to show the release candidate to gain more visibility across different

450
00:33:23,780 --> 00:33:30,020
communities and different practitioners, different stakeholders in order to get to release candidate

451
00:33:30,020 --> 00:33:32,820
to version 1 in October.

452
00:33:32,820 --> 00:33:39,380
During these meetings, we're going to spin off a conversation about data because there

453
00:33:39,380 --> 00:33:50,940
is a strong requirement for good quality data sets and an increased amount of awareness

454
00:33:50,940 --> 00:34:01,580
in the practice of building data sets that are valid, respectful, trustworthy, you know,

455
00:34:01,580 --> 00:34:10,500
they are clean, they're transparent, they're fair in how -- what they represent, et cetera.

456
00:34:10,500 --> 00:34:12,740
And there is very few of these.

457
00:34:12,740 --> 00:34:18,180
There are very few data sets that are large enough, that are good enough -- that are good

458
00:34:18,180 --> 00:34:20,300
in that term.

459
00:34:20,300 --> 00:34:28,980
And what's becoming more clear to me also is that the data community hasn't been -- needs

460
00:34:28,980 --> 00:34:32,460
to also -- to have these conversations.

461
00:34:32,460 --> 00:34:37,860
So we're going to be partnering with organizations that are more into that data space and we're

462
00:34:37,860 --> 00:34:45,580
going to be helping supporting conversations around data in parallel or, you know, as a

463
00:34:45,580 --> 00:34:50,300
spin off of this project.

464
00:34:50,300 --> 00:34:54,100
>> Jacob, go ahead.

465
00:34:54,100 --> 00:34:57,340
>> Yeah, sorry.

466
00:34:57,340 --> 00:34:58,380
Last one.

467
00:34:58,380 --> 00:35:03,140
You touched on this just now, but I guess one thing that I've been thinking about a

468
00:35:03,140 --> 00:35:11,140
lot is how do we -- or I guess OSI maybe -- take back the term?

469
00:35:11,140 --> 00:35:22,580
Because I feel like it's been falsely attributed to systems that are pretty clearly not open

470
00:35:22,580 --> 00:35:26,460
source by this definition.

471
00:35:26,460 --> 00:35:31,780
And like you said, you're going to go and, you know, talk with more people and have those

472
00:35:31,780 --> 00:35:32,780
meetings.

473
00:35:32,780 --> 00:35:36,940
But, yeah, just curious what your thoughts are about that.

474
00:35:36,940 --> 00:35:42,380
>> Yeah, this is a known issue, unfortunately.

475
00:35:42,380 --> 00:35:43,980
Yes, exactly.

476
00:35:43,980 --> 00:35:47,740
Having a definition helps to take back the term.

477
00:35:47,740 --> 00:35:53,020
And having people, enough people who support it and are willing to say we're going to use

478
00:35:53,020 --> 00:35:54,020
it.

479
00:35:54,020 --> 00:35:55,580
This is what we mean when we say open source AI.

480
00:35:55,580 --> 00:35:59,140
And they're going to be multiplying the idea.

481
00:35:59,140 --> 00:36:02,660
You know, they're going to go -- so I'll tell you an anecdote.

482
00:36:02,660 --> 00:36:12,700
I was in -- I was at a meeting last week with a lot of friends of open in general.

483
00:36:12,700 --> 00:36:16,420
And there was a lot of pushback.

484
00:36:16,420 --> 00:36:26,940
Common shared pushback against Meta's use of the open source AI name to identify, to

485
00:36:26,940 --> 00:36:27,940
talk about Lama.

486
00:36:27,940 --> 00:36:29,380
Lama2 specifically.

487
00:36:29,380 --> 00:36:38,220
So it's -- many others will be joining in our effort to clarify the public what that

488
00:36:38,220 --> 00:36:39,220
means.

489
00:36:39,220 --> 00:36:44,260
And it's a little bit the same work that -- the same way that it happens with the open source

490
00:36:44,260 --> 00:36:45,260
software.

491
00:36:45,260 --> 00:36:51,900
Like lots of -- there is people -- some people still complain and say, well, but I mean something

492
00:36:51,900 --> 00:36:52,900
else.

493
00:36:52,900 --> 00:36:57,580
But there is always a very large pushback of supporters of the open source definition who

494
00:36:57,580 --> 00:37:06,180
say, no, it's -- you may have all the opinions you want, but open source is defined by the

495
00:37:06,180 --> 00:37:07,180
open source initiative.

496
00:37:07,180 --> 00:37:11,180
And it's maintained by the open source initiative.

497
00:37:11,180 --> 00:37:19,620
And that brings the cloud up.

498
00:37:19,620 --> 00:37:28,580
Yeah, so that's a big thing.

499
00:37:28,580 --> 00:37:30,660
This doesn't end with version 1.

500
00:37:30,660 --> 00:37:38,860
We know that we will have -- most likely it's going to be -- it's going to have to be maintained

501
00:37:38,860 --> 00:37:41,640
and improved and reviewed.

502
00:37:41,640 --> 00:37:49,640
So the board of the OSI is already at work to think about the -- how we're going to be

503
00:37:49,640 --> 00:37:53,040
maintaining -- we're going to be maintaining this definition.

504
00:37:53,040 --> 00:38:03,000
Which is very different from what the open source definition for software is.

505
00:38:03,000 --> 00:38:07,920
Right?

506
00:38:07,920 --> 00:38:19,200
We're trying to have -- so someone was asking before I saw the meetings and -- oh, it was

507
00:38:19,200 --> 00:38:20,200
Justin.

508
00:38:20,200 --> 00:38:22,240
Yeah, we're talking about meetings, et cetera.

509
00:38:22,240 --> 00:38:24,320
And how to participate.

510
00:38:24,320 --> 00:38:29,800
So we're trying to have everything public.

511
00:38:29,800 --> 00:38:34,360
So that no one gets surprised when at the end we come up with the definition.

512
00:38:34,360 --> 00:38:40,360
We're having this sequence of town halls every two weeks at different times so that we can

513
00:38:40,360 --> 00:38:44,160
accommodate multiple time zones.

514
00:38:44,160 --> 00:38:47,880
We have discussions on the forums.

515
00:38:47,880 --> 00:38:54,360
We do publish on the blog every week a summary of what's happening on the forums.

516
00:38:54,360 --> 00:38:59,560
So there are multiple ways to get involved.

517
00:38:59,560 --> 00:39:06,360
To stay aware and to give comments during this process.

518
00:39:06,360 --> 00:39:09,160
We'll publish the roadshow.

519
00:39:09,160 --> 00:39:13,120
Also the roadshow meetings are going to be in different parts of the world.

520
00:39:13,120 --> 00:39:18,560
We have partnered with existing conferences.

521
00:39:18,560 --> 00:39:23,720
In order to gather people who are already being at a place.

522
00:39:23,720 --> 00:39:25,240
Already going to that place.

523
00:39:25,240 --> 00:39:33,560
So we're going to be in -- at the -- well, we'll publish it next week.

524
00:39:33,560 --> 00:39:34,560
Or the week after.

525
00:39:34,560 --> 00:39:42,160
But I can say that we're going to touch every continent starting from June.

526
00:39:42,160 --> 00:39:43,640
Starting from June until October.

527
00:39:43,640 --> 00:39:47,800
So we're going to be in Africa in June.

528
00:39:47,800 --> 00:39:50,800
We're going to be in September.

529
00:39:50,800 --> 00:39:54,520
We're going to be in Hong Kong.

530
00:39:54,520 --> 00:39:57,720
Europe, Paris, North America, Mexico.

531
00:39:57,720 --> 00:40:01,080
I don't remember when.

532
00:40:01,080 --> 00:40:06,040
So there's many, many opportunities to meet in person.

533
00:40:06,040 --> 00:40:09,760
And we're raising also funds and disclosing this.

534
00:40:09,760 --> 00:40:16,440
By the hope is that we're going to have also travel grants for people who want to participate.

535
00:40:16,440 --> 00:40:19,400
And they can apply and get to some of these meetings.

536
00:40:19,400 --> 00:40:24,200
So hopefully we'll be able to have a very good roadshow.

537
00:40:24,200 --> 00:40:28,560
And gather a lot of support.

538
00:40:28,560 --> 00:40:31,520
And if not, we're going to keep on going next year also.

539
00:40:31,520 --> 00:40:33,520
Multiple opportunities.

540
00:40:33,520 --> 00:40:41,200
>> Thank you, Sam, for offering to host us in Toronto.

541
00:40:41,200 --> 00:40:42,200
Okay.

542
00:40:42,200 --> 00:40:43,200
Yes.

543
00:40:43,200 --> 00:40:47,200
And thank you for your comment, Jacob.

544
00:40:47,200 --> 00:40:48,200
Okay.

545
00:40:48,200 --> 00:40:51,200
Let's see what else we have in the deck.

546
00:40:51,200 --> 00:40:52,200
Oh!

547
00:40:52,200 --> 00:40:53,200
Q&A.

548
00:40:53,200 --> 00:40:56,200
>> I haven't really started.

549
00:40:56,200 --> 00:40:57,200
So yeah.

550
00:40:57,200 --> 00:40:59,040
That's the end of our deck.

551
00:40:59,040 --> 00:41:01,360
So we do have more time in the meeting.

552
00:41:01,360 --> 00:41:04,120
If anyone else has a question.

553
00:41:04,120 --> 00:41:10,600
And this would be a great time if you're someone who often doesn't ask a question in a meeting.

554
00:41:10,600 --> 00:41:11,600
Please do.

555
00:41:11,600 --> 00:41:12,640
We really invite you to.

556
00:41:12,640 --> 00:41:18,840
And if you prefer to put it in the chat rather than to speak publicly, then that is also

557
00:41:18,840 --> 00:41:20,160
most available to you.

558
00:41:20,160 --> 00:41:25,160
>> All right.

559
00:41:25,160 --> 00:41:49,360
Jacob, thank you for sharing the information.

560
00:41:49,360 --> 00:41:53,640
You're welcome to join the forums if you're not already.

561
00:41:53,640 --> 00:41:56,160
They're easy to use.

562
00:41:56,160 --> 00:41:58,080
And should be smooth.

563
00:41:58,080 --> 00:42:03,200
And once you're a you can become a free member of the OSI to join the forums.

564
00:42:03,200 --> 00:42:04,880
It's very easy.

565
00:42:04,880 --> 00:42:05,880
>> Yes.

566
00:42:05,880 --> 00:42:07,200
I joined earlier this week.

567
00:42:07,200 --> 00:42:10,360
I had some troubles with logging in.

568
00:42:10,360 --> 00:42:11,360
But I got that fixed.

569
00:42:11,360 --> 00:42:12,360
So...

570
00:42:12,360 --> 00:42:13,360
>> Cheers.

571
00:42:13,360 --> 00:42:14,360
>> Thank you, guys.

572
00:42:14,360 --> 00:42:15,360
>> Thank you.

573
00:42:15,360 --> 00:42:16,360
>> Bye.

574
00:42:20,360 --> 00:42:21,360
>> See.

575
00:42:21,360 --> 00:42:22,360
Just in typing.

576
00:42:22,360 --> 00:42:23,360
But I'll wait.

577
00:42:23,360 --> 00:42:24,360
But I can stop the recording at this point.

578
00:42:24,360 --> 00:42:24,360
I think.

579
00:42:24,360 --> 00:42:25,360
Okay.

580
00:42:25,360 --> 00:42:26,360
So...

581
00:42:26,360 --> 00:42:27,360
I'm going to stop the recording.

### End of last town hall held on 2024-03-08 ###

### Start of next town hall held on 2024-03-22 ###
--- Presentation for 2024-03-22 ---
OPEN SOURCE AI DEFINITION
Online public townhall
March 22, 2024
last updated: March 21, 2024 (SM)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

The objective for 2024

Open Source AI Deﬁnition
version 1.0

3

hackmd.io/@opensourceinitiative/osaid-0-0-5

Deﬁnition of AI system

Preamble
Out of scope issues
4 freedoms
Legal checklist

4

Deﬁnition of AI system

Preamble

Done … ish?

Out of scope issues
4 freedoms
Legal terms checklist

Working on

What is Open Source AI
An Open Source AI is an AI system made available to the public under terms that grant the freedoms to:
●
●
●
●

Use the system for any purpose and without having to ask for permission.
Study how the system works and inspect its components.
Modify the system for any purpose, including to change its output.
Share the system for others to use with or without modifications, for any purpose.

Data: transparency
requirements only

Precondition to exercise these freedoms is to have access to the preferred form to make modifications to the system. For machine learning
systems that means having public access to:
●
●
●

Data: Sufficiently detailed information on how the system was trained, including the training methodologies and techniques, the training
data sets used, information about the provenance of those data sets, their scope and characteristics; how the data was obtained and
selected, the labeling procedures and data cleaning methodologies.
Code: The code used for pre-processing data, the code used for training, validation and testing, the supporting libraries like tokenizers
and hyperparameters search code (if used), the inference code, and the model architecture.
Model: The model parameters, including weights. Where applicable, these should include checkpoints from key intermediate stages of
training as well as the final optimizer state.

We need to talk about “systems”
because openness is a combination
of availability of multiple artifacts

Alt: we talk about “open
weights” only

What phase 2 will look like
For each AI system, build a table like:
Required component

Link to resource

Legal framework

Data pre-processing code

URL

OSI-approved license

Training, validation and testing code

URL

…

Inference code

URL

…

Supporting libraries and tools

URL

…

Model architecture

URL

…

Model parameters

URL

???
7

Getting the speciﬁcations
AI systems

Active working
groups:
- Llama2
- Pythia
Setting up:
- BLOOM
- OpenCV

List of
components

Legal
frameworks

Legal
documents

Checklist

What elements are
necessary to:
- use
- study
- modify
- share
an AI system?

For each artifact,
evaluate which
laws apply. Some
will be under
“Intellectual
Property” regimes,
some will be under
other regimes.

We’ll match the
components and
the identiﬁed legal
frameworks with
the terms of the
legal documents
already in use,
where available.

After repeating
this exercise
enough times,
we’ll be able to
generalize the
outcomes and
write the specs to
evaluate the
freedoms granted.

2024 timeline

System testing work stream
Stakeholder consultation work stream
Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

Draft 0.0.8

Townhall +

… October
Monthly Virtual
Meetings

Release
version 1.0

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

v. 1.0

What phase 2 will look like
For each AI system, build a table like:
Required component

Link to resource

Legal framework

Data pre-processing code

URL

OSI-approved license

Training, validation and testing code

URL

…

Inference code

URL

…

Supporting libraries and tools

URL

…

Model architecture

URL

…

Model parameters

URL

???
10

Deep Dive AI in-person meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

PyCon US

May 20 - 23

Europe

?

Africa

Nigeria

Abuja

OSCA

June 6 - 8

Latin America

Mexico

Mexico D.F.

Latam OSS

July 19 - 20

Asia Paciﬁc

Hong Kong

Hong Kong

AI_dev

August 23

North America

United States

Raleigh

All Things Open

Oct 27 - 29

May ?

Join the conversation
● discuss.opensource.org
● Public forum
● Join as OSI member
○ Free or full
○ SSO with other OSI
websites

12

Q&A

13

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the
deﬁnitional process.

14

Criteria for RC1 and v. 1.0
RC1

version 1

- Expected outcome of
in-person meeting end
May/early June!
- The draft is completed
in all its parts
- The draft is supported
by at least 2
representatives for
each of the 6
stakeholder groups

- Expected outcome of
in-person and online
meetings through the
summer/early autumn
- The draft is endorsed by at
least 5 reps for each of the
stakeholder groups
- Announced in late October

15

Help us ﬁnd stakeholders
System Creator

License Creator

Regulator

Licensee

End User

Subject

Makes AI system
and/or
component that
will be studied,
used, modified,
or shared
through an open
source license
(e.g., ML
researcher in
academia or
industry)

Writes or edits
the open source
license to be
applied to the AI
system or
component;
includes
compliance
(e.g., IP lawyer)

Writes or edits
rules governing
licenses and
systems (e.g.
government
policy-maker)

Seeks to study,
use modify, or
share an open
source AI
system (e.g. AI
engineer, health
researcher,
education
researcher)

Consumes a
system output,
but does not
seek to study,
use, modify, or
share the
system (e.g.,
student using a
chatbot to write
a report, artist
creating an
image)

Affected
upstream or
downstream by
a system output
without
interacting with it
intentionally;
includes
advocates for
this group (e.g.
people with loan
denied, or
content creators)

✅

✅

⚠

✅

⚠

⚠

Enough to start

Enough to start

Leads to US, EU,
Singapore, no
commitment yet

Enough to start

Which org is squarely in
this space?

ACLU, Algorithmic
Justice League

16


--- Subtitles for 2024-03-22 --- ###
1
00:00:00,001 --> 00:00:09,480
All right, everyone, welcome to this, I don't know how many we've done of this already,

2
00:00:09,480 --> 00:00:17,100
public town hall to get an update on the process to define open source AI.

3
00:00:17,100 --> 00:00:22,720
As you all know, this is a long and hard process.

4
00:00:22,720 --> 00:00:28,860
The agreements that we have in here, I like to remind everyone, and especially I'm fond

5
00:00:28,860 --> 00:00:32,100
of the fact that we're moving forward.

6
00:00:32,100 --> 00:00:37,360
We must be keep going and we must reach a conclusion.

7
00:00:37,360 --> 00:00:44,100
And if we find something that is blocking the conversation, we take notes, we try to

8
00:00:44,100 --> 00:00:46,600
move on, we'll get back to it later.

9
00:00:46,600 --> 00:00:52,740
And we want to find a solution which is really relevant for delivering something on time

10
00:00:52,740 --> 00:00:56,440
and reaching agreements.

11
00:00:56,440 --> 00:01:04,680
This is what we set for our objective, and I'd like to remind this.

12
00:01:04,680 --> 00:01:13,640
We want to get to a workable, usable, agreed upon definition of open source AI.

13
00:01:13,640 --> 00:01:18,760
This is absolutely the goal and objective.

14
00:01:18,760 --> 00:01:28,960
And I got to say that I'm getting more comfortable that we're reaching a conclusion.

15
00:01:28,960 --> 00:01:30,960
And in fact, in...

16
00:01:30,960 --> 00:01:35,800
Okay, got a wrong slide here.

17
00:01:35,800 --> 00:01:36,800
Okay.

18
00:01:36,800 --> 00:01:39,360
And my system is really slow.

19
00:01:39,360 --> 00:01:40,360
Okay.

20
00:01:40,360 --> 00:01:48,960
So here's the status on the structure of the definition, which many of you have already

21
00:01:48,960 --> 00:01:50,080
seen.

22
00:01:50,080 --> 00:01:56,280
It's made of a definition of AI system at the beginning, the preamble where we set the

23
00:01:56,280 --> 00:02:01,840
rules, we set the principles, actually, the principles that we want and why we want to

24
00:02:01,840 --> 00:02:08,640
have a definition, what value and what advantages we think that's going to bring to the world

25
00:02:08,640 --> 00:02:11,040
or to the ecosystem in general.

26
00:02:11,040 --> 00:02:16,280
We also set clear out of scope, and so the boundaries of what's in scope for a definition,

27
00:02:16,280 --> 00:02:17,960
what's left out.

28
00:02:17,960 --> 00:02:24,120
And finally, the four freedoms, which are the basic understanding, the most succinct

29
00:02:24,120 --> 00:02:29,160
way of communicating what is an open source AI.

30
00:02:29,160 --> 00:02:32,520
And below that, we're going to be...

31
00:02:32,520 --> 00:02:39,520
We're working on defining the legal terms, and we'll talk about the draft six, which

32
00:02:39,520 --> 00:02:42,520
was released at the beginning of the month.

33
00:02:42,520 --> 00:02:47,480
And I think it is going to give us a clear understanding.

34
00:02:47,480 --> 00:02:55,560
So what I want to say here is that it looks like very few comments are coming in on the

35
00:02:55,560 --> 00:03:03,000
top parts of the document, like the draft, preamble, out of scope, AI system definition,

36
00:03:03,000 --> 00:03:07,760
and for freedom seem to be fairly set, especially the first three.

37
00:03:07,760 --> 00:03:13,760
The fourth, maybe some small details, but it looks like these parts of the document

38
00:03:13,760 --> 00:03:14,760
look done.

39
00:03:14,760 --> 00:03:21,840
I really encourage you all to reread also the top part.

40
00:03:21,840 --> 00:03:24,280
Don't get fixated on what's working on.

41
00:03:24,280 --> 00:03:26,880
The more...

42
00:03:26,880 --> 00:03:33,120
I'd love to get a sense of what's happening, and if really we can consider the top parts

43
00:03:33,120 --> 00:03:37,320
complete, and if they're going to be...

44
00:03:37,320 --> 00:03:40,920
So that we avoid having surprises at the last minute.

45
00:03:40,920 --> 00:03:47,680
Now on the legal check terms, I mean, legal terms, the checklist, let's talk about what's

46
00:03:47,680 --> 00:03:53,440
in draft six, which is very, very new.

47
00:03:53,440 --> 00:03:58,840
The four freedoms haven't been changed at all.

48
00:03:58,840 --> 00:04:05,460
What's added in draft zero six is the statement of the precondition.

49
00:04:05,460 --> 00:04:08,240
What is that you need to exercise the freedom?

50
00:04:08,240 --> 00:04:15,440
What is the fact, the statement that you need access to the preferred form to make modifications

51
00:04:15,440 --> 00:04:16,440
to the system?

52
00:04:16,440 --> 00:04:23,320
This is a sentence that is quite well understood in the realm of software.

53
00:04:23,320 --> 00:04:29,880
We know because of practice, because it's been defined in various licenses, that access

54
00:04:29,880 --> 00:04:35,960
to the preferred form to make modifications to a program, software program, you need to

55
00:04:35,960 --> 00:04:43,440
have access not only to the source code, but also documentation on how to rebuild it, and

56
00:04:43,440 --> 00:04:45,480
the tools and scripts to build.

57
00:04:45,480 --> 00:04:52,600
So you need to have the compilation, for example, the make files and other pieces.

58
00:04:52,600 --> 00:05:01,360
The GPLD3, for example, has a very detailed list of what's necessary to make modifications

59
00:05:01,360 --> 00:05:02,360
to the program.

60
00:05:02,360 --> 00:05:08,800
Now for machine learning systems, and we focus on machine learning here because as an example

61
00:05:08,800 --> 00:05:18,160
of what an AI system needs in order to have the preferred form to make modifications,

62
00:05:18,160 --> 00:05:19,160
you need three things.

63
00:05:19,160 --> 00:05:25,040
Well, three sets of things, three categories of things.

64
00:05:25,040 --> 00:05:27,080
Let's start from the bottom.

65
00:05:27,080 --> 00:05:33,480
On the model front, we need model parameters, including the weights.

66
00:05:33,480 --> 00:05:45,040
And maybe in some cases, we may need checkpoints for the stages of training.

67
00:05:45,040 --> 00:05:47,640
And that's for the model requirement.

68
00:05:47,640 --> 00:05:49,760
Then there is software that we need.

69
00:05:49,760 --> 00:05:57,360
And the software that we need is software used for pre-processing the data, the software

70
00:05:57,360 --> 00:06:03,640
used for training, validation, testing, and supporting libraries for the execution, like

71
00:06:03,640 --> 00:06:08,840
the tokenizers, the search code, if there are the hyperparameter search code, if there

72
00:06:08,840 --> 00:06:10,440
is one.

73
00:06:10,440 --> 00:06:13,720
Then we need to know how to run inference on it.

74
00:06:13,720 --> 00:06:20,760
And we need to have the model architecture, which is also a piece of code, usually.

75
00:06:20,760 --> 00:06:29,760
Now at the top, data, and I have highlighted it, is the sufficient detailed information

76
00:06:29,760 --> 00:06:31,160
of how the system was trained.

77
00:06:31,160 --> 00:06:40,160
So this is not access, being able to download the full dataset as it was used for training.

78
00:06:40,160 --> 00:06:46,520
But it's enough information to understand what went, from the transparency perspective,

79
00:06:46,520 --> 00:06:52,120
what went into building that dataset, how it's been trained, how it's been collected,

80
00:06:52,120 --> 00:07:01,520
who collected it, the procedures for labeling, if there was any reinforcement training, human

81
00:07:01,520 --> 00:07:08,680
feedback, non-human feedback, rags, what have you, all of the methodologies that went into

82
00:07:08,680 --> 00:07:10,560
building that dataset.

83
00:07:10,560 --> 00:07:19,480
So this is something that many people have highlighted as still an area of contention.

84
00:07:19,480 --> 00:07:28,320
And I'm fully aware of the fact that data is the most controversial part of the open

85
00:07:28,320 --> 00:07:35,440
source of AI in general, and open source AI specifically, even more, because there are

86
00:07:35,440 --> 00:07:37,840
so many open questions.

87
00:07:37,840 --> 00:07:42,240
Now this is one of the boulders that I have highlighted at the beginning.

88
00:07:42,240 --> 00:07:47,040
We know that this is an issue, but we've been going around for over a year, and we can't

89
00:07:47,040 --> 00:07:48,040
figure it out.

90
00:07:48,040 --> 00:07:50,040
So let's move on.

91
00:07:50,040 --> 00:07:53,300
Let's finish this investigation phase.

92
00:07:53,300 --> 00:07:58,160
Let's get to a complete draft using these assumptions, that we don't need access to

93
00:07:58,160 --> 00:08:04,040
the full dataset, and see what happens.

94
00:08:04,040 --> 00:08:06,040
See what we end up with.

95
00:08:06,040 --> 00:08:12,240
Once we go back, we can revisit this decision, if it really looks like we have reached a

96
00:08:12,240 --> 00:08:22,320
wrong or counterintuitive or counterproductive, or maybe it's decent and good enough that

97
00:08:22,320 --> 00:08:25,120
we can work with it.

98
00:08:25,120 --> 00:08:30,000
Now the data transparency requirements, this is something that we will need to elaborate

99
00:08:30,000 --> 00:08:31,800
on in this new phase.

100
00:08:31,800 --> 00:08:44,520
But let's see, because the wording that I've used in this draft are mostly taken from the

101
00:08:44,520 --> 00:08:54,640
draft of the EU, European Union's AI Act, which, and these requirements have been already

102
00:08:54,640 --> 00:08:59,680
criticized for not being clear enough, or not being extensive enough.

103
00:08:59,680 --> 00:09:04,160
So I'm going to be on the data front.

104
00:09:04,160 --> 00:09:14,960
We are going to be working closely with Creative Commons and Open Future, rather, to elaborate

105
00:09:14,960 --> 00:09:22,200
a little bit more, to understand more the issue of data, and the issue of data governance,

106
00:09:22,200 --> 00:09:29,400
as this is a sophisticated and complicated topic.

107
00:09:29,400 --> 00:09:40,680
And all right, so phase two, which has started, is to look at the AI systems that we have

108
00:09:40,680 --> 00:09:52,720
investigated in the phase one, which are Bloom, Lama2, Pythia, OpenCV.

109
00:09:52,720 --> 00:10:04,040
And I'm open to add more, but the reason for this phase is to go through the list of required

110
00:10:04,040 --> 00:10:11,200
components, the ones that in the draft 06 are required to exercise the four freedoms,

111
00:10:11,200 --> 00:10:17,320
find them, find the preprocessing code for Lama2, preprocessing code for Pythia, training

112
00:10:17,320 --> 00:10:25,600
and validation code for OpenCV, inference code, what do we use for inference on Bloom,

113
00:10:25,600 --> 00:10:33,720
and find it, check the conditions under which they're made available, which I am not calling

114
00:10:33,720 --> 00:10:41,120
them licenses, because in some cases, these are not copyright, especially model parameters.

115
00:10:41,120 --> 00:10:43,840
It's not clear whether they're copyrighted or not.

116
00:10:43,840 --> 00:10:50,720
So licenses for everything that is code, licenses for everything that there is documentation,

117
00:10:50,720 --> 00:10:55,560
but we need to find all these resources and see how they are distributed, under which

118
00:10:55,560 --> 00:11:02,000
legal frameworks and legal documentation, legal contracts or terms of use, what have

119
00:11:02,000 --> 00:11:03,000
you.

120
00:11:03,000 --> 00:11:10,200
We need to find them, we need to document them, because once we've completed this for

121
00:11:10,200 --> 00:11:18,000
at least the four systems that I have highlighted, then we're going to go into looking at the

122
00:11:18,000 --> 00:11:25,480
legal documents and write analysis, legal analysis on these.

123
00:11:25,480 --> 00:11:34,840
So we're going to look at the Lama license, or the Lama terms of use, the Bloom rail frameworks

124
00:11:34,840 --> 00:11:41,920
for distribution, use, etc. of Bloom, etc.

125
00:11:41,920 --> 00:11:47,800
We're going to look at the Apache 2 license that is used by OpenCV, if I remember correctly,

126
00:11:47,800 --> 00:11:55,960
in some of the pieces of OpenCV, and see how the language of the Apache license matches

127
00:11:55,960 --> 00:12:08,200
the intention and how compatible it is with the digital artifact, the model weights, the

128
00:12:08,200 --> 00:12:12,160
model parameters, etc.

129
00:12:12,160 --> 00:12:14,420
That will give us the...

130
00:12:14,420 --> 00:12:20,200
Once we finish this, and I'm hoping, aiming to finish this towards the end of May, we

131
00:12:20,200 --> 00:12:26,960
should have the complete, we should reach a complete, feature complete list of elements

132
00:12:26,960 --> 00:12:31,960
for a knowledge, have enough knowledge to be able to say, "Okay, this is what we think

133
00:12:31,960 --> 00:12:33,640
open source AI looks like."

134
00:12:33,640 --> 00:12:39,120
It needs to come with these components, the components need to be made available with

135
00:12:39,120 --> 00:12:49,560
these, need to give out these freedoms, they need to allow these things.

136
00:12:49,560 --> 00:12:53,320
And most likely, it's going to be a checklist, very similar.

137
00:12:53,320 --> 00:12:55,640
Well, we'll see what...

138
00:12:55,640 --> 00:12:59,160
The principles are going to be very similar to what we have in the open source definition

139
00:12:59,160 --> 00:13:02,000
now, most likely.

140
00:13:02,000 --> 00:13:07,960
So those are the steps that we have in mind.

141
00:13:07,960 --> 00:13:16,960
And in terms of timeline, I think that we are still on time.

142
00:13:16,960 --> 00:13:23,360
I still think that we're going to be able to have a meeting in June.

143
00:13:23,360 --> 00:13:27,360
We're still discussing at this point, we're getting a little bit too close to June to

144
00:13:27,360 --> 00:13:33,760
have an in-person meeting, but I have started to think that we might have, in order to...

145
00:13:33,760 --> 00:13:43,720
We might have a virtual meeting, but hold on, we're still...

146
00:13:43,720 --> 00:13:46,700
There are a lot of opportunities, a lot of trips that we can take.

147
00:13:46,700 --> 00:13:53,480
We may have, instead of a one big meeting, we may have multiple ones at different times

148
00:13:53,480 --> 00:13:58,760
around the time of June, because there are so many events where we're really committed

149
00:13:58,760 --> 00:14:05,000
to go to, and we're going to be meeting many of the stakeholders at these events in Paris,

150
00:14:05,000 --> 00:14:08,520
at PyCon in Pittsburgh.

151
00:14:08,520 --> 00:14:13,920
And there's a meeting in Africa also at the beginning of June.

152
00:14:13,920 --> 00:14:23,720
So we might be able to distribute this stakeholder meeting and the issue of a release candidate

153
00:14:23,720 --> 00:14:26,640
around the month of June, rather than one big event.

154
00:14:26,640 --> 00:14:32,640
But in any case, October is still the release of version one, stable version, whatever we're

155
00:14:32,640 --> 00:14:35,120
going to call it.

156
00:14:35,120 --> 00:14:39,960
And we'll add all things open.

157
00:14:39,960 --> 00:14:45,240
So keep that in mind.

158
00:14:45,240 --> 00:14:51,400
And yeah, these are the dates that I mentioned we already have in mind, committed.

159
00:14:51,400 --> 00:14:56,000
So we're thinking about this roadshow, once we have a release candidate feature complete

160
00:14:56,000 --> 00:14:57,000
definition.

161
00:14:57,000 --> 00:15:10,560
The idea is to go through to these events and show the other meetings in this, but these

162
00:15:10,560 --> 00:15:15,360
are the main ones, the top ones with the organizers.

163
00:15:15,360 --> 00:15:22,480
We're partnering with the organizers so that we can have maximum exposure and we can use

164
00:15:22,480 --> 00:15:34,720
the feedback between June, late May to October.

165
00:15:34,720 --> 00:15:41,480
And of course, you're more than welcome to continue to come online, like we're being

166
00:15:41,480 --> 00:15:44,240
super transparent.

167
00:15:44,240 --> 00:15:50,520
We have these working that are set up to do the analysis, the early drafts of the analysis,

168
00:15:50,520 --> 00:16:02,240
and then publish them on the forums where we are hoping to get more comments and feed

169
00:16:02,240 --> 00:16:04,120
into the machine.

170
00:16:04,120 --> 00:16:11,680
So no one is really surprised at the end when the definition is announced.

171
00:16:11,680 --> 00:16:15,800
Easy to use as forums powered by open source discourse.

172
00:16:15,800 --> 00:16:19,560
So fun and also mobile friendly.

173
00:16:19,560 --> 00:16:20,560
All right.

174
00:16:20,560 --> 00:16:28,200
With that, I'm happy to get any questions if there is any.

175
00:16:28,200 --> 00:16:32,040
And if not, we can continue the conversation online.

176
00:16:32,040 --> 00:16:51,880
All right.

### End of last town hall held on 2024-03-22 ###

### Start of next town hall held on 2024-04-05 ###
--- Presentation for 2024-04-05 ---
OPEN SOURCE AI DEFINITION
Online public townhall
April 5, 2024
last updated: March 21, 2024 (SM)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

The objective for 2024

Open Source AI Deﬁnition
version 1.0

3

hackmd.io/@opensourceinitiative/osaid-0-0-5

Deﬁnition of AI system

Preamble
Out of scope issues
4 freedoms
Legal checklist

4

Deﬁnition of AI system

Preamble

Done … ish?

Out of scope issues
4 freedoms
Legal terms checklist

Revising draft

Open Source AI Deﬁnition v. 0.0.6
An Open Source AI is an AI system made available to the public under terms that grant the freedoms to:
●
●
●
●

Use the system for any purpose and without having to ask for permission.
Study how the system works and inspect its components.
Modify the system for any purpose, including to change its output.
Share the system for others to use with or without modiﬁcations, for any purpose.

Precondition to exercise these freedoms is to have access to the preferred form to make modiﬁcations to
the system. For machine learning systems that means having public access to:
●

●
transparency
requirements
only

●

Data: Sufficiently detailed information on how the system was trained, including the training
methodologies and techniques, the training data sets used, information about the provenance of
those data sets, their scope and characteristics; how the data was obtained and selected, the
labeling procedures and data cleaning methodologies.
Code: The code used for pre-processing data, the code used for training, validation and testing, the
supporting libraries like tokenizers and hyperparameters search code (if used), the inference code,
and the model architecture.
Model: The model parameters, including weights. Where applicable, these should include
checkpoints from key intermediate stages of training as well as the ﬁnal optimizer state.
6

System Review Workgroups

● Creating content for deﬁnition v. 0.0.7
● Release by next Friday, April 12th

7

Workgroups
Selected for diversity of approaches to AI openness:
1.

Pythia: open science project, with a permissive license

2.

BLOOM: open science project, with lots of details released but
shared with a restrictive license

3.

Llama 2: commercial project, accompanied by limited amount of
science and with a restrictive license

4.

OpenCV: open source project, with ML components outside of the
generative AI space

8

Members
Llama 2
1.
2.
3.
4.
5.
6.
7.
8.

Bastien Guerry
DINUM, French
public administration
Ezequiel Lanza
Intel
Roman Shaposhnik
Apache Software
Foundation
Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute
of Paris
Mo Zhou Debian,
Johns Hopkins
University
Victor Lu
independent
database consultant

Pythia
OpenCV
BLOOM
1.
Rahmat Akintola
1.
George C. G. Barbosa
1.
Seo-Young Isabelle
Cubeseed Africa
Fundação Oswaldo Cruz
Hwang Samsung
2.
Ignatius Ezeani
2.
Daniel Brumund GIZ
2.
Cailean
Osborne
Lancaster University
FAIR Forward - AI for all
University
of
Oxford,
3.
Kevin Harerimana
3.
Danish Contractor
CMU Africa
Linux Foundation
BLOOM Model Gov. WG
4.
Satya Mallick
4.
Abdoulaye Diack
3.
Stella Biderman
OpenCV
Google
EleutherAI
5.
David Manset
5.
Jaan Li University of
4.
Justin Colannino
ITU
Tartu, Phare Health
Microsoft
6.
Phil Nelson
6.
Jean-Pierre Lorre
5.
Hailey
Schoelkopf
OpenCV
LINAGORA,
7.
Tlamelo Makati
EleutherAI
OpenLLM-France
WiMLDS Gaborone,
7.
Ofentse Phuti WiMLDS
6.
Aviya Skowron
Technological
Gaborone
EleutherAI
University Dublin
8.
Caleb Fianku Quao
8.
Minyechil Alehegn
To achieve better global
Kwame Nkrumah
Tefera Mizan Tepi
University of Science and representation, we conducted
University
Technology, Kumasi
outreach to Black, Indigenous,
9.
Akosua Twumasi
and other People of Color,
Ghana Health
particularly women and
Service
9
individuals from the Global South.

Phase 1: Deciding Required Components
Component Voting

January 15

Vote
Compilation
Recommendation
Report
example: Llama 2

Process: From mid-January through February,
system workgroups voted on which components
should be required for a system to be deﬁned as
open. These votes were then publicly tabulated and a recommendations
report was publicly shared on the forum. The recommendations became
version 0.0.6 of the deﬁnition.

March 10

Deﬁnition
v. 0.0.6

10

Phase 2: Finetuning the Component Checklist
Deﬁnition v. 0.0.6
Checklist for Doc Review

Deﬁnition to Checklist: A
week after the v.0.0.6 release
in mid-March, we went back
the the workgroup members
to ask them to help us
ﬁnetune the requirements
checklist implied by version
0.0.6 (left).
This v 0.0.6 checklist includes
the components categorized
as required or likely required
by voting in Phase 1, plus a list
of data transparency
requirements already in force
under the EU’s AI Act.

Requirements: Open Source AI Deﬁnition 0.0.6

11

Document Review Spreadsheet

example; BLOOM review spreadsheet
To be added in version 0.0.7

12

Document Reviewers
Llama 2

BLOOM

Pythia

OpenCV

Affiliated

Affiliated

Affiliated

Affiliated

1.
2.

Davide Testuggine
Meta
Jonathan Torres
Meta

Unaffiliated
3.

4.

Stefano Zacchiroli
Polytechnic Institute
of Paris
Victor Lu independent
database consultant

1.

Danish Contractor
BLOOM Model
Governance
Workgroup

Unaffiliated
2.

Jaan Li University of
Tartu, Phare Health

1.
2.
3.

Stella Biderman
EleutherAI
Aviya Skowron
EleutherAI
Hailey Schoelkopf
EleutherAI

1.

none

Unaffiliated

2.

none

Unaffiliated
4.

Seo-Young
Isabelle Hwang
Samsung
Volunteers
needed!
Email or DM
Mer

13

Representation: Relation to Open Source AI
Stakeholder

Description

Example

1. System
Creator

Makes AI system and/or component that will be
studied, used, modiﬁed, or shared through an
open source license

ML researcher in academia or industry

2. License
Creator

Writes or edits the open source license to be
applied to the AI system or component,
includes compliance

IP lawyer

3. Regulator

Writes or edits rules governing licenses and
systems

government policy-maker

4. Licensee

Seeks to study, use modify, or share an open
source AI system

AI engineer in industry, health researcher in
academia

5. End User

Consumes a system output, but does not seek
to study, use, modify, or share the system

student using a chatbot to write a report, artist
creating an image

6. Subject

Affected upstream or downstream by a system
output without interacting with it intentionally +
advocates for this group.

photographer who ﬁnds their image in training
dataset (upstream), mortgage applicant
evaluated by a bank’s AI system (downstream)

14

Representation: Global Inclusion and Equity

15

Next Steps

16

2024 timeline

System testing work stream
Stakeholder consultation work stream
OSAID v. 0.0.7
by next Friday,
April 12th

Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Townhalls +

Townhall +

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

PyCon
Workshop
May 17th,
Pittsburgh)

(≈

Draft 0.0.8

… October
Monthly Virtual
Meetings

Release
version 1.0

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

v. 1.0

Deep Dive AI in-person meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

PyCon US

May 17*

Europe

?

Africa

Nigeria

Abuja

OSCA

June 6 - 8

Latin America

Mexico

Mexico D.F.

Latam OSS

July 19 - 20

Asia Paciﬁc

Hong Kong

Hong Kong

AI_dev

August 23

North America

United States

Raleigh

All Things Open

Oct 27 - 29

*conﬁrmed

May ?

Join the conversation
● discuss.opensource.org
● Public forum
● Join as OSI member
○ Free or full
○ SSO with other OSI
websites

19

Q&A

20

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the
deﬁnitional process.

21

Criteria for RC1 and v. 1.0
RC1

version 1

- Expected outcome of
in-person meeting end
May/early June!
- The draft is completed
in all its parts
- The draft is supported
by at least 2
representatives for
each of the 6
stakeholder groups

- Expected outcome of
in-person and online
meetings through the
summer/early autumn
- The draft is endorsed by at
least 5 reps for each of the
stakeholder groups
- Announced in late October

22

Help us ﬁnd stakeholders
System Creator

License Creator

Regulator

Licensee

End User

Subject

Makes AI system
and/or
component that
will be studied,
used, modified,
or shared
through an open
source license
(e.g., ML
researcher in
academia or
industry)

Writes or edits
the open source
license to be
applied to the AI
system or
component;
includes
compliance
(e.g., IP lawyer)

Writes or edits
rules governing
licenses and
systems (e.g.
government
policy-maker)

Seeks to study,
use modify, or
share an open
source AI
system (e.g. AI
engineer, health
researcher,
education
researcher)

Consumes a
system output,
but does not
seek to study,
use, modify, or
share the
system (e.g.,
student using a
chatbot to write
a report, artist
creating an
image)

Affected
upstream or
downstream by
a system output
without
interacting with it
intentionally;
includes
advocates for
this group (e.g.
people with loan
denied, or
content creators)

✅

✅

⚠

✅

⚠

⚠

Enough to start

Enough to start

Leads to US, EU,
Singapore, no
commitment yet

Enough to start

Which org is squarely in
this space?

ACLU, Algorithmic
Justice League

23

Finetuning with Document Review
AI systems

Active
workgroups:
- Llama2
- Pythia
- BLOOM
Recruiting
reviewers
- OpenCV

List of
components

Legal
frameworks

Legal
documents

What elements are
necessary to:
- use
- study
- modify
- share
an AI system?

For each
component,
evaluate which
laws apply. Some
will be under
“Intellectual
Property” regimes,
some will be
under other
regimes.

Next, match the
components and
legal frameworks with the
terms of the
legal documents,
if they exist.

These are listed in
deﬁnition v. 0.0.6.

Finetuned

Checklist
After repeating
this exercise
through multiple
systems, we’ll be
able to generalize
the outcomes and
write the specs to
evaluate the
freedoms granted.
These will appear
in deﬁnition 0.0.7.


--- Subtitles for 2024-04-05 --- ###
1
00:00:00,001 --> 00:00:02,000
I almost forgot.

2
00:00:02,000 --> 00:00:04,000
All right.

3
00:00:04,000 --> 00:00:06,000
Welcome, everyone.

4
00:00:06,000 --> 00:00:19,000
This is our public town hall where we explain the status, where we are, and highlight the -- what happened in the past two weeks, and we talk about what's coming in the next near future.

5
00:00:19,000 --> 00:00:24,000
So today Mer is going to lead the presentation.

6
00:00:24,000 --> 00:00:26,000
So Mer, it's up to you.

7
00:00:26,000 --> 00:00:28,000
>> Okay.

8
00:00:28,000 --> 00:00:29,000
Thank you.

9
00:00:29,000 --> 00:00:31,000
So I think you all know me.

10
00:00:31,000 --> 00:00:37,000
I'm the process facilitator for co-designing the open source AI definition.

11
00:00:37,000 --> 00:00:46,000
And I think you all have seen this, just that we have some community agreements that we use at all meetings.

12
00:00:46,000 --> 00:00:51,000
One mic, one speaker is about not interrupting.

13
00:00:51,000 --> 00:00:57,000
Take space, make space just says if you tend to be more quiet, we invite you to speak up.

14
00:00:57,000 --> 00:01:04,000
And if you tend to speak up more easily, to give space for others to speak as well.

15
00:01:04,000 --> 00:01:12,000
Kindness just reminding us to be gentle with each other because this is quite hard, but we don't have to be.

16
00:01:12,000 --> 00:01:18,000
Forward motion is acknowledging that there's so many challenges and obstacles to this work.

17
00:01:18,000 --> 00:01:22,000
And just that we can choose for those to stop the process.

18
00:01:22,000 --> 00:01:31,000
But rather instead that if we hit a boulder, we note it and we walk around it and come back in the future as needed.

19
00:01:31,000 --> 00:01:33,000
And solution seeking is similar.

20
00:01:33,000 --> 00:01:37,000
It's just that it's easier to say, oh, this doesn't work, that won't work.

21
00:01:37,000 --> 00:01:41,000
And it's more challenging and vulnerable to say, hey, how about this?

22
00:01:41,000 --> 00:01:42,000
Could we do it this way?

23
00:01:42,000 --> 00:01:52,000
But that we need that kind of, I guess, intellectual energy to make this definition a good one.

24
00:01:52,000 --> 00:01:57,000
So, yeah, so this is a slide probably anyone who's been to one of these has seen before.

25
00:01:57,000 --> 00:02:02,000
We're creating the version 1.0 of the open source AI definition this year.

26
00:02:02,000 --> 00:02:05,000
And this is where we are now.

27
00:02:05,000 --> 00:02:07,000
And these are the parts of the definition.

28
00:02:07,000 --> 00:02:18,000
Defining an AI system, a preamble about the need for the definition, issues that are out of scope, definitions of the four freedoms, studies, modify and share.

29
00:02:18,000 --> 00:02:29,000
And that was the first part of our co-design process at the end of last year was co-designing the specific words that we're using to define those concepts.

30
00:02:29,000 --> 00:02:33,000
And that will be shared later in the presentation in case you need a refresher.

31
00:02:33,000 --> 00:02:38,000
And what we're doing now is we're working on a draft of the checklist.

32
00:02:38,000 --> 00:02:47,000
So what are the required components for an AI system to be considered open by according to OSI?

33
00:02:47,000 --> 00:02:53,000
And that summarizes what I've just told you.

34
00:02:53,000 --> 00:02:55,000
We're pretty much finished with the rest of it.

35
00:02:55,000 --> 00:03:01,000
And we're working on a draft of the checklist.

36
00:03:01,000 --> 00:03:05,000
I guess also if you have questions, you can share them in the chat.

37
00:03:05,000 --> 00:03:14,000
And I may or may not see them, but Stefano and Nick are there to do so.

38
00:03:14,000 --> 00:03:20,000
So, yeah, so this is version 0.0.6, which is the current version of the definition.

39
00:03:20,000 --> 00:03:31,000
And you can see at the top the terms, use, study, modify, share, and how we're defining those.

40
00:03:31,000 --> 00:03:35,000
That's not new for this version, but it's important content.

41
00:03:35,000 --> 00:03:43,000
And the thing that was new with this version is that there was a lot of question, as you can all imagine,

42
00:03:43,000 --> 00:03:48,000
about requiring data and what has come out.

43
00:03:48,000 --> 00:03:56,000
And this version, and I'll talk about our process, is that we have transparency requirements only.

44
00:03:56,000 --> 00:04:04,000
So data sets themselves are not required, but we have transparency requirements in the form of documentation requirements.

45
00:04:04,000 --> 00:04:08,000
And I'll go into that.

46
00:04:08,000 --> 00:04:14,000
So this is going into our process, how we've been co-designing the definition.

47
00:04:14,000 --> 00:04:18,000
We've been doing it through these system review workgroups primarily.

48
00:04:18,000 --> 00:04:27,000
And right now the workgroups are still active, and they're creating content for version 7 of 0.0.7,

49
00:04:27,000 --> 00:04:33,000
which will be released next Friday.

50
00:04:33,000 --> 00:04:38,000
And then this is a reminder of what the workgroups, the focus of them.

51
00:04:38,000 --> 00:04:45,000
They're focusing on different AI systems that have varying approaches to openness, to the concept of openness.

52
00:04:45,000 --> 00:04:54,000
So we have Pythia, Bloom, Lama2, and OpenCV.

53
00:04:54,000 --> 00:04:56,000
And these are the members of those groups.

54
00:04:56,000 --> 00:05:05,000
Part of being a member of the group is that you agree to have your name and affiliation shared publicly for the sake of the transparency of the process.

55
00:05:05,000 --> 00:05:16,000
And just to also note that these groups reflected also some outreach that we did to have better global representation,

56
00:05:16,000 --> 00:05:23,000
particularly of Black, Indigenous, and other people of color, women, and individuals from the global south,

57
00:05:23,000 --> 00:05:29,000
because this is going to be a global standard.

58
00:05:29,000 --> 00:05:35,000
So this is what we did in phase one at the beginning of the year.

59
00:05:35,000 --> 00:05:40,000
In each working group, we had component voting.

60
00:05:40,000 --> 00:05:51,000
So we took a list of components from the model openness framework, which is based on a paper by the Linux Foundation and others,

61
00:05:51,000 --> 00:05:56,000
and we thought it was a great list of components, generalized components across multiple systems.

62
00:05:56,000 --> 00:05:59,000
So we used that as our components list.

63
00:05:59,000 --> 00:06:13,000
And we had members of the work groups vote with their initials as to whether they thought that each component was necessary for the system to be studied, used, modified, and shared.

64
00:06:13,000 --> 00:06:27,000
So building on that foundation of the principles, which we'd already established, we used that to develop work group recommendations on whether each component should be required.

65
00:06:27,000 --> 00:06:43,000
Then, in the example that was from the Lama II group, I then compiled the votes and developed a rubric that made a recommendation on the components based on the number of votes.

66
00:06:43,000 --> 00:06:51,000
So we obviously could have had a system where any component that got even one vote, that would mean it was required.

67
00:06:51,000 --> 00:06:53,000
We didn't do it that way.

68
00:06:53,000 --> 00:07:00,000
We said, let's have a set of minimum requirements for openness.

69
00:07:00,000 --> 00:07:08,000
And then the results of that, which ended up being a Likert scale, basically.

70
00:07:08,000 --> 00:07:20,000
So there were components that had the most votes and would definitely be required, that might be required, then I think possibly required, unlikely to be required, not required.

71
00:07:20,000 --> 00:07:34,000
And then we posted that list in the forum, that recommendations report, and then that list became version 0.0.6.

72
00:07:34,000 --> 00:07:41,000
Yeah, and now this is what we're doing now. We're doing phase two, fine tuning the component list.

73
00:07:41,000 --> 00:07:56,000
So we have this checklist in 0.0.6, which then I put into a slightly different format, basically adding documentation.

74
00:07:56,000 --> 00:08:02,000
So documentation was listed in 0.1.6 textually.

75
00:08:02,000 --> 00:08:13,000
But it wasn't in the checklist table. So I put the documentation requirements into a checklist format.

76
00:08:13,000 --> 00:08:17,000
And right now the work groups are...

77
00:08:17,000 --> 00:08:20,000
I hope you can't hear the pinging in the background.

78
00:08:20,000 --> 00:08:23,000
My computer. Hopefully that's just me.

79
00:08:23,000 --> 00:08:31,000
So now all the work groups, they have another spreadsheet they're filling out, lucky them. This is the example from Bloom.

80
00:08:31,000 --> 00:08:38,000
But they're identifying what is the documentation for each of these required components.

81
00:08:38,000 --> 00:08:46,000
And then is it those drop downs, those gray drop downs, I think it's allowed or

82
00:08:46,000 --> 00:08:49,000
allowed, not allowed effectively.

83
00:08:49,000 --> 00:08:56,000
You know, is use allowed, not allowed, studied, not allowed, not allowed, modification, sharing for each component.

84
00:08:56,000 --> 00:09:02,000
And then for 0.0.7, we're also seeking to fill in these blanks, which are in red.

85
00:09:02,000 --> 00:09:11,000
So what would be the legal framework for model parameters, including weights, and what would be the legal framework for all these forms of documentation.

86
00:09:11,000 --> 00:09:18,000
Stefano, do you want to jump in at any, add anything?

87
00:09:18,000 --> 00:09:20,000
No, you're covering all of it.

88
00:09:20,000 --> 00:09:23,000
Okay, sounds good. I'll continue then.

89
00:09:23,000 --> 00:09:27,000
So, yeah, so these are the document reviewers.

90
00:09:27,000 --> 00:09:35,000
Again, we said if you want to be a document reviewer, you need to share your name and affiliation.

91
00:09:35,000 --> 00:09:43,000
And here we wanted to be sure that there was at least one unaffiliated reviewer.

92
00:09:43,000 --> 00:09:49,000
People that are affiliated are creators or advisors of the systems under review.

93
00:09:49,000 --> 00:09:53,000
And they have the most technical knowledge.

94
00:09:53,000 --> 00:10:02,000
And yet also have obviously their own set of preferences around how a system might be reviewed.

95
00:10:02,000 --> 00:10:05,000
So we also have unaffiliated reviewers.

96
00:10:05,000 --> 00:10:09,000
And we are looking good for Lama 2 Bloom and Pythia.

97
00:10:09,000 --> 00:10:17,000
We don't have anyone for OpenCV. So it's possible that OpenCV simply won't be reviewed in this phase, which would be sad.

98
00:10:17,000 --> 00:10:19,000
But we would love for someone to do it.

99
00:10:19,000 --> 00:10:24,000
We've asked on the forum, don't have any takers yet.

100
00:10:24,000 --> 00:10:26,000
But so this is a formal call.

101
00:10:26,000 --> 00:10:36,000
If you would like to volunteer, chat or message me on the forum, or we would love for this review to happen.

102
00:10:36,000 --> 00:10:40,000
But so far we don't have anyone to do it.

103
00:10:40,000 --> 00:10:44,000
And yes, talking about representation.

104
00:10:44,000 --> 00:10:48,000
So we have two ways of looking at representation.

105
00:10:48,000 --> 00:10:56,000
The first way is the way that I was talking about of around identity, not specifically related to open AI.

106
00:10:56,000 --> 00:11:00,000
But then the second way we have of looking at representation is relation to open source AI.

107
00:11:00,000 --> 00:11:09,000
And so we have six stakeholder groups, system creator, license creator, regulator, licensee, end user and subject.

108
00:11:09,000 --> 00:11:12,000
And you can see those descriptions and examples.

109
00:11:12,000 --> 00:11:27,000
And right now, the people most involved in this phase are system and license creators and licensees who tend to have that ability to analyze license documents.

110
00:11:27,000 --> 00:11:30,000
Other individuals are welcome.

111
00:11:30,000 --> 00:11:34,000
And of course, everyone falls into six subjects.

112
00:11:34,000 --> 00:11:41,000
So if you're thinking, oh, I don't fit into any other group, trust me, you fall into six subjects of AI.

113
00:11:41,000 --> 00:11:44,000
So, yes, we welcome everyone to join.

114
00:11:44,000 --> 00:11:55,000
And there will be as we get out of the more technical aspects of the review, it will be easier for people without that technical knowledge to participate.

115
00:11:55,000 --> 00:11:59,000
Although they are invited at any point to be involved.

116
00:11:59,000 --> 00:12:08,000
And then this is just, again, the way that we're ensuring global inclusion and equity with phrases like this.

117
00:12:08,000 --> 00:12:19,000
You know, black, indigenous, Latina and other people of color, women, queer, transgender, non-binary people, people with disabilities, poor and working class backgrounds are encouraged to respond.

118
00:12:19,000 --> 00:12:31,000
So just whenever we're sharing about our project, just saying we want you to be involved, even if you're not seeing people like yourself involved yet.

119
00:12:31,000 --> 00:12:34,000
And I think that's the end of the slides.

120
00:12:34,000 --> 00:12:35,000
Next steps.

121
00:12:35,000 --> 00:12:36,000
Ah, yes.

122
00:12:36,000 --> 00:12:38,000
So here we are in April.

123
00:12:38,000 --> 00:12:44,000
We are going to be releasing Open 0.7 by next Friday.

124
00:12:44,000 --> 00:12:52,000
Also next month there is a live workshop at PyCon in Pittsburgh, probably will be the 17th.

125
00:12:52,000 --> 00:13:02,000
And that will be the review of -- I guess we'll be turning that into 0.0.8.

126
00:13:02,000 --> 00:13:11,000
But it's basically going to be a live opportunity for people to be involved in the next version of the -- yeah.

127
00:13:11,000 --> 00:13:15,000
The -- creating the definition.

128
00:13:15,000 --> 00:13:18,000
And then we also have these meetings that we have planned.

129
00:13:18,000 --> 00:13:25,000
And this purple line is the one that's been confirmed that we will be there.

130
00:13:25,000 --> 00:13:31,000
But we are planning and hoping to be at these other events around the world this summer.

131
00:13:31,000 --> 00:13:34,000
Do you want to say anything about that, Stefano?

132
00:13:34,000 --> 00:13:35,000
>> Yeah.

133
00:13:35,000 --> 00:13:43,000
If you go back to the previous slide, there is one thing that we want to highlight also.

134
00:13:43,000 --> 00:13:48,000
That we're still targeting reaching release candidate in June.

135
00:13:48,000 --> 00:13:59,000
Which means that the workshop at PyCon is going to be very crucial to -- because it's going to be probably the last draft before we go to release candidate.

136
00:13:59,000 --> 00:14:01,000
It's going to be very close.

137
00:14:01,000 --> 00:14:10,000
Or we're hoping to be -- to have it in shape to be close to a future complete, actually.

138
00:14:10,000 --> 00:14:13,000
It should be future complete by May.

139
00:14:13,000 --> 00:14:18,000
And cleaned up at a meeting in June.

140
00:14:18,000 --> 00:14:22,000
We've been hoping to hold one in person.

141
00:14:22,000 --> 00:14:33,000
But we're thinking at the moment, because of opportunities and because of time crunch, it might be an online meeting of a couple of hours.

142
00:14:33,000 --> 00:14:40,000
So we're going to be reaching out to the crucial stakeholders that we want invited.

143
00:14:40,000 --> 00:14:49,000
And you're welcome to also candidate yourself if you think you may want to join this in June.

144
00:14:49,000 --> 00:14:52,000
Just reach out to both me and Mer.

145
00:14:52,000 --> 00:14:59,000
We'll be looping you in.

146
00:14:59,000 --> 00:15:00,000
>> Okay.

147
00:15:00,000 --> 00:15:03,000
Sounds good.

148
00:15:03,000 --> 00:15:07,000
So this is just a reminder that we do have this forum.

149
00:15:07,000 --> 00:15:12,000
I think everyone here knows that because they probably joined this meeting based on the forum post.

150
00:15:12,000 --> 00:15:17,000
But this is our main mechanism of public transparency is the forum.

151
00:15:17,000 --> 00:15:20,000
Obviously not everyone can attend these town halls.

152
00:15:20,000 --> 00:15:23,000
So please do join.

153
00:15:23,000 --> 00:15:28,000
And that's at discuss.opensource.org.

154
00:15:28,000 --> 00:15:31,000
And now we have Q&A.

155
00:15:31,000 --> 00:15:34,000
So thank you.

156
00:15:34,000 --> 00:15:42,000
>> Yeah, thanks, Mer.

157
00:15:42,000 --> 00:15:49,000
Does anyone have any curiosity or question or ideas?

158
00:15:49,000 --> 00:15:52,000
Have you reviewed the draft?

159
00:15:52,000 --> 00:16:06,000
You left the comments on the draft already.

160
00:16:06,000 --> 00:16:12,000
>> Yes.

161
00:16:12,000 --> 00:16:29,000
Can you share the link to the draft just because I'm sharing my screen?

162
00:16:29,000 --> 00:16:30,000
>> Of course.

163
00:16:31,000 --> 00:16:35,000
You can comment directly on the HackMD website.

164
00:16:35,000 --> 00:16:44,000
Or there are dedicated -- there is a dedicated topic on the forum that I just linked.

165
00:16:44,000 --> 00:16:46,000
It's pinned at the top.

166
00:16:46,000 --> 00:17:00,000
You can comment generally on the proposal, see what others have been saying already in general on the draft.

167
00:17:00,000 --> 00:17:01,000
>> Yeah.

168
00:17:01,000 --> 00:17:04,000
And feel free to chat or raise your hands.

169
00:17:04,000 --> 00:17:14,000
Whatever you prefer.

170
00:17:14,000 --> 00:17:28,000
>> There is -- so on the call for volunteers to review OpenCV, I also want to say that this is a pretty simple task.

171
00:17:28,000 --> 00:17:32,000
It doesn't require specific knowledge of AI in general.

172
00:17:32,000 --> 00:17:45,000
Of all the systems, I think OpenCV should have pretty much everything available under free and open source license.

173
00:17:45,000 --> 00:17:49,000
It should be fairly easy to go through the list of components.

174
00:17:49,000 --> 00:17:52,000
There are only eight or ten.

175
00:17:52,000 --> 00:17:56,000
And try to find them basically in the OpenCV repository.

176
00:17:56,000 --> 00:17:58,000
So it should be fairly easy.

177
00:17:58,000 --> 00:18:07,000
And in time, it's a great way to get familiar with the process and what we're doing.

178
00:18:07,000 --> 00:18:09,000
And be listed as a contributor, of course.

179
00:18:09,000 --> 00:18:16,000
Because we'll be giving recognition.

180
00:18:16,000 --> 00:18:20,000
>> So I may be able to jump in on this one.

181
00:18:20,000 --> 00:18:24,000
I will talk to Mary next week and see if this is an option.

182
00:18:24,000 --> 00:18:29,000
>> I will give you the time and the knowledge to do that.

183
00:18:29,000 --> 00:18:30,000
>> That would be great.

184
00:18:30,000 --> 00:18:32,000
Yeah, thanks, Ofer.

185
00:18:32,000 --> 00:18:35,000
And Ricardo is just commenting, this is great.

186
00:18:35,000 --> 00:18:38,000
And saying that he learned about the model openness framework recently.

187
00:18:38,000 --> 00:18:41,000
Yeah, it was only just published on ArchiveX.

188
00:18:41,000 --> 00:18:43,000
The model openness framework.

189
00:18:43,000 --> 00:18:44,000
>> It's pretty new.

190
00:18:44,000 --> 00:18:46,000
>> We only work on this for a few months.

191
00:18:46,000 --> 00:18:54,000
A couple of months, maybe.

192
00:18:54,000 --> 00:18:55,000
>> All right.

193
00:18:55,000 --> 00:18:59,000
If there are no more questions, then I think we can close it here.

194
00:18:59,000 --> 00:19:02,000
Thanks, everyone, for joining and listening in.

195
00:19:02,000 --> 00:19:07,000
We'll be here back again in two weeks at a different time.

196
00:19:07,000 --> 00:19:13,000
More compatible with the eastern side of the world.

197
00:19:13,000 --> 00:19:17,000
But, you know, keep track of the process and keep leaving your comments.

198
00:19:17,000 --> 00:19:20,000
We're going to see you soon.

199
00:19:20,000 --> 00:19:22,000
>> Yeah, thanks, everyone.

### End of last town hall held on 2024-04-05 ###

### Start of next town hall held on 2024-04-19 ###
--- Presentation for 2024-04-19 ---
OPEN SOURCE AI DEFINITION
Online public townhall
April 19, 2024
last updated: April 18, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

OSI’s objective for 2024

Open Source AI Definition

3
3

Open Source AI Definition

Where Are We Now?

4

Open
Source AI
Definition
Elements

Preamble
Out of Scope Issues
4 Freedoms

v.0.0.7

Legal Checklist

-

Scan me!

Open
Source AI
Definition
Elements

Preamble
Out of Scope Issues
4 Freedoms

Done … ish?

v.0.0.7

Legal Checklist

Revising
draft

Open Source AI Definition

The Co-Design Process
Fall 2023: The 4 Freedoms
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

7

The 4 Freedoms for AI
Use • Study • Modify • Share

What should these
open source principles mean
for artificial intelligence?

8

Co-Design Workshop: Raleigh
All Things Open | October 2023

9

Co-Design Workshop: Monterey
Linux Foundation Member Summit | October 2023

10

Co-Design Workshop: Addis Ababa
Digital Public Goods Alliance Members Meeting | November 2023

11

Open Source AI Definition
The 4 Freedoms for AI
1. Use the system for any purpose and without having
to ask for permission.
2. Study how the system works and inspect its
components.
3. Modify the system for any purpose, including to
change its output.
4. Share the system for others to use with or without
modifications, for any purpose.
12

Open Source AI Definition

The Co-Design Process
Winter 2023-24: Required Components
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

13

Required Components for Open Source AI

What components must be
open in order for an AI system
to be used, studied, modified,
and shared?

14

Co-Design Workshop: San Jose
AI.dev | December 2023

15

Virtual Workgroups
Selected to represent a diversity of approaches to AI openness:
1.

Llama 2: commercial project, accompanied by limited amount of
science and with a restrictive license

2.

BLOOM: open science project, with lots of details released but
shared with a restrictive license

3.

Pythia: open science project, with a permissive license

4.

OpenCV: open source project, with ML components outside of the
generative AI space

16

better global representation, we conducted outreach to Black, Indigenous,
Workgroup Members Toandachieve
other people of color, particularly women and individuals from the Global South.

Llama 2
1.
2.
3.
4.
5.
6.
7.
8.

Bastien Guerry
DINUM, French public
administration
Ezequiel Lanza Intel
Roman Shaposhnik
Apache Software
Foundation
Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute of
Paris
Mo Zhou Debian,
Johns Hopkins
University
Victor Lu independent
database consultant

BLOOM
1.
2.
3.
4.
5.
6.
7.
8.

George C. G. Barbosa
Fundação Oswaldo Cruz
Daniel Brumund GIZ
FAIR Forward - AI for all
Danish Contractor
BLOOM Model Gov. WG
Abdoulaye Diack
Google
Jaan Li University of
Tartu, Phare Health
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
Ofentse Phuti WiMLDS
Gaborone
Caleb Fianku Quao
Kwame Nkrumah
University of Science and
Technology, Kumasi

Pythia
1.
2.

3.
4.

Seo-Young Isabelle
Hwang Samsung
Cailean Osborne
University of Oxford,
Linux Foundation
Stella Biderman
EleutherAI
Justin Colannino
Microsoft

5.

Hailey Schoelkopf
EleutherAI

6.

Aviya Skowron
EleutherAI

Over 50% of all
workgroup
participants are
people of color.

OpenCV
1.
2.
3.
4.
5.
6.
7.

8.
9.
10.

Rahmat Akintola
Cubeseed Africa
Ignatius Ezeani
Lancaster University
Kevin Harerimana CMU
Africa
Satya Mallick OpenCV
David Manset ITU
Phil Nelson
OpenCV
Tlamelo Makati
WiMLDS Gaborone,
Technological University
Dublin
Minyechil Alehegn
Tefera Mizan Tepi
University
Akosua Twumasi
Ghana Health Service
Rasim Sen Oasis
Software Technology Ltd.

Workgroups: Required Component Selection
Component List

18

Available on arXiv
CC BY-NC-SA 4.0

Workgroups: Required Component Selection
Component
List
Component Votes

19

Example:
Llama 2
Workgroup

Workgroups: Required Component Selection
Component
List

Component
Votes
Vote Compilation

20

Workgroups: Required Component Selection
Component
List

Component
Votes
Vote
Compilation
Recommendation Report

21

v.0.0.6

Workgroups: Required Component Selection
Component
List

Component
Votes
Vote
Compilation

Recommendation
Report
Definition
Checklist

22

v.0.0.6

Open Source AI Definition v.0.0.7
Required Components

23

Open Source AI Definition v.0.0.7
Required Components: Legal Frameworks

24

Open Source AI Definition v.0.0.7
Required Components: Legal Frameworks

25

Open Source AI Definition

The Co-Design Process
Representation, Inclusion, and Equity
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

26

Equitable and inclusive stakeholder
representation isn’t only about for
justice, it’s about legitimacy.

A global definition requires
global consultation.
27

Stakeholder

Description

Example

1. System
Creator

Makes AI system and/or component that will be
studied, used, modified, or shared through an open
source license

ML researcher in academia or industry

2. License
Creator

Writes or edits the open source license to be applied
to the AI system or component, includes compliance

IP lawyer

3. Regulator

Writes or edits rules governing licenses and systems

government policy-maker

4. Licensee

Seeks to study, use modify, or share an open source
AI system

AI engineer in industry, health researcher in
academia

5. End User

Consumes a system output, but does not seek to
study, use, modify, or share the system

student using a chatbot to write a report, artist
creating an image

6. Subject

Affected upstream or downstream by a system output
without interacting with it intentionally + advocates for
this group.

photographer who finds their image in training
28
dataset (upstream), mortgage applicant
evaluated
by a bank’s AI system (downstream)

28

Stakeholder

Most
involved
in current
phase

Description

Example

1. System
Creator

Makes AI system and/or component that will be
studied, used, modified, or shared through an open
source license

ML researcher in academia or industry

2. License
Creator

Writes or edits the open source license to be applied
to the AI system or component, includes compliance

IP lawyer

3. Regulator

Writes or edits rules governing licenses and systems

government policy-maker

4. Licensee

Seeks to study, use modify, or share an open source
AI system

AI engineer in industry, health researcher in
academia

5. End User

Consumes a system output, but does not seek to
study, use, modify, or share the system

student using a chatbot to write a report, artist
creating an image

6. Subject

Affected upstream or downstream by a system output
without interacting with it intentionally + advocates for
this group.

photographer who finds their image in training
29
dataset (upstream), mortgage applicant
evaluated
by a bank’s AI system (downstream)

29

30

Open Source AI Definition

The Co-Design Process
Next Steps
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

31

System testing work stream
Stakeholder consultation work stream

2024 Timeline
OSAID v. 0.0.7
last Friday, April
12th

Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Townhalls +

Townhall +

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7

PyCon
Workshop
May 17th,
Pittsburgh)

(≈

Draft 0.0.8

… October
Monthly Virtual
Meetings

Release stable
version

OSI In-Person
Stakeholder
Meeting (date
+ place TBD)

RC1

Stable
Version

In-Person Meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

PyCon US

May 17

Europe

France

Paris

OW2

June

Africa

Nigeria

Lagos

Sustain Africa

June

North America

United States

New York

OSPOs for Good

July 9 - 11

Asia Paciﬁc

China

Hong Kong

AI_dev

August 23

Latin America

Argentina

Buenos Aires

Nerdearla

September

Europe

France

Paris

(data governance)

September

North America

United States

Raleigh

All Things Open

Oct 27 - 29

33

Open Source AI Definition
Stay Connected
●
●

●

Public forum: discuss.opensource.org
Become an OSI Member
○ Free or or full
○ SSO with other OSI websites
Biweekly virtual town halls
34

Q&A

35

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the
deﬁnitional process.

36


--- Subtitles for 2024-04-19 --- ###
1
00:00:00,001 --> 00:00:00,840
Good.

2
00:00:00,840 --> 00:00:05,160
Let's get started.

3
00:00:05,160 --> 00:00:09,440
Thanks everyone for joining this public town hall.

4
00:00:09,440 --> 00:00:13,800
I don't even remember the fifth in the series,

5
00:00:13,800 --> 00:00:16,880
but there's been a while we've tried to be transparent

6
00:00:16,880 --> 00:00:18,960
about the process and what's happening

7
00:00:18,960 --> 00:00:22,000
and give a very quick overview

8
00:00:22,000 --> 00:00:25,000
of what's been happening in the past few months

9
00:00:25,000 --> 00:00:29,960
and look at the latest draft

10
00:00:29,960 --> 00:00:34,960
and have a quick glance at what's coming next

11
00:00:34,960 --> 00:00:38,160
because we're really approaching the last months,

12
00:00:38,160 --> 00:00:42,320
actually the last weeks of the drafting process.

13
00:00:42,320 --> 00:00:46,280
We really are getting close to something that is workable.

14
00:00:46,280 --> 00:00:48,600
Let's start with our community agreements

15
00:00:48,600 --> 00:00:52,880
to remind everyone that we are welcoming a place,

16
00:00:52,880 --> 00:00:56,360
we're also trying to balance with moving forward

17
00:00:56,360 --> 00:01:00,040
and finding converging towards a solution,

18
00:01:00,040 --> 00:01:04,040
leaving the largest building blocking paths,

19
00:01:04,040 --> 00:01:10,520
elements for something that can be fixed in the future.

20
00:01:10,520 --> 00:01:15,720
And we are aiming quickly towards having a definition

21
00:01:15,720 --> 00:01:17,920
for open source AI by the end of the year.

22
00:01:17,920 --> 00:01:22,320
So where do we stand now?

23
00:01:23,320 --> 00:01:28,320
Remember, this is a reminder of what we've been working on.

24
00:01:28,320 --> 00:01:33,800
We have a preamble that sets the objective

25
00:01:33,800 --> 00:01:36,120
of why we're doing this, what are the objectives,

26
00:01:36,120 --> 00:01:41,120
what are the reasons why we want to have a clear definition

27
00:01:41,120 --> 00:01:44,640
of open source AI, why that's necessary.

28
00:01:44,640 --> 00:01:48,600
And we want to also clarify what we think needs to be fixed

29
00:01:48,600 --> 00:01:52,720
and we need to be addressed the issues that AI creates

30
00:01:52,720 --> 00:01:55,960
are to be resolved at a different level,

31
00:01:55,960 --> 00:01:58,600
at the government level, governance level,

32
00:01:58,600 --> 00:02:02,400
deployment levels, and not inside the documents

33
00:02:02,400 --> 00:02:07,400
that are used to distribute and make AI systems available.

34
00:02:07,400 --> 00:02:12,800
Then we have worked quite heavily to define the freedoms,

35
00:02:12,800 --> 00:02:16,760
to define exactly what is open source AI in practice,

36
00:02:16,760 --> 00:02:21,760
what are the identifying elements of when we see it,

37
00:02:21,760 --> 00:02:27,000
we will know that that system is an open source AI.

38
00:02:27,000 --> 00:02:31,640
This top part, before what we call the legal checklist,

39
00:02:31,640 --> 00:02:35,400
this top part is the most important piece

40
00:02:35,400 --> 00:02:37,840
and the most urgent piece that we need to get right

41
00:02:37,840 --> 00:02:39,800
and we need to get it right quickly.

42
00:02:39,800 --> 00:02:46,320
What follows below is the space,

43
00:02:46,320 --> 00:02:50,960
is what we are reviewing and revising at this stage.

44
00:02:50,960 --> 00:02:53,360
The top part looks fairly stable

45
00:02:53,360 --> 00:02:56,720
and I really urge everyone to look at it,

46
00:02:56,720 --> 00:02:59,520
to give it a very solid, deep read,

47
00:02:59,520 --> 00:03:05,440
to see, to highlight possible mistakes,

48
00:03:05,440 --> 00:03:09,160
possible areas of improvement,

49
00:03:09,160 --> 00:03:11,840
because we really need to put this to bed

50
00:03:11,840 --> 00:03:13,040
as quickly as possible.

51
00:03:13,160 --> 00:03:17,160
And since no one has been giving a lot of comments to this

52
00:03:17,160 --> 00:03:19,400
in the past couple of iterations,

53
00:03:19,400 --> 00:03:24,400
I'd love to tell everyone, to remind everyone

54
00:03:24,400 --> 00:03:29,720
that this is very important that we complete the process

55
00:03:29,720 --> 00:03:33,120
and this top part is really, really, really the top,

56
00:03:33,120 --> 00:03:34,360
the most important one.

57
00:03:34,360 --> 00:03:39,080
What comes below is the,

58
00:03:39,080 --> 00:03:41,720
what comes below the legal checklist is,

59
00:03:43,000 --> 00:03:45,840
basically, an operation manual,

60
00:03:45,840 --> 00:03:48,800
or at least recommendations and the initial recommendations

61
00:03:48,800 --> 00:03:52,360
for operators of the,

62
00:03:52,360 --> 00:03:56,560
for the people who would be reviewing AI systems

63
00:03:56,560 --> 00:04:00,880
and evaluate whether they satisfy the definition

64
00:04:00,880 --> 00:04:03,080
about what is open source AI.

65
00:04:03,080 --> 00:04:08,000
And because these are strongly targeting,

66
00:04:08,000 --> 00:04:10,280
this checklist is strongly targeting

67
00:04:10,280 --> 00:04:11,480
machine learning systems,

68
00:04:11,480 --> 00:04:15,560
because those are the systems that introduce new elements

69
00:04:15,560 --> 00:04:19,280
and pose more challenges at the moment.

70
00:04:19,280 --> 00:04:21,160
In the future, it may be different,

71
00:04:21,160 --> 00:04:24,480
but at the moment, machine learning is what is creating

72
00:04:24,480 --> 00:04:29,480
this uncertainties around what is open source,

73
00:04:29,480 --> 00:04:34,600
rather than other systems.

74
00:04:34,600 --> 00:04:39,600
And the main reason is because the AI systems require data

75
00:04:40,600 --> 00:04:44,160
and they produce, machine learning systems require data

76
00:04:44,160 --> 00:04:46,080
and they produce model weights.

77
00:04:46,080 --> 00:04:51,080
And these two elements are fairly new in the software world.

78
00:04:51,080 --> 00:04:55,000
And we haven't really been thinking about how they interact

79
00:04:55,000 --> 00:04:56,560
with the open source definition.

80
00:04:56,560 --> 00:05:00,320
This whole exercise of finding the open source AI definition

81
00:05:00,320 --> 00:05:02,920
revolves around these disruption, if you want,

82
00:05:02,920 --> 00:05:07,400
this disturbance in the understanding

83
00:05:07,400 --> 00:05:08,960
of what open source means.

84
00:05:09,840 --> 00:05:12,160
And that's what we are analyzing.

85
00:05:12,160 --> 00:05:17,160
This checklist is the piece that we are using

86
00:05:17,160 --> 00:05:21,600
to evaluate whether a system is open source or not,

87
00:05:21,600 --> 00:05:25,960
the response to the open source definition for AI or not.

88
00:05:25,960 --> 00:05:29,920
So what we've been doing, we started in the fall

89
00:05:29,920 --> 00:05:34,920
and we found these principles with co-design workshops

90
00:05:34,920 --> 00:05:37,400
that we run in multiple parts of the world,

91
00:05:37,400 --> 00:05:40,640
like in October in Raleigh, North Carolina,

92
00:05:40,640 --> 00:05:41,960
then we went to Monterey.

93
00:05:41,960 --> 00:05:47,320
We went to Alisabeba with the United Nations

94
00:05:47,320 --> 00:05:50,200
Digital Public Goods Alliance to run another workshop.

95
00:05:50,200 --> 00:05:53,240
And the result of these conversations,

96
00:05:53,240 --> 00:05:55,600
also we had conversations online,

97
00:05:55,600 --> 00:05:59,520
came out, produced this Four Freedoms for AI,

98
00:05:59,520 --> 00:06:02,280
which is basically the four freedom for free software,

99
00:06:03,240 --> 00:06:08,240
adapted and reviewed and purposed for AI systems,

100
00:06:08,240 --> 00:06:12,240
thinking about the definition of AI system.

101
00:06:12,240 --> 00:06:18,320
Then we focused on understanding the required components

102
00:06:18,320 --> 00:06:24,360
that one needs to have for a machine learning system

103
00:06:24,360 --> 00:06:26,880
in order to exercise those freedoms.

104
00:06:30,520 --> 00:06:33,960
So what do you actually need in order to have access

105
00:06:33,960 --> 00:06:35,680
to these systems?

106
00:06:35,680 --> 00:06:40,680
So we had, again, more workshops in person in San Jose,

107
00:06:40,680 --> 00:06:46,920
and then we run virtual work groups online

108
00:06:46,920 --> 00:06:50,360
to analyze four specific systems,

109
00:06:50,360 --> 00:06:52,960
Lama2, Bloom, PTI, and OpenCV.

110
00:06:52,960 --> 00:06:56,480
And you will see a variety of elements in here.

111
00:06:56,480 --> 00:06:59,800
You see Lama2, it's a commercial project

112
00:06:59,800 --> 00:07:04,280
and distributed with legal terms

113
00:07:04,280 --> 00:07:05,920
that include restrictions,

114
00:07:05,920 --> 00:07:08,680
and the Open Source Initiative has already called Lama2

115
00:07:08,680 --> 00:07:10,120
not an open source system.

116
00:07:10,120 --> 00:07:15,760
But it's relevant to have the analysis.

117
00:07:15,760 --> 00:07:18,160
We looked at Bloom because it's an open science project.

118
00:07:18,160 --> 00:07:22,760
It's very open in terms of it distributes

119
00:07:22,760 --> 00:07:24,080
a lot of its components,

120
00:07:24,080 --> 00:07:27,440
and it's very complete from the science perspective.

121
00:07:27,440 --> 00:07:30,320
Also, we know that it's been shared with our license

122
00:07:30,320 --> 00:07:34,920
and with legal terms that prevent some uses.

123
00:07:34,920 --> 00:07:36,840
PTI is another open science project,

124
00:07:36,840 --> 00:07:38,520
and it's very permissive.

125
00:07:38,520 --> 00:07:42,920
And OpenCV is similarly an open source project

126
00:07:42,920 --> 00:07:47,840
with lots of openness and lots of open components

127
00:07:47,840 --> 00:07:50,520
made available, but it's non-generative.

128
00:07:50,520 --> 00:07:53,720
So it's a computer vision, machine learning,

129
00:07:53,720 --> 00:07:54,760
machine learning system.

130
00:07:54,760 --> 00:07:57,720
So adding a little bit of variety.

131
00:07:57,720 --> 00:08:01,760
We asked for volunteers and we reached out to volunteers

132
00:08:01,760 --> 00:08:06,760
to have the most, the widest possible diversity

133
00:08:06,760 --> 00:08:10,840
in different levels and different stages.

134
00:08:10,840 --> 00:08:14,480
We invited experts from like the developers

135
00:08:14,480 --> 00:08:18,240
and technical experts of each of these groups.

136
00:08:18,240 --> 00:08:22,360
And we included others.

137
00:08:22,360 --> 00:08:26,480
Like we invited Davide Testugine and Jonathan Torres

138
00:08:26,480 --> 00:08:29,960
from Meta into the LAMA working group explicitly

139
00:08:29,960 --> 00:08:32,080
because they are experts.

140
00:08:32,080 --> 00:08:34,920
And if someone couldn't find a component,

141
00:08:34,920 --> 00:08:37,520
we could ask the experts to go look for them

142
00:08:37,520 --> 00:08:41,000
and to tell us whether those were available or not.

143
00:08:41,000 --> 00:08:44,280
And that's the reason why they're here together with,

144
00:08:44,280 --> 00:08:46,720
for example, Stella Biderman from PTI.

145
00:08:48,040 --> 00:08:53,040
And we worked also, we aligned our work.

146
00:08:53,040 --> 00:09:01,280
We started with the model openness framework

147
00:09:01,280 --> 00:09:05,440
that is produced by Matt White, Ibrahim, Adad

148
00:09:05,440 --> 00:09:09,760
and others from the Linux Foundation and other researchers

149
00:09:09,760 --> 00:09:12,320
who they have analyzed machine learning

150
00:09:12,320 --> 00:09:15,640
and they grouped the required, the components

151
00:09:15,640 --> 00:09:20,160
that go into producing, distributing a system.

152
00:09:20,160 --> 00:09:25,200
And we based on that list of components,

153
00:09:25,200 --> 00:09:28,480
we asked the volunteers in the working groups

154
00:09:28,480 --> 00:09:31,240
to go look and see which of these components

155
00:09:31,240 --> 00:09:33,520
you think is required to study,

156
00:09:33,520 --> 00:09:35,960
which one is required to use the system,

157
00:09:35,960 --> 00:09:38,280
which one is required to share and modify.

158
00:09:38,280 --> 00:09:40,920
And they voted for each of these components.

159
00:09:42,080 --> 00:09:47,080
Then we composed and weighted all those results,

160
00:09:47,080 --> 00:09:49,240
all those votes.

161
00:09:49,240 --> 00:09:53,240
And we came up with a recommendation summary

162
00:09:53,240 --> 00:09:59,480
of what is required and what is less necessary

163
00:09:59,480 --> 00:10:06,080
or gathered less votes from the working groups.

164
00:10:06,080 --> 00:10:09,640
And that has produced the latest,

165
00:10:11,120 --> 00:10:15,600
no, that has produced a list of components

166
00:10:15,600 --> 00:10:19,120
for which we needed to go look at the legal documents.

167
00:10:19,120 --> 00:10:22,720
So we had the required components,

168
00:10:22,720 --> 00:10:27,080
we grouped them into three main areas for code,

169
00:10:27,080 --> 00:10:30,840
for what we call model, and then for data

170
00:10:30,840 --> 00:10:35,400
because the votes didn't reach a very high threshold

171
00:10:35,400 --> 00:10:37,280
for the votes of the working groups

172
00:10:37,280 --> 00:10:39,680
didn't reach a very high threshold

173
00:10:39,680 --> 00:10:43,400
to require the original training dataset.

174
00:10:43,400 --> 00:10:46,520
And because of other practical considerations,

175
00:10:46,520 --> 00:10:51,000
we substituted the information,

176
00:10:51,000 --> 00:10:54,560
we built a proxy for the access,

177
00:10:54,560 --> 00:10:57,200
having access to the original training dataset.

178
00:10:57,200 --> 00:11:01,880
We replaced it with requirements for data transparency.

179
00:11:01,880 --> 00:11:03,840
Like we wanna have the maximum amount

180
00:11:03,840 --> 00:11:06,320
of transparency on the data.

181
00:11:06,320 --> 00:11:09,040
And we wanna have also the tools

182
00:11:09,040 --> 00:11:12,960
instead of the actual dataset,

183
00:11:12,960 --> 00:11:17,960
the tools to build compatible data datasets.

184
00:11:17,960 --> 00:11:19,640
So we want to have the code

185
00:11:19,640 --> 00:11:23,160
that went into building that dataset as an alternative.

186
00:11:23,160 --> 00:11:27,240
And now with that list of the required components,

187
00:11:27,240 --> 00:11:30,000
we went and looked at the legal frameworks

188
00:11:30,000 --> 00:11:31,400
for each of these components.

189
00:11:31,400 --> 00:11:35,800
And we ended up with having basically everything

190
00:11:35,800 --> 00:11:37,760
we discovered basically that everything,

191
00:11:37,760 --> 00:11:38,960
all the required components,

192
00:11:38,960 --> 00:11:42,000
they fall under copyright quite clearly

193
00:11:42,000 --> 00:11:44,040
except one of the components.

194
00:11:44,040 --> 00:11:46,400
So everything that is code,

195
00:11:46,400 --> 00:11:49,960
so in this slide, you can see everything

196
00:11:49,960 --> 00:11:54,000
that is in the pre-training, I mean, the code section,

197
00:11:54,000 --> 00:11:55,640
in the data transparency section,

198
00:11:55,640 --> 00:11:58,120
that's documentation basically.

199
00:11:58,120 --> 00:12:00,360
And in model architecture,

200
00:12:00,360 --> 00:12:03,800
those are all distributed as code

201
00:12:03,800 --> 00:12:05,320
that is written by a human,

202
00:12:05,320 --> 00:12:08,560
falls squarely under copyright

203
00:12:08,560 --> 00:12:12,360
and maybe patent law eventually.

204
00:12:12,360 --> 00:12:15,400
But it's something that we are very familiar with.

205
00:12:15,400 --> 00:12:17,400
It's an environment that we understand very well.

206
00:12:17,400 --> 00:12:21,000
It's an environment where we have, it's a legal framework.

207
00:12:21,000 --> 00:12:24,320
These are legal frameworks for which we have licenses

208
00:12:24,320 --> 00:12:27,720
and clear understanding of what those means.

209
00:12:27,720 --> 00:12:30,280
But for the model parameters,

210
00:12:30,280 --> 00:12:31,960
and these include, for example,

211
00:12:31,960 --> 00:12:35,080
the weights and the biases.

212
00:12:35,080 --> 00:12:40,080
For model parameters, we don't have a universal understanding

213
00:12:40,080 --> 00:12:50,280
around the world of what these fall under,

214
00:12:50,280 --> 00:12:52,360
what kind of laws they fall under.

215
00:12:52,360 --> 00:12:55,600
So we have to be a bit more careful evaluating

216
00:12:55,600 --> 00:13:00,760
what are the terms that we want to apply here

217
00:13:00,760 --> 00:13:03,640
and how we want to have,

218
00:13:03,640 --> 00:13:05,920
evaluate the legal terms

219
00:13:05,920 --> 00:13:08,200
under which model parameters are distributed.

220
00:13:08,200 --> 00:13:12,280
So I mentioned that we worked a lot

221
00:13:12,280 --> 00:13:17,280
to get global representation in this process,

222
00:13:17,280 --> 00:13:21,840
because ultimately we want,

223
00:13:21,840 --> 00:13:23,640
we have engaged with,

224
00:13:23,640 --> 00:13:26,160
we have identified these groups of stakeholders,

225
00:13:26,160 --> 00:13:28,880
like system creator, license creators, regulators,

226
00:13:28,880 --> 00:13:32,080
licensees, end users, and subject.

227
00:13:32,080 --> 00:13:35,440
And we have tried to engage with as many as possible.

228
00:13:35,440 --> 00:13:39,000
With the most involved ones in the current phase

229
00:13:39,000 --> 00:13:41,480
have been the licensees and system creators

230
00:13:41,480 --> 00:13:43,400
and license creators,

231
00:13:43,400 --> 00:13:48,400
so lawyers and integrators and developers.

232
00:13:48,400 --> 00:13:54,600
And we are expanding our reach now at this stage

233
00:13:55,440 --> 00:13:59,040
to end users, subjects, and regulators.

234
00:13:59,040 --> 00:14:00,640
And as a proxy for regulators,

235
00:14:00,640 --> 00:14:02,480
we're gonna be engaging with civil society

236
00:14:02,480 --> 00:14:05,720
who talks to regulators, lawmakers around the world.

237
00:14:05,720 --> 00:14:10,960
And so we have closed that phase,

238
00:14:10,960 --> 00:14:14,640
so we have now the next steps,

239
00:14:14,640 --> 00:14:17,200
what's happening in the next steps.

240
00:14:17,200 --> 00:14:21,600
We're getting ready to release the next draft, draft 08.

241
00:14:21,600 --> 00:14:24,680
I'm actually at a conference this week

242
00:14:24,680 --> 00:14:26,400
where I gathered a lot of feedback

243
00:14:26,400 --> 00:14:30,360
and may be able to release at 08 before May,

244
00:14:30,360 --> 00:14:35,360
and maybe go into Pittsburgh at the PyCon

245
00:14:35,360 --> 00:14:38,680
with an even higher version number,

246
00:14:38,680 --> 00:14:43,680
even more clearly towards a release candidate with a 0.1.

247
00:14:43,680 --> 00:14:47,280
So skipping that level

248
00:14:47,280 --> 00:14:50,560
and going into minor release numbering.

249
00:14:53,440 --> 00:14:55,440
And like a feature complete,

250
00:14:55,440 --> 00:15:00,440
but with close to be a release candidate in June.

251
00:15:00,440 --> 00:15:06,400
Between in June, we'll hold in-person or online.

252
00:15:06,400 --> 00:15:12,560
This is getting into a territory

253
00:15:12,560 --> 00:15:14,400
where we'd love to have it in person,

254
00:15:14,400 --> 00:15:16,760
but we also would love to have different people

255
00:15:16,760 --> 00:15:18,800
from different parts of the world.

256
00:15:18,800 --> 00:15:23,800
And so it's probably most likely gonna be online

257
00:15:23,800 --> 00:15:27,040
instead of in-person, but we'll see what happens.

258
00:15:27,040 --> 00:15:30,280
In any case, in June, we wanna have a group

259
00:15:30,280 --> 00:15:33,720
of important relevant stakeholders

260
00:15:33,720 --> 00:15:36,440
who have been involved into the drafting process

261
00:15:36,440 --> 00:15:41,440
to meet and release the release candidate number one.

262
00:15:41,440 --> 00:15:45,360
And during the summer months between July

263
00:15:45,360 --> 00:15:48,240
and the end of October,

264
00:15:48,240 --> 00:15:53,240
we have a plan to go through a worldwide roadshow

265
00:15:53,240 --> 00:15:58,960
to demonstrate, to illustrate, to advocate for,

266
00:15:58,960 --> 00:16:02,600
and gather in different parts of the world

267
00:16:02,600 --> 00:16:07,000
and gather more sustained to support

268
00:16:07,000 --> 00:16:10,160
and endorsement from different groups and organizations

269
00:16:10,160 --> 00:16:14,400
so that at the end of the October in Raleigh

270
00:16:14,400 --> 00:16:16,360
at All Things Open,

271
00:16:16,360 --> 00:16:21,360
the OSI board can review the draft with the comments

272
00:16:21,360 --> 00:16:26,440
and release the stable version at the end of the month.

273
00:16:26,440 --> 00:16:29,240
And this is our plan.

274
00:16:29,240 --> 00:16:32,600
I think we're getting very close to the finish line

275
00:16:32,600 --> 00:16:34,440
and it's really exciting.

276
00:16:34,440 --> 00:16:37,280
If you haven't been involved until now,

277
00:16:37,280 --> 00:16:40,720
I really, really, really, really encourage you

278
00:16:40,720 --> 00:16:43,040
to go to the online forums

279
00:16:43,040 --> 00:16:47,920
where we have healthy, deep conversations.

280
00:16:47,920 --> 00:16:53,320
Keep coming to these down-home meetings,

281
00:16:53,320 --> 00:16:55,760
ask questions, and stay engaged

282
00:16:55,760 --> 00:16:59,120
because we are making history right now.

283
00:16:59,120 --> 00:17:04,120
We are writing a definition that we hope will remove,

284
00:17:04,120 --> 00:17:08,760
that we want people to use to remove uncertainties

285
00:17:08,760 --> 00:17:12,520
and doubts from the market.

286
00:17:12,520 --> 00:17:15,400
People are releasing more models.

287
00:17:15,400 --> 00:17:17,360
They're using the open source moniker.

288
00:17:17,360 --> 00:17:19,560
They are confusing regulators

289
00:17:19,560 --> 00:17:21,440
and legislators around the world.

290
00:17:21,440 --> 00:17:24,560
And we need to provide certainties.

291
00:17:24,560 --> 00:17:27,080
We need to provide a stable view

292
00:17:27,080 --> 00:17:29,640
of what open source AI means

293
00:17:29,640 --> 00:17:32,320
so that regulation can come in

294
00:17:32,320 --> 00:17:39,160
and encourage innovation without causing harm,

295
00:17:39,160 --> 00:17:42,720
without spreading more disinformation

296
00:17:42,720 --> 00:17:46,600
and providing a positive environment.

297
00:17:46,600 --> 00:17:49,240
So with that, I'm gonna take a pause.

298
00:17:49,240 --> 00:17:54,000
And if you have any questions, I'm happy to respond.

299
00:17:54,000 --> 00:17:59,000
Any curiosities?

300
00:17:59,000 --> 00:18:21,160
- Yeah, I just have a question.

301
00:18:21,160 --> 00:18:24,160
So again, you may have touched based on it,

302
00:18:24,160 --> 00:18:26,000
but I was not sure it was clear.

303
00:18:26,000 --> 00:18:29,600
You do not touch at all on the data used for training

304
00:18:29,600 --> 00:18:33,160
except getting some form of transparency.

305
00:18:33,160 --> 00:18:34,520
Do I get it right?

306
00:18:34,520 --> 00:18:37,760
- Yes, that is correct.

307
00:18:37,760 --> 00:18:40,680
This is a, it's really,

308
00:18:40,680 --> 00:18:43,960
we started to work on a frequently asked question document

309
00:18:43,960 --> 00:18:46,680
because it's becoming recurrent now

310
00:18:46,680 --> 00:18:50,560
as more people are becoming aware of the process.

311
00:18:50,560 --> 00:18:53,360
The data issue has been,

312
00:18:53,360 --> 00:18:55,840
we have debated it at the very beginning

313
00:18:55,840 --> 00:18:58,000
and it's been really confusing.

314
00:18:58,000 --> 00:19:01,880
It's been really, it's been going around in circles.

315
00:19:01,880 --> 00:19:04,120
Like the very big issue with data

316
00:19:04,120 --> 00:19:09,120
is that if we require the original dataset

317
00:19:09,120 --> 00:19:12,480
to be distributed together with the model weights

318
00:19:12,480 --> 00:19:15,240
and parameters and the rest of the code,

319
00:19:15,240 --> 00:19:19,880
we will automatically exclude from the pool

320
00:19:19,880 --> 00:19:22,000
of possible AI systems.

321
00:19:22,000 --> 00:19:26,640
We will exclude systems that don't have data

322
00:19:26,640 --> 00:19:30,320
where the data is not available, right?

323
00:19:30,320 --> 00:19:32,240
Like federated learning,

324
00:19:32,240 --> 00:19:39,240
federated learning or privacy preserving,

325
00:19:39,240 --> 00:19:44,720
training mechanisms, all of those, for example,

326
00:19:44,720 --> 00:19:48,320
will not be part of the open source AI ecosystem

327
00:19:48,320 --> 00:19:50,680
because there is no data.

328
00:19:50,680 --> 00:19:55,200
The other reason is that many times

329
00:19:55,200 --> 00:19:59,840
you have the right to download, for example,

330
00:19:59,840 --> 00:20:02,920
information from a website

331
00:20:02,920 --> 00:20:06,040
in order to do data mining on it,

332
00:20:06,040 --> 00:20:09,520
but you don't have the right to redistribute it further.

333
00:20:09,520 --> 00:20:12,720
So also in these cases,

334
00:20:12,720 --> 00:20:14,680
you got a model parameters

335
00:20:14,680 --> 00:20:17,800
that you can definitely use.

336
00:20:17,800 --> 00:20:20,840
You have instructions to rebuild that dataset,

337
00:20:20,840 --> 00:20:23,760
but you cannot really have the original dataset

338
00:20:23,760 --> 00:20:25,680
because it's illegal to distribute it.

339
00:20:25,680 --> 00:20:30,840
And so to obviate for these issues,

340
00:20:30,840 --> 00:20:35,040
it's much easier and probably also more relevant

341
00:20:35,040 --> 00:20:38,400
to have access instead as a proxy to the tooling,

342
00:20:38,400 --> 00:20:41,320
to the filtering and to the instructions

343
00:20:41,320 --> 00:20:45,800
on how the dataset was built as a minimum.

344
00:20:45,800 --> 00:20:48,760
Again, these are default requirement.

345
00:20:48,760 --> 00:20:52,560
Nothing really prevents from something like

346
00:20:52,560 --> 00:20:57,920
Starcoder or other systems

347
00:20:57,920 --> 00:21:00,160
that have been built with open science in mind

348
00:21:00,160 --> 00:21:02,880
and have been very careful.

349
00:21:02,880 --> 00:21:07,120
Like Luther AI is working on the pile

350
00:21:07,120 --> 00:21:10,200
to a new dataset that is more,

351
00:21:10,200 --> 00:21:14,600
since they became more concerned

352
00:21:14,600 --> 00:21:18,840
about the copyright status of the input,

353
00:21:18,840 --> 00:21:21,320
the training datasets,

354
00:21:21,320 --> 00:21:22,960
they're rebuilding the pile,

355
00:21:22,960 --> 00:21:27,640
excluding all the possible sources of lawsuits

356
00:21:27,640 --> 00:21:30,880
and DMCA takedowns.

357
00:21:30,880 --> 00:21:35,080
So nothing excludes from building datasets

358
00:21:35,080 --> 00:21:36,920
that are relevant and important

359
00:21:36,920 --> 00:21:40,120
and can be distributed further,

360
00:21:40,120 --> 00:21:45,120
but that would make the open source AI,

361
00:21:45,120 --> 00:21:48,960
would put the, having that as a harder requirement,

362
00:21:48,960 --> 00:21:52,120
would put the open source AI at a disadvantage

363
00:21:52,120 --> 00:21:57,000
compared to the commercial proprietary systems

364
00:21:57,000 --> 00:21:59,800
where they basically don't even disclose

365
00:21:59,800 --> 00:22:03,720
the list of the data.

366
00:22:03,720 --> 00:22:04,560
Yeah.

367
00:22:04,560 --> 00:22:07,240
- That makes sense.

368
00:22:07,240 --> 00:22:08,080
Thanks a lot.

369
00:22:08,080 --> 00:22:10,120
So that's indeed very clear.

370
00:22:10,120 --> 00:22:10,960
- Absolutely.

371
00:22:10,960 --> 00:22:11,800
- Thank you.

372
00:22:11,800 --> 00:22:12,640
- Absolutely.

373
00:22:12,640 --> 00:22:16,160
All right.

374
00:22:16,160 --> 00:22:17,480
If there are no more questions,

375
00:22:17,480 --> 00:22:21,640
then I will close it here.

376
00:22:21,640 --> 00:22:25,200
And again, please go to the forums

377
00:22:25,200 --> 00:22:27,840
and there is a long thread about data

378
00:22:27,840 --> 00:22:30,880
where you can see the past conversation

379
00:22:30,880 --> 00:22:34,560
on this very hot topic.

380
00:22:34,560 --> 00:22:38,160
And yeah, we can continue there,

381
00:22:38,160 --> 00:22:42,520
or we can also move on to the other big issue,

382
00:22:42,520 --> 00:22:45,520
which is defining and understanding a little bit better,

383
00:22:45,520 --> 00:22:49,200
gathering more comments on the legal status

384
00:22:49,200 --> 00:22:54,200
of the model parameters and understand,

385
00:22:54,200 --> 00:22:57,600
get more suggestions on how we should treat them.

386
00:22:57,600 --> 00:23:02,960
Thanks everyone for joining and talk to you soon.

387
00:23:02,960 --> 00:23:05,080
In two weeks, we'll redo this.

388
00:23:05,080 --> 00:23:05,920
Bye.

### End of last town hall held on 2024-04-19 ###

### Start of next town hall held on 2024-05-03 ###
--- Presentation for 2024-05-03 ---
OPEN SOURCE AI DEFINITION
Online public townhall
May 3, 2024
last updated: May 2, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

OSI’s objective for 2024

Open Source AI Definition

3
3

Open Source AI Definition

Where Are We Now?

4

Open
Source AI
Definition
Elements

Preamble
4 Freedoms

v.0.0.8

Legal Checklist

-

Open
Source AI
Definition
Elements

Preamble
4 Freedoms

v.0.0.8

Now feature
complete for
required and
optional
components

Legal Checklist

-

Open Source AI Definition

How Did We Get Here?

7

Open Source AI Definition

The 4 Freedoms for AI
Fall 2023
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

8

The 4 Freedoms for AI
Use • Study • Modify • Share

What should these
open source principles mean
for artificial intelligence?

9

Co-Design Workshop: Raleigh
All Things Open | October 2023

10

Co-Design Workshop: Monterey
Linux Foundation Member Summit | October 2023

11

Co-Design Workshop: Addis Ababa
Digital Public Goods Alliance Members Meeting | November 2023

12

Open Source AI Definition
The 4 Freedoms for AI
1. Use the system for any purpose and without having
to ask for permission.
2. Study how the system works and inspect its
components.
3. Modify the system for any purpose, including to
change its output.
4. Share the system for others to use with or without
modifications, for any purpose.
13

Open Source AI Definition

Required Components
Winter 2023-24
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

14

Required Components for Open Source AI

What components must be
open in order for an AI system
to be used, studied, modified,
and shared?

15

Co-Design Workshop: San Jose
AI.dev | December 2023

16

Virtual Workgroups
Selected to represent a diversity of approaches to AI openness:
1.

Llama 2: commercial project, accompanied by limited amount of
science and with a restrictive license

2.

BLOOM: open science project, with lots of details released but
shared with a restrictive license

3.

Pythia: open science project, with a permissive license

4.

OpenCV: open source project, with ML components outside of the
generative AI space

17

better global representation, we conducted outreach to Black, Indigenous,
Workgroup Members Toandachieve
other people of color, particularly women and individuals from the Global South.

Llama 2
1.
2.
3.
4.
5.
6.
7.
8.

Bastien Guerry
DINUM, French public
administration
Ezequiel Lanza Intel
Roman Shaposhnik
Apache Software
Foundation
Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute of
Paris
Mo Zhou Debian,
Johns Hopkins
University
Victor Lu independent
database consultant

BLOOM
1.
2.
3.
4.
5.
6.
7.
8.

George C. G. Barbosa
Fundação Oswaldo Cruz
Daniel Brumund GIZ
FAIR Forward - AI for all
Danish Contractor
BLOOM Model Gov. WG
Abdoulaye Diack
Google
Jaan Li University of
Tartu, Phare Health
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
Ofentse Phuti WiMLDS
Gaborone
Caleb Fianku Quao
Kwame Nkrumah
University of Science and
Technology, Kumasi

Pythia
1.
2.

3.
4.

Seo-Young Isabelle
Hwang Samsung
Cailean Osborne
University of Oxford,
Linux Foundation
Stella Biderman
EleutherAI
Justin Colannino
Microsoft

5.

Hailey Schoelkopf
EleutherAI

6.

Aviya Skowron
EleutherAI

Over 50% of all
workgroup
participants are
people of color.

OpenCV
1.
2.
3.
4.
5.
6.
7.

8.
9.
10.

Rahmat Akintola
Cubeseed Africa
Ignatius Ezeani
Lancaster University
Kevin Harerimana CMU
Africa
Satya Mallick OpenCV
David Manset ITU
Phil Nelson
OpenCV
Tlamelo Makati
WiMLDS Gaborone,
Technological University
Dublin
Minyechil Alehegn
Tefera Mizan Tepi
University
Akosua Twumasi
Ghana Health Service
Rasim Sen Oasis
Software Technology Ltd.

Workgroups: Required Component Selection
Component List

19

Available on arXiv
CC BY-NC-SA 4.0

Workgroups: Required Component Selection
Component
List
Component Votes

20

Example:
Llama 2
Workgroup

Workgroups: Required Component Selection
Component
List

Component
Votes
Vote Compilation

21

Workgroups: Required Component Selection
Component
List

Component
Votes
Vote
Compilation
Recommendation Report

22

v.0.0.6

Workgroups: Required Component Selection
Component
List

Component
Votes
Vote
Compilation

Recommendation
Report
Definition
Checklist

23

v.0.0.6

Open Source AI Definition

Legal Documents Review
Early Spring 2024
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

24

Workgroups: Legal Documents Review

25

v.0.0.6

Workgroups: Legal Documents Review

26

v.0.0.6

Workgroups: Legal Documents Review

27

v.0.0.6

Open Source AI Definition
Required Components
v.0.0.8

28

Open Source AI Definition
Optional Components
v.0.0.8

29

Open Source AI Definition

A Representative Process
Diversity, Inclusion, and Equity
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

30

Equitable and inclusive stakeholder
representation isn’t only about for
justice, it’s about legitimacy.

A global definition requires
global consultation.
31

Stakeholder

Description

Example

1. System
Creator

Makes AI system and/or component that will be
studied, used, modified, or shared through an open
source license

ML researcher in academia or industry

2. License
Creator

Writes or edits the open source license to be applied
to the AI system or component, includes compliance

IP lawyer

3. Regulator

Writes or edits rules governing licenses and systems

government policy-maker

4. Licensee

Seeks to study, use modify, or share an open source
AI system

AI engineer in industry, health researcher in
academia

5. End User

Consumes a system output, but does not seek to
study, use, modify, or share the system

student using a chatbot to write a report, artist
creating an image

6. Subject

Affected upstream or downstream by a system output
without interacting with it intentionally + advocates for
this group.

photographer who finds their image in training
32
dataset (upstream), mortgage applicant
evaluated
by a bank’s AI system (downstream)

32

Stakeholder

Most
involved
in current
phase

Description

Example

1. System
Creator

Makes AI system and/or component that will be
studied, used, modified, or shared through an open
source license

ML researcher in academia or industry

2. License
Creator

Writes or edits the open source license to be applied
to the AI system or component, includes compliance

IP lawyer

3. Regulator

Writes or edits rules governing licenses and systems

government policy-maker

4. Licensee

Seeks to study, use modify, or share an open source
AI system

AI engineer in industry, health researcher in
academia

5. End User

Consumes a system output, but does not seek to
study, use, modify, or share the system

student using a chatbot to write a report, artist
creating an image

6. Subject

Affected upstream or downstream by a system output
without interacting with it intentionally + advocates for
this group.

photographer who finds their image in training
33
dataset (upstream), mortgage applicant
evaluated
by a bank’s AI system (downstream)

33

34

Open Source AI Definition

Next Steps
Spring - Fall, 2024
● Creating content for definition v. 0.0.7
● Release by next Friday, April 12th

35

Definition Validation
● Confirm current systems (Llama 2, Pythia, BLOOM,
OpenCV) equally reviewable under v. 0.0.8
● Seeking volunteers to review 1 to 3 additional AI
systems to see how well they align with the
definition
○
●

Contact Mer at Mer@dobiggood.com if you are interested.

Due Monday, May 20th
36

Reviewers

We are interested in reviewing about 10 AI systems self-described as open as part of this definition process.
Those marked (*) have were reviewed in previous phases. Other systems are newly added.

1. Arctic
1.

Seeking volunteer

2. BLOOM*
2.
3.

Danish Contractor
BLOOM Model Gov.
Work Group
Jaan Li University of
Tartu, Phare Health

5. Llama 2*
1.
2.
3.
4.

Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute of
Paris
Victor Lu independent
database consultant

8. OpenCV*
1.

Rasim Sen Oasis
Software Technology
Ltd.

11. T5
1.

Jaan Li University of
Tartu, Phare Health

9. Phi-2
1.
2.

Seo-Young Isabelle
Hwang Samsung
Abdoulaye Diack
Google

3. Falcon
1.

Casey Valk Nutanix

6. Mistral
5.

4. Grok
1.

Victor Lu independent
database consultant

Mark Collier
OpenInfra Foundation

10. Pythia*
3.
4.

7. OLMo
6.
7.

Amanda Casari
Google
Abdoulaye Diack
Google

Seo-Young Isabelle
Hwang Samsung
Stella Biderman
EleutherAI

5.

Hailey Schoelkopf
EleutherAI

6.

Aviya Skowron
EleutherAI

Additional
volunteers
welcome on all
systems

System testing work stream
Stakeholder consultation work stream

2024 Timeline

Release schedule

OSAID v. 0.0.8
last week

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Townhalls +

Townhall +

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7 and 8

PyCon
Workshop
May 17th,
Pittsburgh)

(≈

Draft 0.0.9

… October
Monthly Virtual
Meetings

Release stable
version

Virtual Launch
Event (date
TBD)

RC1

Stable
Version

In-Person Meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

PyCon US

May 17

Europe

France

Paris

OW2

June 11-12

Africa

Virtual

Virtual

Sustain Africa

June

North America

United States

New York

OSPOs for Good

July 9 - 11

Asia Paciﬁc

China

Hong Kong

AI_dev

August 23

Latin America

Argentina

Buenos Aires

Nerdearla

September

Europe

France

Paris

(data governance)

September

North America

United States

Raleigh

All Things Open

Oct 27 - 29

39

Open Source AI Definition
Get Involved
●
●

●
●

Public forum: discuss.opensource.org
Become an OSI Member
○ Free or or full
○ SSO with other OSI websites
Biweekly virtual town halls… like this one!
Volunteer for definition validation (email Mer)
40

Q&A

41

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the
deﬁnitional process.

42


--- Subtitles for 2024-05-03 --- ###
1
00:00:00,001 --> 00:00:07,000
>> Hello, everyone.

2
00:00:07,000 --> 00:00:10,000
>> Hi, welcome.

3
00:00:10,000 --> 00:00:15,000
>> Do you want to take it now?

4
00:00:15,000 --> 00:00:18,000
>> I will.

5
00:00:18,000 --> 00:00:21,000
>> It's becoming a tradition that we tag team on this one.

6
00:00:21,000 --> 00:00:22,000
>> Yeah.

7
00:00:22,000 --> 00:00:24,000
Teamwork.

8
00:00:24,000 --> 00:00:30,000
>> So, yes, welcome to the online public town hall for the

9
00:00:30,000 --> 00:00:33,000
open source AI definition for May 3rd.

10
00:00:33,000 --> 00:00:36,000
We always go through our community agreements.

11
00:00:36,000 --> 00:00:40,000
Those who have been here before are familiar, but just to

12
00:00:40,000 --> 00:00:44,000
remember one mic, one speaker, take space, make space.

13
00:00:44,000 --> 00:00:48,000
So if you ask a question, wait for others to ask before you

14
00:00:48,000 --> 00:00:50,000
ask another one.

15
00:00:50,000 --> 00:00:54,000
If you don't want to ask a question, pause and let others

16
00:00:54,000 --> 00:00:56,000
take a chance to speak.

17
00:00:56,000 --> 00:01:00,000
And if you don't usually speak in a public venue, we invite

18
00:01:00,000 --> 00:01:03,000
you to say what's on your mind.

19
00:01:03,000 --> 00:01:08,000
Kindness, just that we -- the work is hard, but that we be

20
00:01:08,000 --> 00:01:10,000
gentle with each other.

21
00:01:10,000 --> 00:01:15,000
And obviously hate speech is not permitted in this space.

22
00:01:15,000 --> 00:01:17,000
Forward motion.

23
00:01:17,000 --> 00:01:20,000
We start by focusing on what's possible.

24
00:01:20,000 --> 00:01:24,000
And we note obstacles and come back to them, but we reroute

25
00:01:24,000 --> 00:01:27,000
around them and do what is possible in the moment.

26
00:01:27,000 --> 00:01:29,000
We seek solutions.

27
00:01:29,000 --> 00:01:33,000
And we know that that is vulnerable, but it is crucial and

28
00:01:33,000 --> 00:01:36,000
that we are all needed in this work.

29
00:01:36,000 --> 00:01:41,000
And are there any other -- any other community agreements that

30
00:01:41,000 --> 00:01:46,000
we have for this meeting today?

31
00:01:46,000 --> 00:01:48,000
Okay.

32
00:01:48,000 --> 00:01:53,000
So -- playing with my windows one moment.

33
00:01:53,000 --> 00:01:54,000
Okay.

34
00:01:54,000 --> 00:01:56,000
There we go.

35
00:01:56,000 --> 00:01:57,000
Okay.

36
00:01:57,000 --> 00:02:01,000
So, yes, as you all know, our objective for 2024 is to have a

37
00:02:01,000 --> 00:02:05,000
stable version of the open source AI definition and that is

38
00:02:05,000 --> 00:02:07,000
still scheduled for October.

39
00:02:07,000 --> 00:02:09,000
Where are we now?

40
00:02:09,000 --> 00:02:12,000
We have a couple slides that are similar to our last meeting,

41
00:02:12,000 --> 00:02:15,000
but the last meeting was in a different time zone, so I'm

42
00:02:15,000 --> 00:02:18,000
hoping this won't be redundant for anyone.

43
00:02:18,000 --> 00:02:22,000
We are on version 0.0.8, which is feature complete.

44
00:02:22,000 --> 00:02:24,000
It's our feature complete version.

45
00:02:24,000 --> 00:02:28,000
So we have the preamble, the four freedoms, which have been

46
00:02:28,000 --> 00:02:30,000
here for a while.

47
00:02:30,000 --> 00:02:34,000
And then we now have a checklist, legal checklist for

48
00:02:34,000 --> 00:02:38,000
required and optional components that has the required legal

49
00:02:38,000 --> 00:02:40,000
checklist for every single component.

50
00:02:40,000 --> 00:02:44,000
So that has been the new content for this version.

51
00:02:44,000 --> 00:02:47,000
And very briefly, how did we get here?

52
00:02:47,000 --> 00:02:52,000
I see there's chat, and I will trust Stefano and Nick to pause

53
00:02:52,000 --> 00:02:56,000
me if I need to attend to that.

54
00:02:56,000 --> 00:03:00,000
So we started with the four freedoms in the fall of 2023.

55
00:03:00,000 --> 00:03:03,000
I know some of you are actually part of this process.

56
00:03:03,000 --> 00:03:06,000
And this was study, use, modify, and share.

57
00:03:06,000 --> 00:03:10,000
What should these open source principles mean for AI?

58
00:03:10,000 --> 00:03:15,000
And so we had in-person co-design workshops.

59
00:03:15,000 --> 00:03:18,000
We had one at All Things Open.

60
00:03:18,000 --> 00:03:22,000
We had one at Linux Foundation Member Summit.

61
00:03:22,000 --> 00:03:26,000
We had one at the Digital Public Goods Alliance member meeting

62
00:03:26,000 --> 00:03:28,000
in Addis Ababa.

63
00:03:28,000 --> 00:03:32,000
And from that, we came up with the definitions of the four

64
00:03:32,000 --> 00:03:34,000
freedoms for AI.

65
00:03:34,000 --> 00:03:38,000
So these are in 0.0.8.

66
00:03:38,000 --> 00:03:43,000
And they should not surprise anyone, but we've now formalized

67
00:03:43,000 --> 00:03:45,000
them for this technological context.

68
00:03:45,000 --> 00:03:50,000
So we have use the system for any purpose without asking permission,

69
00:03:50,000 --> 00:03:54,000
study the system and inspect its components, modify for any purpose,

70
00:03:54,000 --> 00:03:59,000
and then also to use with or without modification, again,

71
00:03:59,000 --> 00:04:02,000
for any purpose.

72
00:04:02,000 --> 00:04:05,000
I actually will pause for comments at this point.

73
00:04:05,000 --> 00:04:07,000
Let's see.

74
00:04:07,000 --> 00:04:08,000
Yep.

75
00:04:08,000 --> 00:04:09,000
Oh, I see.

76
00:04:09,000 --> 00:04:10,000
Great.

77
00:04:10,000 --> 00:04:15,000
So in the winter, we asked this very big question.

78
00:04:15,000 --> 00:04:19,000
What components must be open in order for an AI system to be used,

79
00:04:19,000 --> 00:04:22,000
studied, modified, and shared, of which there are many,

80
00:04:22,000 --> 00:04:27,000
many different opinions and many, many valid opinions?

81
00:04:27,000 --> 00:04:31,000
And we started doing this, again, by having in-person workshops.

82
00:04:31,000 --> 00:04:35,000
We were at AI Dev in December in San Jose.

83
00:04:35,000 --> 00:04:38,000
And then we got some really good feedback,

84
00:04:38,000 --> 00:04:44,000
which is that working in person only is exclusionary because not

85
00:04:44,000 --> 00:04:46,000
everyone can be in the room.

86
00:04:46,000 --> 00:04:52,000
And so we shifted into virtual work groups in January of this year.

87
00:04:52,000 --> 00:04:57,000
And we also decided to have our work groups focus on specific AI

88
00:04:57,000 --> 00:05:02,000
systems self-described as open so that we could get specific in all

89
00:05:02,000 --> 00:05:06,000
these different questions and debates around open-source AI.

90
00:05:06,000 --> 00:05:10,000
And we're basically looking for systems that represent a diversity of

91
00:05:10,000 --> 00:05:12,000
approaches to AI openness.

92
00:05:12,000 --> 00:05:18,000
And we chose Wama2, Bloom, Pythia, and OpenCV.

93
00:05:18,000 --> 00:05:24,000
And then we recruited members for these groups.

94
00:05:24,000 --> 00:05:28,000
And we were very concerned about having global representation.

95
00:05:28,000 --> 00:05:31,000
So we conducted specific outreach to Black, Indigenous,

96
00:05:31,000 --> 00:05:34,000
and other people of color, particularly women and individuals

97
00:05:34,000 --> 00:05:36,000
from the global South.

98
00:05:36,000 --> 00:05:40,000
And part of being a member of the work group is to have your name and

99
00:05:40,000 --> 00:05:44,000
affiliation shared publicly for the sake of transparency.

100
00:05:44,000 --> 00:05:48,000
And over 50% of work group participants are people of color.

101
00:05:48,000 --> 00:05:50,000
And you can see them here.

102
00:05:50,000 --> 00:05:53,000
And, again, the deck will be available on the website.

103
00:05:53,000 --> 00:05:58,000
So the first thing that the work groups did is they selected the required

104
00:05:58,000 --> 00:05:59,000
components.

105
00:05:59,000 --> 00:06:02,000
So we started with the model openness framework,

106
00:06:02,000 --> 00:06:07,000
which was created by Matt White and colleagues at Linux Foundation.

107
00:06:07,000 --> 00:06:14,000
And you can see those components from that paper on the left,

108
00:06:14,000 --> 00:06:17,000
things like data pre-processing code, training code, evaluation code.

109
00:06:17,000 --> 00:06:22,000
And then there's also model and data components as well that aren't shown.

110
00:06:22,000 --> 00:06:27,000
And then we had work group members vote, initialed vote.

111
00:06:27,000 --> 00:06:33,000
So that's a public vote of is this component required to use or study or

112
00:06:33,000 --> 00:06:36,000
modify or share the system as a whole.

113
00:06:36,000 --> 00:06:40,000
And then I, and this is also public,

114
00:06:40,000 --> 00:06:47,000
created a Likert scale based on the number of votes per component.

115
00:06:47,000 --> 00:06:51,000
And we came up with certain components that were, yes,

116
00:06:51,000 --> 00:06:53,000
this definitely should be required down to no,

117
00:06:53,000 --> 00:06:56,000
there really aren't many votes saying that this should be required.

118
00:06:56,000 --> 00:07:02,000
And then that, that results of that Likert scale, the required, likely,

119
00:07:02,000 --> 00:07:04,000
maybe, probably not, not required,

120
00:07:04,000 --> 00:07:09,000
but into a public forum post for further feedback.

121
00:07:09,000 --> 00:07:14,000
And that became version 0.0.6 of the definition.

122
00:07:14,000 --> 00:07:18,000
So that is the first version of the definition where we actually said,

123
00:07:18,000 --> 00:07:21,000
these are the components that we think should be required for something to be

124
00:07:21,000 --> 00:07:23,000
open source.

125
00:07:23,000 --> 00:07:28,000
And what we just finished early spring was a second activity of the work

126
00:07:28,000 --> 00:07:29,000
groups,

127
00:07:29,000 --> 00:07:33,000
which is to look at the legal documents for these required components and see

128
00:07:33,000 --> 00:07:35,000
are the,

129
00:07:35,000 --> 00:07:39,000
are there legal documents in these systems that are associated with these

130
00:07:39,000 --> 00:07:41,000
components as described?

131
00:07:41,000 --> 00:07:46,000
So we had the required components, which are, there's code components.

132
00:07:46,000 --> 00:07:49,000
As you can see a couple of model components and then a number of data

133
00:07:49,000 --> 00:07:52,000
information or in this earlier version,

134
00:07:52,000 --> 00:07:56,000
data documentation components.

135
00:07:56,000 --> 00:08:01,000
And then we asked volunteers to find links to the documents or licenses that

136
00:08:01,000 --> 00:08:05,000
reference the rights to access and use those components.

137
00:08:05,000 --> 00:08:08,000
And then to evaluate, to look through the document and say,

138
00:08:08,000 --> 00:08:14,000
is use restricted or allowed is modification restricted or allowed is sharing

139
00:08:14,000 --> 00:08:20,000
restricted or allowed for all these different components.

140
00:08:20,000 --> 00:08:23,000
And these are the, so, and this is just a zoom in on the result,

141
00:08:23,000 --> 00:08:27,000
which is 0.0.8. As you saw in the beginning, we have,

142
00:08:27,000 --> 00:08:31,000
we are calling it data information now.

143
00:08:31,000 --> 00:08:34,000
And so data sets are not required,

144
00:08:34,000 --> 00:08:38,000
but information about training and methodologies, scope,

145
00:08:38,000 --> 00:08:42,000
and characteristics provenance labeling procedures,

146
00:08:42,000 --> 00:08:47,000
if used and cleaning methodology is required.

147
00:08:47,000 --> 00:08:51,000
And then we have code components, data, pre-processing code,

148
00:08:51,000 --> 00:08:55,000
training, validation, and testing inference and supporting libraries and

149
00:08:55,000 --> 00:09:00,000
tools. And then two model components, architecture and parameters.

150
00:09:00,000 --> 00:09:01,000
And you can see on the right,

151
00:09:01,000 --> 00:09:07,000
the legal frameworks that we're using for each and then a longer list of

152
00:09:07,000 --> 00:09:08,000
optional components.

153
00:09:08,000 --> 00:09:13,000
This is basically the remainder of the model openness framework components

154
00:09:13,000 --> 00:09:19,000
because obviously we want as many components as possible to be open,

155
00:09:19,000 --> 00:09:24,000
but these are not required. So then, so basically all these data sets,

156
00:09:24,000 --> 00:09:27,000
we say we would love if they're available,

157
00:09:27,000 --> 00:09:31,000
but they are not required to for the system to be called open according to

158
00:09:31,000 --> 00:09:34,000
our definition, additional code elements model.

159
00:09:34,000 --> 00:09:36,000
I see for some reason, I didn't put a highlight on model,

160
00:09:36,000 --> 00:09:39,000
but you can see it in there and other, which is for example,

161
00:09:39,000 --> 00:09:43,000
a research paper or a technical report.

162
00:09:43,000 --> 00:09:45,000
And then just again,

163
00:09:45,000 --> 00:09:49,000
that it's very important that we have a representative process.

164
00:09:49,000 --> 00:09:54,000
It's a global definition. And so this requires global consultation and we

165
00:09:54,000 --> 00:10:00,000
have various stakeholder groups that we're,

166
00:10:00,000 --> 00:10:02,000
that as part of our outreach,

167
00:10:02,000 --> 00:10:06,000
particularly as we move into the next phases of the project and I can go back

168
00:10:06,000 --> 00:10:11,000
to this if people are interested in it,

169
00:10:11,000 --> 00:10:15,000
most involved in the current phase are system creators and licensed creators

170
00:10:15,000 --> 00:10:22,000
and licensees. So people seeking to study, use, modify, and share the system.

171
00:10:22,000 --> 00:10:29,000
And then to increase, I guess, identity-based diversity.

172
00:10:29,000 --> 00:10:35,000
We, I'm using phrases like this one that are specifically inviting,

173
00:10:35,000 --> 00:10:38,000
for example, Black, Indigenous, Latina, other people of color, women,

174
00:10:38,000 --> 00:10:41,000
you can read that paragraph.

175
00:10:41,000 --> 00:10:43,000
And then we also do outreach,

176
00:10:43,000 --> 00:10:49,000
specific outreach to bring underrepresented groups into the process.

177
00:10:49,000 --> 00:10:52,000
So next steps, spring through fall.

178
00:10:52,000 --> 00:10:57,000
So now through October, basically. Right now we're doing definition validation.

179
00:10:57,000 --> 00:11:02,000
So we are seeking volunteers to review. It says one to three.

180
00:11:02,000 --> 00:11:06,000
This is not accurate anymore because now we've decided that we want to do about

181
00:11:06,000 --> 00:11:10,000
10 systems total. So it's actually about six additional systems.

182
00:11:10,000 --> 00:11:17,000
Using that same procedure as of the spreadsheet.

183
00:11:17,000 --> 00:11:22,000
So finding the legal document and doing the analysis of each component,

184
00:11:22,000 --> 00:11:25,000
according to these definitions of study, use, modify, and share.

185
00:11:25,000 --> 00:11:29,000
And we're hoping to have that complete by the 20th.

186
00:11:29,000 --> 00:11:32,000
So in about two and a half weeks.

187
00:11:32,000 --> 00:11:37,000
And these are the systems that we're currently looking for to review.

188
00:11:37,000 --> 00:11:42,000
And we have at least one volunteer for everything except Arctic,

189
00:11:42,000 --> 00:11:45,000
Snowflake Arctic, which is a new addition.

190
00:11:45,000 --> 00:11:50,000
But additional volunteers are welcome on all systems. It's a big task.

191
00:11:50,000 --> 00:11:56,000
So I'm sure that Casey and Mark and Victor and Jan and Racine would love to

192
00:11:56,000 --> 00:12:02,000
have a pal to share that review task with.

193
00:12:02,000 --> 00:12:05,000
And you can email me if you're interested in that,

194
00:12:05,000 --> 00:12:09,000
being a volunteer on any of those systems.

195
00:12:09,000 --> 00:12:16,000
Again, to test the definition against the documentation available for these

196
00:12:16,000 --> 00:12:20,000
systems, the legal documents and licenses.

197
00:12:20,000 --> 00:12:26,000
And our timeline for the rest of the year, we did just release 0.0.8.

198
00:12:26,000 --> 00:12:31,000
We may do a 0.0.9, depending on if there are changes that come back,

199
00:12:31,000 --> 00:12:36,000
major changes that come back from this definition validation activity we're

200
00:12:36,000 --> 00:12:38,000
currently engaged in.

201
00:12:38,000 --> 00:12:43,000
We will have a virtual launch event associated with the release of RC1 in

202
00:12:43,000 --> 00:12:46,000
June, and that date will be TBD.

203
00:12:46,000 --> 00:12:50,000
We had been thinking of doing something in person, and then we thought

204
00:12:50,000 --> 00:12:53,000
inclusion, this is a very important event.

205
00:12:53,000 --> 00:12:56,000
We want as many people to be able to participate as possible.

206
00:12:56,000 --> 00:12:58,000
So we decided to do virtual.

207
00:12:58,000 --> 00:13:02,000
And then we will have a stable version released in October,

208
00:13:02,000 --> 00:13:03,000
and that will be in person.

209
00:13:03,000 --> 00:13:08,000
That will be at All Things Open in North Carolina.

210
00:13:08,000 --> 00:13:12,000
And we have additional virtual and in-person meetings where we'll be

211
00:13:12,000 --> 00:13:18,000
sharing and seeking feedback on, I guess, RC1, release candidate 1.

212
00:13:18,000 --> 00:13:22,000
You can see where we'll be.

213
00:13:22,000 --> 00:13:25,000
And some events, you can see we have the month but not the date.

214
00:13:25,000 --> 00:13:32,000
So we're still working on certain details of this road show.

215
00:13:32,000 --> 00:13:38,000
So, yes, we would love for you to be on the public forum discussed at

216
00:13:38,000 --> 00:13:41,000
opensource.org, and I think that's the QR code.

217
00:13:41,000 --> 00:13:45,000
We'll take you to that site, which you can join for free or become a

218
00:13:45,000 --> 00:13:46,000
paid member.

219
00:13:46,000 --> 00:13:50,000
And we have these biweekly town halls, which you're at right now, so you

220
00:13:50,000 --> 00:13:51,000
know about those.

221
00:13:51,000 --> 00:13:55,000
And then if you're interested in volunteering for definition validation,

222
00:13:55,000 --> 00:14:00,000
you can email me or you can direct message me on the -- on that discussed

223
00:14:00,000 --> 00:14:01,000
platform.

224
00:14:01,000 --> 00:14:03,000
So thank you.

225
00:14:03,000 --> 00:14:10,000
And, yes, Stefano, do you have anything to say before we do the Q&A?

226
00:14:10,000 --> 00:14:12,000
>> I don't know.

227
00:14:12,000 --> 00:14:14,000
Maybe we can share the news.

228
00:14:14,000 --> 00:14:16,000
Shall we?

229
00:14:16,000 --> 00:14:18,000
>> Oh, yes, Stefano.

230
00:14:18,000 --> 00:14:23,000
>> Well, you know, we got -- we have received a grant from the Sloan

231
00:14:23,000 --> 00:14:25,000
Foundation.

232
00:14:25,000 --> 00:14:30,000
So we'll be doing a lot of the -- we're well positioned to have a lot of

233
00:14:30,000 --> 00:14:35,000
participation, a lot of those travel, a lot of those trips to those events

234
00:14:35,000 --> 00:14:37,000
will be supported by this program.

235
00:14:37,000 --> 00:14:40,000
Maybe you can go back.

236
00:14:40,000 --> 00:14:43,000
I had something also I wanted to mention.

237
00:14:43,000 --> 00:14:46,000
Go to slide 28.

238
00:14:46,000 --> 00:14:49,000
I don't know why I remembered this in my head.

239
00:14:49,000 --> 00:14:54,000
I said -- mental note that I wanted to say something here.

240
00:14:54,000 --> 00:14:56,000
Oh, yes.

241
00:14:56,000 --> 00:15:01,000
So here you can see how the legal frameworks in the column legal

242
00:15:01,000 --> 00:15:06,000
frameworks, we have the legal frameworks, the legal frameworks, the

243
00:15:06,000 --> 00:15:12,000
legal frameworks, we have -- we talk about data information available

244
00:15:12,000 --> 00:15:15,000
under OSD compliant license.

245
00:15:15,000 --> 00:15:23,000
And when you look at the code, like inference or data preprocessing, you

246
00:15:23,000 --> 00:15:28,000
see that the legal framework is available under OSI approved license.

247
00:15:28,000 --> 00:15:33,000
And then if you look at the model -- oh, no, yes.

248
00:15:33,000 --> 00:15:37,000
Parameters -- oh, it's cut.

249
00:15:37,000 --> 00:15:48,000
Available under OSD -- parameter says OSD conformant terms.

250
00:15:48,000 --> 00:15:57,000
I think it's interesting here because code, we know that is licensed -- I

251
00:15:57,000 --> 00:16:00,000
mean, we know the licenses, we know the legal frameworks, we know it's

252
00:16:00,000 --> 00:16:04,000
copyright mostly, there is some patent issue, but we've been using OSI

253
00:16:04,000 --> 00:16:06,000
approved license for a long time.

254
00:16:06,000 --> 00:16:10,000
So straightforward, we don't have any problem.

255
00:16:10,000 --> 00:16:18,000
Judging whether the code component are suitable, are available under the

256
00:16:18,000 --> 00:16:20,000
open source principles.

257
00:16:20,000 --> 00:16:26,000
For data information, which is mostly documentation, it's a little bit

258
00:16:26,000 --> 00:16:34,000
fuzzy, but still we can debate -- we know we can identify -- there is no

259
00:16:34,000 --> 00:16:36,000
definition of open documentation.

260
00:16:36,000 --> 00:16:43,000
But I think we can easily identify licenses that give us the possibility

261
00:16:43,000 --> 00:16:49,000
to read the documentation, modify, and redistribute to others.

262
00:16:49,000 --> 00:16:56,000
So that's why we're using the term OSD compliant license.

263
00:16:56,000 --> 00:17:01,000
Although there is a question whether we should use OSD compliant or OSD

264
00:17:01,000 --> 00:17:03,000
compatible.

265
00:17:03,000 --> 00:17:08,000
So compatible with the open source definition or compliant with the open

266
00:17:08,000 --> 00:17:10,000
source definition.

267
00:17:10,000 --> 00:17:13,000
So it's a little bit of a debate and it's ongoing on the forum.

268
00:17:13,000 --> 00:17:18,000
I encourage you to go find the thread and vote.

269
00:17:18,000 --> 00:17:21,000
There is actually a little poll in there.

270
00:17:21,000 --> 00:17:27,000
But the model parameters is interesting because model parameters don't seem

271
00:17:27,000 --> 00:17:30,000
to fall under copyright law.

272
00:17:30,000 --> 00:17:38,000
And there is discussion whether any other exclusive rights apply in

273
00:17:38,000 --> 00:17:40,000
different jurisdictions.

274
00:17:40,000 --> 00:17:47,000
So it's different between like Europe, United States, China, UK.

275
00:17:47,000 --> 00:17:50,000
There is still a little bit of a debate.

276
00:17:50,000 --> 00:17:55,000
So that's why we're using a more generic term that says OSD conformant.

277
00:17:55,000 --> 00:18:00,000
So we want to have -- leave the flexibility to interpret how the

278
00:18:00,000 --> 00:18:04,000
principles or how these parameters are distributed.

279
00:18:04,000 --> 00:18:09,000
Until the dust settles on the legal point of view.

280
00:18:09,000 --> 00:18:14,000
These are the questions -- these are the points that I think will have

281
00:18:14,000 --> 00:18:17,000
to be clarified in the next few weeks during the validation process

282
00:18:17,000 --> 00:18:20,000
among other things.

283
00:18:20,000 --> 00:18:24,000
So if you have opinions, if you know someone who might have opinions,

284
00:18:24,000 --> 00:18:28,000
now is the time to go and check the document.

285
00:18:28,000 --> 00:18:33,000
Because the definition, the draft 08, 008 is feature complete.

286
00:18:33,000 --> 00:18:35,000
It has all the elements that we want.

287
00:18:35,000 --> 00:18:40,000
And if no one really objects or if there are no strong pushback, I think

288
00:18:40,000 --> 00:18:45,000
it's going to be -- the release candidate is going to look very similar

289
00:18:45,000 --> 00:18:48,000
to what we have now.

290
00:18:48,000 --> 00:18:56,000
So any questions, I'm happy to -- me and Mer here are happy to answer.

291
00:18:56,000 --> 00:18:58,000
>> A question on the GDPR.

292
00:18:58,000 --> 00:19:00,000
I can read it.

293
00:19:00,000 --> 00:19:03,000
Shall I read it for the recording or Jan, do you want to come off mute?

294
00:19:03,000 --> 00:19:06,000
>> Yeah, go ahead.

295
00:19:06,000 --> 00:19:08,000
>> I can unmute.

296
00:19:08,000 --> 00:19:12,000
So there's been questions about like when it's -- a model is generating

297
00:19:12,000 --> 00:19:18,000
results that someone is saying, oh, this isn't -- I want to remove this

298
00:19:18,000 --> 00:19:20,000
from this.

299
00:19:20,000 --> 00:19:23,000
From your results according to GDPR.

300
00:19:23,000 --> 00:19:28,000
Wouldn't it be very nice to also have a possibility to do it?

301
00:19:28,000 --> 00:19:32,000
Because if you just have the model, you cannot really remove it.

302
00:19:32,000 --> 00:19:36,000
Because you have to remove it from the training data and then retrain to

303
00:19:36,000 --> 00:19:38,000
get a new thing.

304
00:19:38,000 --> 00:19:40,000
>> Yeah.

305
00:19:40,000 --> 00:19:42,000
I -- yes.

306
00:19:42,000 --> 00:19:46,000
But that is such a big question in fact.

307
00:19:46,000 --> 00:19:55,000
And I'm really curious to see what the results of the lawsuit that just

308
00:19:55,000 --> 00:19:58,000
happened against open AI.

309
00:19:58,000 --> 00:20:02,000
Granted that is a closed, opaque, we don't know what's happening inside

310
00:20:02,000 --> 00:20:04,000
there.

311
00:20:04,000 --> 00:20:09,000
The fact that the source data is not required is because it's exactly for

312
00:20:09,000 --> 00:20:11,000
this reason.

313
00:20:11,000 --> 00:20:15,000
So if there is private information in the source data sets, those -- that

314
00:20:15,000 --> 00:20:17,000
data set cannot be distributed legally.

315
00:20:17,000 --> 00:20:21,000
So that's why we want to know what's going in there.

316
00:20:21,000 --> 00:20:26,000
You know, we went into the training data set.

317
00:20:26,000 --> 00:20:28,000
We want to know exactly how it was filtered.

318
00:20:28,000 --> 00:20:31,000
You know, the duplication, all that thing.

319
00:20:31,000 --> 00:20:35,000
That's why there is a requirement on data preprocessing in the code

320
00:20:35,000 --> 00:20:37,000
section.

321
00:20:37,000 --> 00:20:39,000
Because that's what it means.

322
00:20:39,000 --> 00:20:43,000
You should -- you know, sometimes it's very valuable also to know exactly

323
00:20:43,000 --> 00:20:50,000
how to have the exact same instruments that went into the training.

324
00:20:50,000 --> 00:20:57,000
Building the data set for the training so that one can retrain the model

325
00:20:57,000 --> 00:21:01,000
and/or readjust the parameters with their own data.

326
00:21:01,000 --> 00:21:03,000
Or similar data.

327
00:21:03,000 --> 00:21:10,000
In fact, the exact spelling of the data information requirement is -- says

328
00:21:10,000 --> 00:21:16,000
it needs to be sufficiently detailed information about the data used to

329
00:21:16,000 --> 00:21:22,000
train the system so that a skilled person can recreate a substantially

330
00:21:22,000 --> 00:21:27,000
equivalent system using the same or similar data.

331
00:21:27,000 --> 00:21:30,000
We hope that this solves the problem.

332
00:21:30,000 --> 00:21:35,000
And if -- honestly, yes, if there is GDPR data inside the data set and the

333
00:21:35,000 --> 00:21:42,000
model itself has it, then you should be able to rebuild the -- with your own

334
00:21:42,000 --> 00:21:47,000
data without it.

335
00:21:47,000 --> 00:21:51,000
And maybe that model itself is illegal, right?

336
00:21:51,000 --> 00:21:53,000
In some legislation.

337
00:21:53,000 --> 00:22:01,000
So there is a higher standard that applies here.

338
00:22:01,000 --> 00:22:03,000
>> Thanks.

339
00:22:03,000 --> 00:22:05,000
Any other questions?

340
00:22:05,000 --> 00:22:10,000
You can chat or take yourself off mute.

341
00:22:10,000 --> 00:22:37,000
If anyone has any other questions or thoughts.

342
00:22:37,000 --> 00:22:41,000
>> I can come back on and just ask a little bit of a background question

343
00:22:41,000 --> 00:22:44,000
here because I missed a couple of town halls.

344
00:22:44,000 --> 00:22:56,000
So what is the idea of not requiring the training data in full and just have

345
00:22:56,000 --> 00:22:58,000
the opaque model?

346
00:22:58,000 --> 00:23:04,000
Because that sort of seems to me to be counterintuitive to the spirit of the

347
00:23:04,000 --> 00:23:07,000
open source definition.

348
00:23:07,000 --> 00:23:12,000
And I understand for the legality, but one can also say, well, all open

349
00:23:12,000 --> 00:23:16,000
source models should be legal.

350
00:23:16,000 --> 00:23:20,000
Is that the only thing or is there something more as well?

351
00:23:20,000 --> 00:23:24,000
>> No, I think there is something more.

352
00:23:24,000 --> 00:23:33,000
So many -- the fact that data set did not make it into the draft 06, was it?

353
00:23:33,000 --> 00:23:41,000
Is because the working group as they were evaluating the various systems,

354
00:23:41,000 --> 00:23:47,000
not enough of them put the requirement of the original data set as strictly

355
00:23:47,000 --> 00:23:52,000
required to modify, study, use, and share.

356
00:23:52,000 --> 00:23:59,000
So they ranked higher other -- like the training information.

357
00:23:59,000 --> 00:24:09,000
Like the training code, the documentation on the data used were ranked

358
00:24:09,000 --> 00:24:10,000
higher.

359
00:24:10,000 --> 00:24:16,000
And so because there are so many machine learning systems that don't have

360
00:24:16,000 --> 00:24:24,000
the original data set, like it just doesn't exist because it's not created.

361
00:24:24,000 --> 00:24:33,000
Like for anything that is used with -- learning and other ways that are

362
00:24:33,000 --> 00:24:35,000
privacy preserving.

363
00:24:35,000 --> 00:24:39,000
Then there is that question of private data.

364
00:24:39,000 --> 00:24:47,000
Where it might go -- I mean, it may be part -- I mean, technically part of

365
00:24:47,000 --> 00:24:53,000
the -- good part of the originating data set.

366
00:24:53,000 --> 00:25:03,000
But can be obfuscated or hidden from the model itself.

367
00:25:03,000 --> 00:25:06,000
Like we were concerned -- we are concerned.

368
00:25:06,000 --> 00:25:12,000
I mean, all of us are concerned about generating -- putting the bar so high

369
00:25:12,000 --> 00:25:14,000
that there is no incentive.

370
00:25:14,000 --> 00:25:18,000
Not only there is no incentive into releasing open source, but there is

371
00:25:18,000 --> 00:25:23,000
actually a very strong disincentive into the open source AI.

372
00:25:23,000 --> 00:25:32,000
And in fact, if you look at the amount of lawsuits that any of the more

373
00:25:32,000 --> 00:25:37,000
open foundation models that have been released, they're receiving just

374
00:25:37,000 --> 00:25:43,000
because they've been transparently saying, hey, we have scour the web and

375
00:25:43,000 --> 00:25:47,000
incorporated pretty much anything that we could find.

376
00:25:47,000 --> 00:25:52,000
That, you know, give us -- while on the other side of the spectrum, we

377
00:25:52,000 --> 00:26:00,000
have things like Lama 3 or open AI, they don't say what's inside the

378
00:26:00,000 --> 00:26:04,000
data sets, what went into their training sets, because they don't want to

379
00:26:04,000 --> 00:26:06,000
be sued.

380
00:26:06,000 --> 00:26:10,000
So we're trying to see if there is a balance to be found.

381
00:26:10,000 --> 00:26:14,000
And let's -- the validation is also for this.

382
00:26:14,000 --> 00:26:23,000
Like do we have a set of models out there that already exist that can

383
00:26:23,000 --> 00:26:27,000
fit into the open source AI definition and we can actually work with

384
00:26:27,000 --> 00:26:29,000
them?

385
00:26:29,000 --> 00:26:36,000
Yeah, we want to be pragmatic, but we also want to maintain the

386
00:26:36,000 --> 00:26:38,000
principles right.

387
00:26:38,000 --> 00:26:44,000
The principles that we want to give users freedom, agency, control

388
00:26:44,000 --> 00:26:48,000
over the technical choices doesn't necessarily mean that we want to

389
00:26:48,000 --> 00:26:52,000
have the full spectrum of everything always open all the time.

390
00:26:52,000 --> 00:26:59,000
Because it may be that there is enough elements even without having

391
00:26:59,000 --> 00:27:01,000
the complete spectrum.

392
00:27:01,000 --> 00:27:06,000
If you have looked at the model openness framework paper, it's an

393
00:27:06,000 --> 00:27:11,000
interesting read because it has -- lists all of those components and

394
00:27:11,000 --> 00:27:21,000
then gives a sliding scale of what's required for the -- to be included

395
00:27:21,000 --> 00:27:23,000
in the Linux foundation projects.

396
00:27:23,000 --> 00:27:28,000
And it scales from basically just give us the model parameters all the

397
00:27:28,000 --> 00:27:30,000
way to give us everything.

398
00:27:30,000 --> 00:27:35,000
And they call it open models, open tooling, when some more extra

399
00:27:35,000 --> 00:27:37,000
pieces are available.

400
00:27:37,000 --> 00:27:39,000
In open science where there is everything.

401
00:27:39,000 --> 00:27:43,000
So we're trying to find a balance in there where there is a bar where

402
00:27:43,000 --> 00:27:47,000
we can say, okay, this is open source AI, but -- and then everything

403
00:27:47,000 --> 00:27:49,000
else goes on top.

404
00:27:49,000 --> 00:27:51,000
Yeah.

405
00:27:51,000 --> 00:27:53,000
All right.

406
00:27:53,000 --> 00:27:56,000
So the transparency is a feature of open source.

407
00:27:56,000 --> 00:27:58,000
>> I just want to pause.

408
00:27:58,000 --> 00:28:00,000
I just want to pause.

409
00:28:00,000 --> 00:28:03,000
Because we can continue this conversation for a long time.

410
00:28:03,000 --> 00:28:05,000
I just want to pause.

411
00:28:05,000 --> 00:28:09,000
Is there anyone else on the call who has a different kind of question?

412
00:28:09,000 --> 00:28:14,000
And if not, we can continue this conversation for a bit of time.

413
00:28:14,000 --> 00:28:16,000
Okay.

414
00:28:16,000 --> 00:28:18,000
No.

415
00:28:18,000 --> 00:28:20,000
All right.

416
00:28:20,000 --> 00:28:22,000
So, yeah.

417
00:28:22,000 --> 00:28:24,000
So we can continue, I don't know, to the half hour maybe talking about

418
00:28:24,000 --> 00:28:26,000
this topic?

419
00:28:26,000 --> 00:28:28,000
>> Yeah.

420
00:28:28,000 --> 00:28:32,000
I just want to also announce the fact that we're going to be talking

421
00:28:32,000 --> 00:28:34,000
about data in a separate talk.

422
00:28:34,000 --> 00:28:39,000
Like there is absolutely a need to better understand the space also

423
00:28:39,000 --> 00:28:41,000
as data becomes functional.

424
00:28:41,000 --> 00:28:43,000
Which is a new thing.

425
00:28:43,000 --> 00:28:45,000
Fairly new thing.

426
00:28:45,000 --> 00:28:47,000
And we want to understand it a little bit better.

427
00:28:47,000 --> 00:28:51,000
One of the stops in September is in France.

428
00:28:51,000 --> 00:28:55,000
And we're working on a workshop specifically to start the

429
00:28:55,000 --> 00:28:59,000
conversation that I'm quite sure is going to take a little bit longer.

430
00:28:59,000 --> 00:29:01,000
Because transparency is the feature.

431
00:29:01,000 --> 00:29:04,000
I want to highlight that.

432
00:29:04,000 --> 00:29:08,000
That's why we're insisting on having data information, provenance

433
00:29:08,000 --> 00:29:10,000
and all of that.

434
00:29:10,000 --> 00:29:13,000
And the code for the training.

435
00:29:13,000 --> 00:29:16,000
The code used for the creation of that data set.

436
00:29:16,000 --> 00:29:20,000
I think it's going to be -- it's a good compromise.

437
00:29:20,000 --> 00:29:28,000
But let's see what else exists in there.

438
00:29:28,000 --> 00:29:30,000
>> I see Claire, you have a comment.

439
00:29:30,000 --> 00:29:33,000
Do you want to come off mute, Claire?

440
00:29:33,000 --> 00:29:35,000
>> Sure.

441
00:29:35,000 --> 00:29:37,000
Thank you.

442
00:29:37,000 --> 00:29:42,000
So this is just a question to see if there is any current effort

443
00:29:42,000 --> 00:29:46,000
going into keeping track of any other definitions of open source

444
00:29:46,000 --> 00:29:48,000
AI.

445
00:29:48,000 --> 00:29:51,000
And specifically thinking about anything that might be referenced

446
00:29:51,000 --> 00:29:55,000
in any emerging regulation or policies that might be coming at a

447
00:29:55,000 --> 00:29:57,000
government level.

448
00:29:57,000 --> 00:30:01,000
Knowing that I think it was referenced in the AI act, but I'm

449
00:30:01,000 --> 00:30:05,000
not sure how it was referenced or what they defined it as in that

450
00:30:05,000 --> 00:30:07,000
act.

451
00:30:07,000 --> 00:30:09,000
>> Yeah.

452
00:30:09,000 --> 00:30:11,000
So thanks for the question.

453
00:30:11,000 --> 00:30:15,000
Because the AI act mentions multiple times, a couple of times

454
00:30:15,000 --> 00:30:19,000
that it's a free and open source AI system without providing any

455
00:30:19,000 --> 00:30:21,000
explanation of what that means.

456
00:30:21,000 --> 00:30:26,000
So it was one of the triggers to push for this project to start

457
00:30:26,000 --> 00:30:31,000
two years ago when we saw -- when I saw the first draft of the

458
00:30:31,000 --> 00:30:36,000
AI act, I was like, oh, we need to have -- we need to help

459
00:30:36,000 --> 00:30:38,000
regulators understand this space.

460
00:30:38,000 --> 00:30:42,000
And we're having fairly good, intense conversations also with

461
00:30:42,000 --> 00:30:48,000
the American agencies which are now under pressure to come up

462
00:30:48,000 --> 00:30:52,000
with regulations inside to -- right.

463
00:30:52,000 --> 00:30:55,000
To control a little bit this market.

464
00:30:55,000 --> 00:30:59,000
As it comes out, there are so many foundation models that it

465
00:30:59,000 --> 00:31:03,000
talks about risks and they all want to know what open source

466
00:31:03,000 --> 00:31:07,000
means, what open means in this space and what the implications

467
00:31:07,000 --> 00:31:11,000
are for public in general, for public interest.

468
00:31:11,000 --> 00:31:13,000
>> Thank you.

469
00:31:13,000 --> 00:31:19,000
>> Maybe one final thought or comment and then we'll close the

470
00:31:19,000 --> 00:31:21,000
session.

471
00:31:21,000 --> 00:31:23,000
I saw you typing.

472
00:31:23,000 --> 00:31:30,000
If you want to have a last thought, you're more than welcome

473
00:31:30,000 --> 00:31:32,000
to share it.

474
00:31:32,000 --> 00:31:46,000
>> Maybe I see types.

475
00:31:46,000 --> 00:31:51,000
I can say watch the space, watch our blog, too, because we're

476
00:31:51,000 --> 00:31:55,000
going to be continuing the conversations around data.

477
00:31:55,000 --> 00:32:00,000
I think it's a really important space where we don't have a lot

478
00:32:00,000 --> 00:32:02,000
of practice.

479
00:32:02,000 --> 00:32:07,000
There are new legal questions that are being raised like what

480
00:32:07,000 --> 00:32:11,000
is exactly right, acceptable when it comes to text and data

481
00:32:11,000 --> 00:32:13,000
mining.

482
00:32:13,000 --> 00:32:16,000
New regulation around text and data mining specifically

483
00:32:16,000 --> 00:32:22,000
appearing around the world like Japan has an approach that is

484
00:32:22,000 --> 00:32:28,000
very -- Europe has introduced it as a new right and it's still

485
00:32:28,000 --> 00:32:32,000
having -- there are some limitations to it, though.

486
00:32:32,000 --> 00:32:37,000
So policies are going to be written and all the lawsuits in

487
00:32:37,000 --> 00:32:41,000
the United States which are very interesting and we're waiting

488
00:32:41,000 --> 00:32:43,000
for them to be clarified.

489
00:32:43,000 --> 00:32:48,000
Yes, so Claire, the outreach plan is vast.

490
00:32:48,000 --> 00:32:53,000
Like we've scrolled through with Mayor, but yes, we are reaching

491
00:32:53,000 --> 00:33:00,000
out to a lot of the AI communities, startups, developers,

492
00:33:00,000 --> 00:33:04,000
conferences like new rips and others.

493
00:33:04,000 --> 00:33:06,000
Yeah.

494
00:33:06,000 --> 00:33:12,000
Granted, it's going to -- remember what I like to remind

495
00:33:12,000 --> 00:33:17,000
everyone, that the open source definition came out after at

496
00:33:17,000 --> 00:33:22,000
least 15 years of experience in the free software world and when

497
00:33:22,000 --> 00:33:28,000
the developers were few, computers were not as ubiquitous as

498
00:33:28,000 --> 00:33:34,000
they are now, and it took a while to become so well-known and

499
00:33:34,000 --> 00:33:41,000
widely respected, so we'll have to be doing this work of open

500
00:33:41,000 --> 00:33:43,000
source AI.

501
00:33:43,000 --> 00:33:45,000
We're defining it in a few months.

502
00:33:45,000 --> 00:33:49,000
So we're going to have to do a lot of work after -- continue to

503
00:33:49,000 --> 00:33:55,000
do a lot of this outreach work in the next years.

504
00:33:55,000 --> 00:34:00,000
Yes, if you're coming to PyCon, you'll find us there.

505
00:34:00,000 --> 00:34:05,000
That's our next stop.

506
00:34:05,000 --> 00:34:07,000
All right.

507
00:34:07,000 --> 00:34:08,000
Thanks, everyone.

508
00:34:08,000 --> 00:34:10,000
We host this every two weeks.

509
00:34:10,000 --> 00:34:14,000
I think that next week I'll be at PyCon, so we will not be able

510
00:34:14,000 --> 00:34:16,000
to have this.

511
00:34:16,000 --> 00:34:20,000
But we'll meet again in two weeks and the forums are still

512
00:34:20,000 --> 00:34:22,000
going to be active and available.

513
00:34:22,000 --> 00:34:24,000
Thanks, everyone.

### End of last town hall held on 2024-05-03 ###

### Start of next town hall held on 2024-05-31 ###
--- Presentation for 2024-05-31 ---
OPEN SOURCE AI DEFINITION
Online public townhall
May 31, 2024
last updated: May 28, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

OSI’s objective for 2024

Open Source AI Definition

3

Open Source AI Deﬁnition

Where Are We Now?

4

Open
Source AI
Definition

Preamble
4 Freedoms

v.0.0.8

Legal Checklist

-

Open
Source AI
Definition

Preamble
4 Freedoms

v.0.0.8

Working on review
processes for
determining if an
AI system meets
the definition
requirements

Legal Checklist

-

Open Source AI Deﬁnition

How Did We Get Here?
May 2024

7

Validation Reviewers
1. Arctic
1.

Jesús M.
Gonzalez-Barahona
Universidad Rey Juan
Carlos

2. BLOOM*
2.
3.

Danish Contractor
BLOOM Model Gov.
Work Group
Jaan Li University of
Tartu, One Fact
Foundation

3. Falcon
1.
2.

Casey Valk Nutanix
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France

4. Grok
1.
2.

Victor Lu independent
database consultant
Karsten Wade Open
Community Architects

We’re interested in reviewing about 10 AI systems self-described as open as part of
this definition validation phase. Those marked (*) have were reviewed in previous
phases.

5. Llama 2*
1.
2.
3.
4.

Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute of
Paris
Victor Lu independent
database consultant

7. OLMo
1.
2.

Amanda Casari
Google
Abdoulaye Diack
Google

8. OpenCV*
1.

Rasim Sen Oasis
Software Technology
Ltd.

10. Pythia*
1.
2.

Seo-Young Isabelle
Hwang Samsung
Stella Biderman
EleutherAI

3.

Hailey Schoelkopf
EleutherAI

4.

Aviya Skowron
EleutherAI

6. Mistral
5.
6.
7.

Mark Collier
OpenInfra Foundation
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
Cailean Osborne
University of Oxford,
Linux Foundation

9. Phi-2
3.

Seo-Young Isabelle
Hwang Samsung

11. T5
5.

Jaan Li University of
Tartu, One Fact
Foundation

Open Source AI Definition
Validation phase
v.0.0.8

Open Source AI Definition
Validation phase
v.0.0.8

It was hard for
volunteer reviewers
to find required
documents to do the
review.

Open Source AI Definition
Validation phase
v.0.0.8

This meant a lot of
the review analysis
was left incomplete

Open Source AI Deﬁnition

What’s Next?

June - October 2024

● Complete validation phase (June 10)
● Resolve comments, release v. 0.0.9 after
validation
● Cut the release candidate with sufficient
endorsement
12

Complete the Validation
Phase

Thanks,
LLM360
team!

1. Reach out to AI system creators to
ﬁll in the blanks on their own systems
by pointing us to correct documentation
2. Invite volunteers to also help us ﬁll in these
blanks (forum post forthcoming)
3. We’re also currently engaging in email and
phone debriefs with reviewers to better
understand the blockers they faced.
13

Seek Public Feedback on Initial Results

Please
comment on the
initial report of
our validation
process.

14

Simplify the Validation Process
We’re exploring
replacing the
spreadsheet with
an Evaluation
Card like this
prototype.

15

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Townhalls +

Townhall +

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7 and 8

PyCon
Workshop
May 17th,
Pittsburgh)

(≈

Draft 0.0.9

… October
Monthly Virtual
Meetings

Release stable
version

Virtual Launch
Event (date
TBD)

RC1

Stable
Version

In-Person Meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

✓ PyCon US

May 17

Europe

France

Paris

OW2

June 11-12

Africa

Virtual

Virtual

Sustain Africa

June

North America

United States

New York

OSPOs for Good

July 9 - 11

Asia Paciﬁc

China

Hong Kong

AI_dev

August 23

Latin America

Argentina

Buenos Aires

Nerdearla

September

Europe

France

Paris

(data governance)

October

North America

United States

Raleigh

All Things Open

Oct 27 - 29

17

The renewed discussion on data
1. The AWS Open Source team posted a
range of concerns with v 0.0.8, foremost on
data.
2. Linux Foundation team recommended
adding Data card to the required
components. Also they argued that Data
preprocessing code is unlikely to be shared
if the dataset is not shared, too.
18

Other relevant posts
1. The LLM360 team voluntarily ran their system
through the v.0.0.8 review process.
2. Stefano posted on whether and how OSI should
certify Open Source AI.

19

Participation Options
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
● Volunteer for to fill in the blanks on definition
validation (email or DM Mer or Stefano)
20

Q&A

21

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the
deﬁnitional process.

22


--- Subtitles for 2024-05-31 --- ###
1
00:00:00,001 --> 00:00:06,280
So, thank you.

2
00:00:06,280 --> 00:00:07,560
So hi everyone.

3
00:00:07,560 --> 00:00:12,920
My name is Mare and I'm leading the code of design process for the open source AI definition

4
00:00:12,920 --> 00:00:22,680
and Stefano is with me and this is the public town hall for May 31st.

5
00:00:22,680 --> 00:00:27,760
And those of you who have been here before will recognize our community agreements.

6
00:00:27,760 --> 00:00:30,920
I'll go through them very quickly.

7
00:00:30,920 --> 00:00:35,600
Basically one person speaks at a time.

8
00:00:35,600 --> 00:00:40,720
And we ask that you obviously mute yourself unless we're in the Q&A session.

9
00:00:40,720 --> 00:00:46,880
Take space, make space just means that if you tend to speak up, take a moment to pause

10
00:00:46,880 --> 00:00:52,680
to let others speak and if you do tend to be quiet, we also invite you to share your

11
00:00:52,680 --> 00:00:53,840
thoughts.

12
00:00:53,840 --> 00:00:59,760
And then also once we do get into Q&A, we can have whoever asks a question, there will

13
00:00:59,760 --> 00:01:04,000
be a response and then we'll say wait for another person to ask a question so we don't

14
00:01:04,000 --> 00:01:09,360
just have a back and forth with one speaker.

15
00:01:09,360 --> 00:01:14,120
Kindness this work is hard, but let's be gentle with each other.

16
00:01:14,120 --> 00:01:19,680
Forward motion, focus on what is possible in doing it because this is hard work.

17
00:01:19,680 --> 00:01:24,880
So we note obstacles and we come back to them, but we do what is possible in the moment.

18
00:01:24,880 --> 00:01:26,040
Likewise solution seeking.

19
00:01:26,040 --> 00:01:32,960
So saying something is not possible, yes, and it's very vulnerable to say, hey, how

20
00:01:32,960 --> 00:01:38,400
about this, but that is what we need to have as a mindset to get this work done.

21
00:01:38,400 --> 00:01:42,040
And are there any other community agreements that people would like to propose that are

22
00:01:42,040 --> 00:01:46,080
not here and you can just put them in the chat.

23
00:01:46,080 --> 00:01:49,080
Oh, hi, Anna.

24
00:01:49,080 --> 00:01:51,080
Hi, Anna.

25
00:01:51,080 --> 00:01:52,080
Okay.

26
00:01:52,080 --> 00:01:55,780
I'll just continue.

27
00:01:55,780 --> 00:02:00,080
So yes, so we are creating the open source AI definition.

28
00:02:00,080 --> 00:02:02,640
That's our project for this year.

29
00:02:02,640 --> 00:02:03,840
And where are we now?

30
00:02:03,840 --> 00:02:10,840
We're on version 0.0.8, which was released in April and the parts should be familiar.

31
00:02:10,840 --> 00:02:12,160
We have a preamble.

32
00:02:12,160 --> 00:02:17,280
We have the four freedoms, which we're not going to go into today, but they're used,

33
00:02:17,280 --> 00:02:21,960
we modify and share.

34
00:02:21,960 --> 00:02:29,280
And then we have the legal checklist, which is basically operationalizing the four freedoms

35
00:02:29,280 --> 00:02:35,680
by identifying which components are required, which was voted on by our community co-design

36
00:02:35,680 --> 00:02:37,040
process.

37
00:02:37,040 --> 00:02:43,920
And then what is the license or legal framework for each component.

38
00:02:43,920 --> 00:02:47,760
And Stefano, step in if you have any additions.

39
00:02:47,760 --> 00:02:56,080
What we're working on now is to review, is the review process for determining if an AI

40
00:02:56,080 --> 00:02:59,200
system meets the definition requirements.

41
00:02:59,200 --> 00:03:06,200
And yeah, we would like to review about 10 systems before we release, do release candidate

42
00:03:06,200 --> 00:03:11,120
one in June, which is coming up soon.

43
00:03:11,120 --> 00:03:13,040
A little bit of how did we get here?

44
00:03:13,040 --> 00:03:17,280
This is the recent past, not the story of the whole project as some previous town halls

45
00:03:17,280 --> 00:03:19,480
have shared.

46
00:03:19,480 --> 00:03:22,560
So these are the people who've been working on this.

47
00:03:22,560 --> 00:03:28,240
At this point, we have 11 systems that we're looking at, and this is listing the reviewers.

48
00:03:28,240 --> 00:03:38,920
We started with the asterisked systems, Bloom, Lama2, Pythia and OpenCV.

49
00:03:38,920 --> 00:03:40,800
And then we added on seven more.

50
00:03:40,800 --> 00:03:45,200
So you can see.

51
00:03:45,200 --> 00:03:52,760
And we had this spreadsheet-based validation process where component listed on the far

52
00:03:52,760 --> 00:03:59,960
left, then the legal framework from the definition, then the individual reviewers asked to find

53
00:03:59,960 --> 00:04:04,680
on the internet, which I'll get to later, the legal document as provided by the system

54
00:04:04,680 --> 00:04:10,560
creators, and then to look at that document and determine, does this document give the

55
00:04:10,560 --> 00:04:16,800
ability to study, use, modify, share that particular required component?

56
00:04:16,800 --> 00:04:20,320
And it was hard.

57
00:04:20,320 --> 00:04:25,840
It was in most cases not possible for volunteer reviewers to find the required documents necessary

58
00:04:25,840 --> 00:04:28,160
to do the review.

59
00:04:28,160 --> 00:04:32,800
And as a result, the analysis was also not possible.

60
00:04:32,800 --> 00:04:36,960
So let me just see what my next slide is.

61
00:04:36,960 --> 00:04:42,000
So what this means is that -- oh, there's a new slide here.

62
00:04:42,000 --> 00:04:43,200
I see.

63
00:04:43,200 --> 00:04:49,400
So what that means is that we are still needing to work on the validation, and we're needing

64
00:04:49,400 --> 00:04:52,600
to work with system creators.

65
00:04:52,600 --> 00:04:59,000
We need from system creators their identification of this is the document that you need, this

66
00:04:59,000 --> 00:05:02,480
is the license on this component of our system.

67
00:05:02,480 --> 00:05:07,880
We've just realized that that's -- it's a required component of even being able to test

68
00:05:07,880 --> 00:05:10,720
our own definition.

69
00:05:10,720 --> 00:05:16,960
And okay, we're going to complete this validation phase by the 10th and resolve comments and

70
00:05:16,960 --> 00:05:24,320
release a version 0.0.9 after the validation and cut a release candidate with sufficient

71
00:05:24,320 --> 00:05:25,320
endorsement.

72
00:05:25,320 --> 00:05:29,000
Stefano, did you want to say anything else on the slide?

73
00:05:29,000 --> 00:05:39,120
>> Well, maybe just add a little bit about the validation phase and how it's going.

74
00:05:39,120 --> 00:05:52,000
We realize that basically that checklist is complicated to -- for people, even for experts,

75
00:05:52,000 --> 00:06:02,560
computer experts, to find these components without the expertise of the actual -- the

76
00:06:02,560 --> 00:06:06,760
people who have created the systems is really complicated.

77
00:06:06,760 --> 00:06:14,480
So we really -- at this point, we really have to engage with system creators, the original

78
00:06:14,480 --> 00:06:28,920
creators of PHY, and LLAMA, LLAMA 2 and 3, and GROK, and Mestral, and PTI, et cetera,

79
00:06:28,920 --> 00:06:35,960
Falco, and ask them to provide the list of components.

80
00:06:35,960 --> 00:06:40,400
Or we need to find another way of validating.

81
00:06:40,400 --> 00:06:52,600
Because we had conversations with the Linux Foundation also, and they have a similar concern

82
00:06:52,600 --> 00:07:04,560
for the model openness framework, which we have -- we are reusing for the list of components.

83
00:07:04,560 --> 00:07:09,160
So it is complicated.

84
00:07:09,160 --> 00:07:14,360
But the intention here needs to be -- you know, I'd like to clarify.

85
00:07:14,360 --> 00:07:23,680
The intention here is to provide a definition that is general purpose, that we can apply

86
00:07:23,680 --> 00:07:30,120
to different technology, that can to some extent resist the test of time.

87
00:07:30,120 --> 00:07:39,960
This components piece is really targeted at the latest generation of transformers and

88
00:07:39,960 --> 00:07:42,680
large language models.

89
00:07:42,680 --> 00:07:49,000
The architectures that -- of the systems that we have here, neural networks, et cetera.

90
00:07:49,000 --> 00:07:57,320
So we're really trying to strike a balance between setting principles that are high level

91
00:07:57,320 --> 00:08:01,960
and valid for a longer term.

92
00:08:01,960 --> 00:08:11,840
And provide a checklist for the evaluation of the openness of these systems.

93
00:08:11,840 --> 00:08:12,840
So yeah.

94
00:08:12,840 --> 00:08:13,840
That's it.

95
00:08:13,840 --> 00:08:20,160
We're a little bit -- and I just posted a few minutes ago, probably an hour ago, on

96
00:08:20,160 --> 00:08:23,560
the forum, like a comment along the same lines.

97
00:08:23,560 --> 00:08:26,160
Of what I just said.

98
00:08:26,160 --> 00:08:30,600
There is one curiosity here that probably some people have been -- that I've heard people

99
00:08:30,600 --> 00:08:31,600
asking.

100
00:08:31,600 --> 00:08:35,720
Go back one slide, Nat, please.

101
00:08:35,720 --> 00:08:39,000
Where it says -- the column that says legal document.

102
00:08:39,000 --> 00:08:41,880
And why not call it license?

103
00:08:41,880 --> 00:08:52,240
And that's because the licenses are -- is a term that is really tied to the concept

104
00:08:52,240 --> 00:08:53,480
of copyright.

105
00:08:53,480 --> 00:08:57,440
So it works really for documentation and code.

106
00:08:57,440 --> 00:09:05,200
But for model parameters, the copyright is most likely not applicable.

107
00:09:05,200 --> 00:09:09,120
And the same also for data.

108
00:09:09,120 --> 00:09:12,120
Copyright is not necessarily applicable.

109
00:09:12,120 --> 00:09:18,140
So those are usually referred to as agreements in legal terms.

110
00:09:18,140 --> 00:09:23,080
So that's why we're not calling them -- legal document is more of a generic term that covers

111
00:09:23,080 --> 00:09:30,920
both licenses and agreements in terms of service and other names.

112
00:09:30,920 --> 00:09:33,560
>> So I'll just comment a little on Nick.

113
00:09:33,560 --> 00:09:37,280
And then Dan, I'm going to leave your question to the Q&A.

114
00:09:37,280 --> 00:09:42,880
So just to clarify, because this is a concern that people have of the idea that would system

115
00:09:42,880 --> 00:09:46,800
creators be evaluating their own systems as part of a formal process?

116
00:09:46,800 --> 00:09:47,800
No.

117
00:09:47,800 --> 00:09:54,360
I think it would be that system creators are providing documentation and then there are

118
00:09:54,360 --> 00:10:00,200
independent reviewers looking at that documentation and confirming, yes, this describes the component

119
00:10:00,200 --> 00:10:01,200
as required.

120
00:10:01,200 --> 00:10:03,320
I don't know, Stefano, if you have anything to add on that.

121
00:10:03,320 --> 00:10:08,040
But just this idea of independent review is still part of the process.

122
00:10:08,040 --> 00:10:09,040
Okay.

123
00:10:09,040 --> 00:10:11,520
All right.

124
00:10:11,520 --> 00:10:17,840
So yeah, this is just basically saying what Stefano was talking about.

125
00:10:17,840 --> 00:10:23,060
Reaching out to system creators, I would add that, yeah, we are also looking into collaborating

126
00:10:23,060 --> 00:10:27,160
with the Linux Foundation on this because we are using their component list.

127
00:10:27,160 --> 00:10:32,040
And they do have -- I guess I can't -- I'm not sure what I can announce.

128
00:10:32,040 --> 00:10:33,040
I know they have a launch.

129
00:10:33,040 --> 00:10:39,200
But they're also working on a solution to this documentation challenge.

130
00:10:39,200 --> 00:10:41,420
And so we're looking at how can we collaborate with them.

131
00:10:41,420 --> 00:10:47,080
So that we're not both going to system creators and both asking them for documents, but where

132
00:10:47,080 --> 00:10:58,280
we can be asking the system creators to funnel their documentation into the same location.

133
00:10:58,280 --> 00:11:02,760
This volunteers, I'm not sure that's going to work.

134
00:11:02,760 --> 00:11:07,720
But if volunteers do have this knowledge and can help us fill in the blanks, that's great.

135
00:11:07,720 --> 00:11:11,500
Yes, and we're learning from reviewers, which is basically what Stefano has talked about.

136
00:11:11,500 --> 00:11:15,300
This idea of needing the collaboration of creators, system creators is what we found

137
00:11:15,300 --> 00:11:18,300
from talking to reviewers.

138
00:11:18,300 --> 00:11:19,400
Yeah.

139
00:11:19,400 --> 00:11:26,260
So this is -- yes, there is a report that Stefano referenced, which is just basically

140
00:11:26,260 --> 00:11:29,920
in text talking through what I'm sharing with you now.

141
00:11:29,920 --> 00:11:33,700
And that's a QR code to the forum.

142
00:11:33,700 --> 00:11:38,980
And yeah, we've been thinking through what are ways to simplify the validation process.

143
00:11:38,980 --> 00:11:44,100
At this point, it seems like making the documentation for each component easy to find and review

144
00:11:44,100 --> 00:11:46,940
is probably the number one blocker.

145
00:11:46,940 --> 00:11:50,780
There may also be blockers related to format.

146
00:11:50,780 --> 00:11:58,040
And we're looking at the idea -- Stefano created this design, the idea of an evaluation card.

147
00:11:58,040 --> 00:12:01,740
Maybe that format would be easier to work with than a spreadsheet.

148
00:12:01,740 --> 00:12:06,740
But in any case, having the document to review is probably the number one need that we have

149
00:12:06,740 --> 00:12:07,740
right now.

150
00:12:07,740 --> 00:12:08,740
And Josh, I see your hand.

151
00:12:08,740 --> 00:12:16,100
And I will answer during the Q&A.

152
00:12:16,100 --> 00:12:18,180
And then just to have our timeline.

153
00:12:18,180 --> 00:12:23,260
So we are still looking to release RC1 next month.

154
00:12:23,260 --> 00:12:26,220
And then the stable version in October.

155
00:12:26,220 --> 00:12:27,420
Yeah.

156
00:12:27,420 --> 00:12:31,140
And also a virtual launch event, I guess, is still something we're planning for next

157
00:12:31,140 --> 00:12:32,140
month.

158
00:12:32,140 --> 00:12:33,140
RC1.

159
00:12:33,140 --> 00:12:34,980
We'll see how that goes.

160
00:12:34,980 --> 00:12:37,420
And then we have some in-person meetings.

161
00:12:37,420 --> 00:12:39,180
We were at PyCon.

162
00:12:39,180 --> 00:12:45,500
And Nick created a forum post about that, summaring what we're up to at PyCon.

163
00:12:45,500 --> 00:12:49,860
And yeah, throughout the summer, we have a roadshow going.

164
00:12:49,860 --> 00:12:53,260
And we'll have a data event in October.

165
00:12:53,260 --> 00:12:58,860
That's an issue that's come up throughout the process, is what the definition should

166
00:12:58,860 --> 00:13:03,020
be saying with regards to particularly training data.

167
00:13:03,020 --> 00:13:06,300
So yes, speaking of which.

168
00:13:06,300 --> 00:13:08,420
So yeah, so this is a huge issue.

169
00:13:08,420 --> 00:13:12,460
It's a very important issue in the definition.

170
00:13:12,460 --> 00:13:18,540
And the AWS, Amazon Web Services, open source team posted a range of different concerns

171
00:13:18,540 --> 00:13:22,140
with our current version on the forum.

172
00:13:22,140 --> 00:13:32,140
And the Linux team, primarily the issue of wanting data to be included as a requirement.

173
00:13:32,140 --> 00:13:41,580
And then also the Linux Foundation has recommended adding a data card and removing data processing

174
00:13:41,580 --> 00:13:42,580
code.

175
00:13:42,580 --> 00:13:48,100
So there are various institutional actors and individuals that have made requests about

176
00:13:48,100 --> 00:13:49,460
changing the definition.

177
00:13:49,460 --> 00:13:55,060
And I also just want to affirm to everyone on the call that no changes will be made without

178
00:13:55,060 --> 00:13:58,060
a very clear, structured public process.

179
00:13:58,060 --> 00:14:03,260
So you're not going to wake up one morning and a new requirement is in there, or a requirement

180
00:14:03,260 --> 00:14:05,100
has been removed.

181
00:14:05,100 --> 00:14:09,020
We're sharing with you the requests for transparency.

182
00:14:09,020 --> 00:14:14,700
But we will have a public decision-making process about any changes to the definition.

183
00:14:14,700 --> 00:14:18,060
Let's see how much.

184
00:14:18,060 --> 00:14:19,160
Yes.

185
00:14:19,160 --> 00:14:21,220
So this was helpful and useful.

186
00:14:21,220 --> 00:14:28,540
So the LLM 360 team voluntarily ran their system through version 8 review process.

187
00:14:28,540 --> 00:14:34,780
So again, self-review wouldn't be something that would happen in a formal review process,

188
00:14:34,780 --> 00:14:38,780
but this was very, very helpful just that they were able to look through the component

189
00:14:38,780 --> 00:14:43,020
list and the description of the components and said, yes, we understand what each one

190
00:14:43,020 --> 00:14:44,020
says.

191
00:14:44,020 --> 00:14:46,700
Yes, we have documentation associated with each of these components.

192
00:14:46,700 --> 00:14:49,820
Yes, we think it's fair and well-structured.

193
00:14:49,820 --> 00:14:50,820
And they have their post.

194
00:14:50,820 --> 00:14:55,500
Again, that QR code will take you to the forum, which includes all these different posts I'm

195
00:14:55,500 --> 00:14:58,340
describing.

196
00:14:58,340 --> 00:15:02,300
And then, yes, Stefano also posted about certification.

197
00:15:02,300 --> 00:15:10,720
So what is the process for determining that an AI system meets or does not meet the open

198
00:15:10,720 --> 00:15:12,300
source AI definition?

199
00:15:12,300 --> 00:15:15,740
Is that a certification process that lives at OSI?

200
00:15:15,740 --> 00:15:16,960
Does it live somewhere else?

201
00:15:16,960 --> 00:15:18,820
That's something we're also considering.

202
00:15:18,820 --> 00:15:19,980
And Stefano has a post on that.

203
00:15:19,980 --> 00:15:22,460
If you have thoughts, please comment on that.

204
00:15:22,460 --> 00:15:23,460
Yeah.

205
00:15:23,460 --> 00:15:25,660
And this was also prompted by--

206
00:15:25,660 --> 00:15:26,660
Go ahead.

207
00:15:26,660 --> 00:15:27,660
Yeah.

208
00:15:27,660 --> 00:15:34,780
That request was also prompted by the AWS team, which is something that we were already,

209
00:15:34,780 --> 00:15:36,620
you know, back of our mind.

210
00:15:36,620 --> 00:15:38,620
It's definitely for the future.

211
00:15:38,620 --> 00:15:43,920
It's not an immediate urgency now to decide whether we want to have more certifications

212
00:15:43,920 --> 00:15:45,380
and how that would look like.

213
00:15:45,380 --> 00:15:48,420
But I think it's an interesting question to pose.

214
00:15:48,420 --> 00:15:56,220
And if you have any thoughts, please join the forum and contribute to that conversation.

215
00:15:56,220 --> 00:16:00,940
The question is if OSI should engage in the certification and how.

216
00:16:00,940 --> 00:16:03,540
If yes, how?

217
00:16:03,540 --> 00:16:05,180
It's not necessary.

218
00:16:05,180 --> 00:16:12,220
I don't think it's mandatory for OSI to necessarily engage into this process of certification.

219
00:16:12,220 --> 00:16:13,220
Yeah.

220
00:16:13,220 --> 00:16:16,860
And that was also something that came out of the PyCon workshop that we did.

221
00:16:16,860 --> 00:16:24,460
The number one question from participants was how do I know if an open source-- a system

222
00:16:24,460 --> 00:16:27,940
is open source according to a definition?

223
00:16:27,940 --> 00:16:28,940
You know, what's the process?

224
00:16:28,940 --> 00:16:31,980
How do I know as a system creator even?

225
00:16:31,980 --> 00:16:37,220
So that's also something that directed us in that-- toward that question.

226
00:16:37,220 --> 00:16:38,660
Okay.

227
00:16:38,660 --> 00:16:40,380
Yes.

228
00:16:40,380 --> 00:16:46,180
So just ways to participate have been shared already.

229
00:16:46,180 --> 00:16:49,780
But just the link to the forum is discussed at opensource.org.

230
00:16:49,780 --> 00:16:54,100
You need to create an account, which is free if you want.

231
00:16:54,100 --> 00:16:58,180
Or you can become a member and give us a little donation.

232
00:16:58,180 --> 00:16:59,780
OSI is a nonprofit.

233
00:16:59,780 --> 00:17:03,140
We have the biweekly town halls, which you obviously know about.

234
00:17:03,140 --> 00:17:08,580
And then, yeah, if you would like to volunteer, you can DM myself or Stefano.

235
00:17:08,580 --> 00:17:14,420
But primarily me, since it's my role to manage that.

236
00:17:14,420 --> 00:17:16,700
And now we will get to the Q&A.

237
00:17:16,700 --> 00:17:19,500
So I'll start with Dan, and then I'll call on Josh.

238
00:17:19,500 --> 00:17:23,460
So Dan, I'll just read your question.

239
00:17:23,460 --> 00:17:29,420
Are we looking all the way down to the library level for SBOMs?

240
00:17:29,420 --> 00:17:34,020
And I don't know, Stefano, if you need additional information on that or--

241
00:17:34,020 --> 00:17:39,620
I was wondering, I'm not sure what you mean, Dan.

242
00:17:39,620 --> 00:17:45,220
I can give you voice if you want to speak out loud or if you want to elaborate a little

243
00:17:45,220 --> 00:17:53,220
bit more, because I'm not sure which library level or which SBOMs you're referring to.

244
00:17:53,220 --> 00:17:54,220
Okay.

245
00:17:54,220 --> 00:17:55,220
Dan's typing.

246
00:17:55,220 --> 00:17:58,220
Yeah, there you go.

247
00:17:58,220 --> 00:18:00,220
Is that clarifying?

248
00:18:00,220 --> 00:18:01,220
Yeah.

249
00:18:01,220 --> 00:18:03,820
I'm thinking GitHub repositories.

250
00:18:03,820 --> 00:18:09,780
So for AI systems, there are three type of components that are required.

251
00:18:09,780 --> 00:18:15,820
Then they're grouped into data information, which is mainly made of documentation.

252
00:18:15,820 --> 00:18:28,780
There is model parameters, which is basically a set of one and zero organized in tables.

253
00:18:28,780 --> 00:18:38,700
And then there is code components that include the architecture of the system, the code used

254
00:18:38,700 --> 00:18:45,500
for assembling and creating the data set, the code for running the training, and the

255
00:18:45,500 --> 00:18:48,060
code for inference.

256
00:18:48,060 --> 00:18:51,740
So are we looking at the library level?

257
00:18:51,740 --> 00:19:01,140
I mean, for the code piece, SBOMs, I'm assuming you're talking about the code.

258
00:19:01,140 --> 00:19:05,940
I don't understand the question.

259
00:19:05,940 --> 00:19:06,940
Right.

260
00:19:06,940 --> 00:19:07,940
Yeah.

261
00:19:07,940 --> 00:19:11,940
Code, we're looking at is the code open source or not?

262
00:19:11,940 --> 00:19:17,460
And it's usually pretty easy to understand if it's open source or not.

263
00:19:17,460 --> 00:19:26,540
If it carries an open source license and source code is available, you should be able to figure

264
00:19:26,540 --> 00:19:27,540
it out.

265
00:19:27,540 --> 00:19:32,140
SBOMs are -- no.

266
00:19:32,140 --> 00:19:39,660
So if you're asking how deep we are investigating it, you know, someone was asking yesterday

267
00:19:39,660 --> 00:19:53,420
in a conversation, like, if you -- for inference, you need to run a proprietary system or you

268
00:19:53,420 --> 00:19:58,100
need to run on a proprietary platform.

269
00:19:58,100 --> 00:20:11,500
You can only do inference on G Cloud or something else.

270
00:20:11,500 --> 00:20:12,500
We'll have to think specifically.

271
00:20:12,500 --> 00:20:16,420
We'll have to look at specific examples.

272
00:20:16,420 --> 00:20:24,140
Because we want to -- because there is a thing called the system library exception or there

273
00:20:24,140 --> 00:20:30,740
is an experience that we have from the old days when we didn't have the Linux kernel.

274
00:20:30,740 --> 00:20:37,540
And there was a lot of open source software running on proprietary hardware, on very proprietary

275
00:20:37,540 --> 00:20:39,420
operating systems.

276
00:20:39,420 --> 00:20:40,420
And that was okay.

277
00:20:40,420 --> 00:20:41,540
That was fine.

278
00:20:41,540 --> 00:20:46,380
Because we were working towards having more open source code.

279
00:20:46,380 --> 00:20:55,260
So it didn't matter if you were running an open source photo editor on Windows or, you

280
00:20:55,260 --> 00:21:02,860
know, a kernel, a known free kernel with a GNU user space.

281
00:21:02,860 --> 00:21:04,780
It was still open source software.

282
00:21:04,780 --> 00:21:13,220
So we will have to look at specific examples in these cases to see how deep we want to

283
00:21:13,220 --> 00:21:20,780
go into -- it needs to be open all the way down to the last turtle.

284
00:21:20,780 --> 00:21:21,780
>> Thank you.

285
00:21:21,780 --> 00:21:22,780
Okay.

286
00:21:22,780 --> 00:21:23,780
Josh.

287
00:21:23,780 --> 00:21:24,780
>> I hope that explains.

288
00:21:24,780 --> 00:21:25,780
If you have more questions -- okay.

289
00:21:25,780 --> 00:21:26,780
Good.

290
00:21:26,780 --> 00:21:27,780
Josh.

291
00:21:27,780 --> 00:21:31,020
>> First off, thanks for bringing up the system library extension.

292
00:21:31,020 --> 00:21:35,820
That's the first thing I thought of when I've been thinking about these things.

293
00:21:35,820 --> 00:21:37,980
And the reason why that came about.

294
00:21:37,980 --> 00:21:42,580
Just as you explained so succinctly.

295
00:21:42,580 --> 00:21:49,660
And OSI came out of -- the open source definition came out of all of that.

296
00:21:49,660 --> 00:21:54,980
Having criteria for determining, you know, what's going to be part of this operating

297
00:21:54,980 --> 00:21:57,100
system and whatnot.

298
00:21:57,100 --> 00:22:06,540
Where I feel this -- what I feel this is most analogous to, this process that I've experienced,

299
00:22:06,540 --> 00:22:12,420
is actually the free software foundation's respect to your freedom hardware certification.

300
00:22:12,420 --> 00:22:14,900
Because it had nothing to do with hardware in a sense.

301
00:22:14,900 --> 00:22:19,140
Because they didn't -- they weren't looking at hardware design and things.

302
00:22:19,140 --> 00:22:21,100
That was a nice thing.

303
00:22:21,100 --> 00:22:26,100
If anybody wanted to share their hardware designs, that would be great.

304
00:22:26,100 --> 00:22:34,340
But in designing that program, which is -- I led the launch of that.

305
00:22:34,340 --> 00:22:36,700
And the initial certifications.

306
00:22:36,700 --> 00:22:44,540
It really was about thinking about an ecosystem in a context.

307
00:22:44,540 --> 00:22:45,540
Right?

308
00:22:45,540 --> 00:22:47,860
And having to come up with the set of criteria.

309
00:22:47,860 --> 00:22:49,980
How you go about -- okay.

310
00:22:49,980 --> 00:22:52,860
Here's a person selling a product.

311
00:22:52,860 --> 00:22:58,420
And we want to certify that it's, you know, respects your freedom in these ways.

312
00:22:58,420 --> 00:22:59,420
Right?

313
00:22:59,420 --> 00:23:03,980
But it wasn't just looking at that product.

314
00:23:03,980 --> 00:23:06,420
And what code it shipped with.

315
00:23:06,420 --> 00:23:11,700
That was what we would do to give them certification or not.

316
00:23:11,700 --> 00:23:18,260
But what we did as a community and in working with them, is really encourage them to think

317
00:23:18,260 --> 00:23:20,420
about how they're shipping that.

318
00:23:20,420 --> 00:23:23,340
How they're treating the entire ecosystem.

319
00:23:23,340 --> 00:23:27,740
Not just to support this product, but the idea of product lines and the ability for

320
00:23:27,740 --> 00:23:33,860
a person to take this and make their own potential products or adapt the existing products.

321
00:23:33,860 --> 00:23:35,780
And it feels very similar to that.

322
00:23:35,780 --> 00:23:39,420
That you're going to have a lot of different kinds of hardware.

323
00:23:39,420 --> 00:23:44,980
Or in this case, open AI systems or laboratories, as I kind of think of them.

324
00:23:44,980 --> 00:23:48,740
Or open AI ecosystems.

325
00:23:48,740 --> 00:23:56,180
And it feels like it's a little bit broader than, say, a definition.

326
00:23:56,180 --> 00:24:01,580
But more like, you know, a tree of conditions.

327
00:24:01,580 --> 00:24:09,180
Like, well, for these kinds of systems, there are levels of what a person can do.

328
00:24:09,180 --> 00:24:10,180
You know?

329
00:24:10,180 --> 00:24:14,060
If you ship all of this, it gives them this starting point.

330
00:24:14,060 --> 00:24:16,340
And they can then adapt on top of it.

331
00:24:16,340 --> 00:24:21,300
If you give them, you know, you can give them all of this, but it's not going to be any

332
00:24:21,300 --> 00:24:22,300
of the data.

333
00:24:22,300 --> 00:24:26,340
They're going to have to go out and find all of that to just get started.

334
00:24:26,340 --> 00:24:27,340
To have day one.

335
00:24:27,340 --> 00:24:29,860
It's going to be maybe a year out for them.

336
00:24:29,860 --> 00:24:33,580
Or if they're going to need a certain amount of money.

337
00:24:33,580 --> 00:24:39,980
But we had to do the same kinds of things in the open -- in the hardware certification.

338
00:24:39,980 --> 00:24:46,860
And partly why I bring that up is because we didn't do actual certification.

339
00:24:46,860 --> 00:24:47,860
You know?

340
00:24:47,860 --> 00:24:53,820
Now that I'm in the world of standards development with IEEE, I understand what certification

341
00:24:53,820 --> 00:24:58,820
is much better than I did when I was with the FSF.

342
00:24:58,820 --> 00:25:02,120
But really, it was this criteria.

343
00:25:02,120 --> 00:25:04,700
You could use a trademark.

344
00:25:04,700 --> 00:25:08,500
The FSF's trademark.

345
00:25:08,500 --> 00:25:13,380
You could self-certify was one of the ideas that we were pursuing.

346
00:25:13,380 --> 00:25:19,300
And say, I'm delivering what I believe meets all of these criteria of some version of this

347
00:25:19,300 --> 00:25:20,500
criteria.

348
00:25:20,500 --> 00:25:23,820
And I just want to put that out there.

349
00:25:23,820 --> 00:25:30,740
Because I think there are some amazing historical examples that would be -- that we could run

350
00:25:30,740 --> 00:25:36,620
through this without feeling like we're having a moving target.

351
00:25:36,620 --> 00:25:38,380
Things like Cafe.

352
00:25:38,380 --> 00:25:44,940
And then Facebook's creation of Cafe 2 is, to me, one of the greatest case studies that

353
00:25:44,940 --> 00:25:45,940
we can look at.

354
00:25:45,940 --> 00:25:48,980
A lot of people aren't familiar with it.

355
00:25:48,980 --> 00:25:53,580
And I don't do my civic duty of writing about it.

356
00:25:53,580 --> 00:26:02,300
But to me, it's literally one of the best available sort of things that has gone through

357
00:26:02,300 --> 00:26:06,100
a whole -- its whole life cycle from kind of beginning to end.

358
00:26:06,100 --> 00:26:09,940
Because it's now moved on to other things.

359
00:26:09,940 --> 00:26:16,180
But it's -- I'll leave it -- well, I'll leave everything there.

360
00:26:16,180 --> 00:26:23,140
And then if another time I can come back and I can discuss why I think it's a great learning

361
00:26:23,140 --> 00:26:29,300
example for what is happening here and how it could apply.

362
00:26:29,300 --> 00:26:34,740
But my main kind of point -- and it's kind of a question, I guess -- is, do you feel

363
00:26:34,740 --> 00:26:44,500
this is more like this multi -- like, OSI definition is kind of binary.

364
00:26:44,500 --> 00:26:50,420
Your license is either -- when applied to code, is either meeting this definition or

365
00:26:50,420 --> 00:26:51,420
not.

366
00:26:51,420 --> 00:26:59,700
But I feel the open AI definition really is, it's more like a set of criteria for kinds

367
00:26:59,700 --> 00:27:10,140
of systems or laboratories or like a lab of the box or something that is evaluated and

368
00:27:10,140 --> 00:27:20,980
then potentially given kinds of -- not scores, but, you know, meet certain types of criteria.

369
00:27:20,980 --> 00:27:25,140
And is that where you feel this might be going?

370
00:27:25,140 --> 00:27:31,700
Or are you looking to try to get it to be simpler?

371
00:27:31,700 --> 00:27:32,700
I can't really tell.

372
00:27:32,700 --> 00:27:33,700
>> No, thanks, Josh.

373
00:27:33,700 --> 00:27:34,700
What's your comment?

374
00:27:34,700 --> 00:27:35,700
>> No, I get it.

375
00:27:35,700 --> 00:27:42,340
So it's a frequently asked question, I guess.

376
00:27:42,340 --> 00:27:48,060
Only recently there was another one request on the forum about this.

377
00:27:48,060 --> 00:27:54,140
So I believe that strictly the open source AI definition must be binary.

378
00:27:54,140 --> 00:28:03,660
And it must be binary because if it's not binary, then people will -- people, the public,

379
00:28:03,660 --> 00:28:10,900
politicians, regulators, business managers, and business owners, venture capitals, et

380
00:28:10,900 --> 00:28:20,940
cetera, will expect that also the open source definition will imply a range of openness.

381
00:28:20,940 --> 00:28:23,220
Which is already there.

382
00:28:23,220 --> 00:28:30,260
If you want, like if you look very carefully, you will see that some software is open source.

383
00:28:30,260 --> 00:28:34,940
And then on top of that open source layer, there is a proprietary piece that renders

384
00:28:34,940 --> 00:28:38,980
the whole stack less useful.

385
00:28:38,980 --> 00:28:40,620
But still it's better than nothing.

386
00:28:40,620 --> 00:28:47,180
So there is in practice, there are -- I can see that some people might interpret them

387
00:28:47,180 --> 00:28:50,300
as ranges of open.

388
00:28:50,300 --> 00:28:53,340
But for the definition itself is binary.

389
00:28:53,340 --> 00:28:55,620
And that's where we're -- what we're aiming at.

390
00:28:55,620 --> 00:28:59,860
We have an open source AI definition that is binary.

391
00:28:59,860 --> 00:29:06,700
Meaning you provide these data information, code, and required data information, required

392
00:29:06,700 --> 00:29:11,580
code components, and required model components, and you're done.

393
00:29:11,580 --> 00:29:12,940
You pass the bar.

394
00:29:12,940 --> 00:29:17,540
Then if you provide more, you are more open.

395
00:29:17,540 --> 00:29:21,540
If you provide less, you're not open source AI.

396
00:29:21,540 --> 00:29:22,820
That's it.

397
00:29:22,820 --> 00:29:24,420
It's crystal clear.

398
00:29:24,420 --> 00:29:29,900
Now there is something, though, that keeps coming into my mind.

399
00:29:29,900 --> 00:29:36,780
The concept of what you can do, and there was someone posting recently on the forum

400
00:29:36,780 --> 00:29:37,940
again.

401
00:29:37,940 --> 00:29:44,940
What you can do with a full open source AI, in other words, or with some of the artifacts

402
00:29:44,940 --> 00:29:52,820
of the machine learning, without having access to some of the components, is immensely more

403
00:29:52,820 --> 00:30:01,860
useful and more powerful and more -- you can do more than you can do with a binary piece

404
00:30:01,860 --> 00:30:04,380
of software without having the source code.

405
00:30:04,380 --> 00:30:11,260
In other words, if I don't give you -- so the open source AI definition requires very

406
00:30:11,260 --> 00:30:17,060
detailed information about the data set, so that -- and the code that you use to build

407
00:30:17,060 --> 00:30:27,380
it so that you can retrain the -- or you can train a new model that has similar capabilities,

408
00:30:27,380 --> 00:30:32,340
similar scoring, for example, in benchmarks, et cetera.

409
00:30:32,340 --> 00:30:37,580
That's the intention of the Draft008.

410
00:30:37,580 --> 00:30:43,860
That capability of retraining a model, especially if it's a large one, is not something that

411
00:30:43,860 --> 00:30:45,260
will happen very often.

412
00:30:45,260 --> 00:30:49,100
It's not like rebuilding a binary, even if it's a large one.

413
00:30:49,100 --> 00:30:54,300
It's still within reach, and it makes a lot of sense for security, for research, for a

414
00:30:54,300 --> 00:30:56,900
lot of other things.

415
00:30:56,900 --> 00:31:02,860
The retraining of models is -- I don't think it's going to be very, very popular as an

416
00:31:02,860 --> 00:31:03,860
activity.

417
00:31:03,860 --> 00:31:13,260
But at the same time, fine tuning and splitting models, re-architecting and things like that

418
00:31:13,260 --> 00:31:21,100
is the most, in my mind, in my -- you know, I'm not an expert, but from what I've already

419
00:31:21,100 --> 00:31:29,820
-- we've already started to see those activities being a lot more exciting and popular.

420
00:31:29,820 --> 00:31:37,140
So in the future, there might be some other -- I mean, in practice, as we go into practice,

421
00:31:37,140 --> 00:31:41,980
we may see some -- something else pop up.

422
00:31:41,980 --> 00:31:42,980
Yeah.

423
00:31:42,980 --> 00:31:43,980
>> Thank you.

424
00:31:43,980 --> 00:31:44,980
>> Yeah.

425
00:31:44,980 --> 00:31:49,660
Custom models are time-consuming, expensive, et cetera.

426
00:31:49,660 --> 00:31:50,660
Yeah.

427
00:31:50,660 --> 00:31:51,660
So more questions.

428
00:31:51,660 --> 00:31:52,660
>> Yeah.

429
00:31:52,660 --> 00:31:55,700
Does anyone who hasn't asked a question have a question?

430
00:31:55,700 --> 00:32:11,940
And you can raise your hand or write in the chat.

431
00:32:11,940 --> 00:32:14,820
And any follow-up questions?

432
00:32:14,820 --> 00:32:19,820
Someone who's already asked a question and wanted to ask a follow-up.

433
00:32:19,820 --> 00:32:20,820
Okay.

434
00:32:20,820 --> 00:32:21,820
Josh.

435
00:32:21,820 --> 00:32:22,820
>> Yeah.

436
00:32:22,820 --> 00:32:35,420
So part of where I'm still struggling here, right, is that -- for good reasons, I should

437
00:32:35,420 --> 00:32:37,060
say, I'll start there.

438
00:32:37,060 --> 00:32:39,460
So that I don't -- I don't want to offend anybody.

439
00:32:39,460 --> 00:32:43,860
I think people have made a lot of good choices and has tried to do good things.

440
00:32:43,860 --> 00:32:52,380
But in general, our community has tried to, except for some, avoided talking about the

441
00:32:52,380 --> 00:33:00,740
fact that when we say an operating system is open source, we don't really mean that.

442
00:33:00,740 --> 00:33:02,260
Right?

443
00:33:02,260 --> 00:33:10,020
We don't really mean that you go and if I pick a piece of code at random, it is going

444
00:33:10,020 --> 00:33:12,380
to be open source.

445
00:33:12,380 --> 00:33:14,360
When we're talking about the operating system.

446
00:33:14,360 --> 00:33:19,700
We mean for the most part, practically speaking, with some exceptions here and there.

447
00:33:19,700 --> 00:33:20,700
And that's important to note.

448
00:33:20,700 --> 00:33:24,660
Those are the exceptions that make -- those are the only parts that are the non-open source

449
00:33:24,660 --> 00:33:27,580
parts at times.

450
00:33:27,580 --> 00:33:33,900
And they're there to enable -- to practically allow people to run on different hardware

451
00:33:33,900 --> 00:33:40,340
systems to allow for things that in life are important.

452
00:33:40,340 --> 00:33:41,340
Right?

453
00:33:41,340 --> 00:33:48,620
Whether they were browser add-ons or they were kernel modules or what have you.

454
00:33:48,620 --> 00:33:49,700
Right?

455
00:33:49,700 --> 00:33:57,300
And so I think it's kind of maybe important to note that if the level of things we're

456
00:33:57,300 --> 00:34:06,500
judging are these multifaceted systems, our definition might not need to be when applied

457
00:34:06,500 --> 00:34:11,800
in normally might not need to be perfect.

458
00:34:11,800 --> 00:34:14,540
Because we've never really done that.

459
00:34:14,540 --> 00:34:20,020
People don't want to take the free software foundation stance of, you know, Debian is

460
00:34:20,020 --> 00:34:22,660
not a free software operating system.

461
00:34:22,660 --> 00:34:23,660
Right?

462
00:34:23,660 --> 00:34:24,660
Like, that's just been brutal.

463
00:34:24,660 --> 00:34:27,260
I lived that for ten years when I worked there.

464
00:34:27,260 --> 00:34:28,260
It was terrible.

465
00:34:28,260 --> 00:34:29,260
I hated it.

466
00:34:29,260 --> 00:34:32,220
But I understood why they took that line.

467
00:34:32,220 --> 00:34:35,620
Because they felt somebody needed to.

468
00:34:35,620 --> 00:34:39,900
Even if it -- but everybody else, and I'm very happy everybody else made the good choice

469
00:34:39,900 --> 00:34:45,900
of being practical and saying Debian is a free and open source operating system and

470
00:34:45,900 --> 00:34:48,140
Red Hat is and whatnot.

471
00:34:48,140 --> 00:34:49,140
Right?

472
00:34:49,140 --> 00:34:53,180
But I wonder if we could do something similar with this.

473
00:34:53,180 --> 00:34:59,460
Where we say, look, here's the pristine version of it.

474
00:34:59,460 --> 00:35:07,540
If there are just things that are kind of added on to enable this to happen in various

475
00:35:07,540 --> 00:35:17,660
contexts, then we don't throw -- we don't just label the whole thing as not open source

476
00:35:17,660 --> 00:35:18,660
AI.

477
00:35:18,660 --> 00:35:19,660
Right?

478
00:35:19,660 --> 00:35:27,500
I think that we -- I think that maybe we should say, when we apply this definition, we can

479
00:35:27,500 --> 00:35:34,220
do it in a way that says, is the bulk sort of kernel of this?

480
00:35:34,220 --> 00:35:42,700
Is there a single way in which you could take all of the -- take a subset, a majority subset

481
00:35:42,700 --> 00:35:49,580
of this and apply it in a circumstance where it is 100% AI and these other things are just

482
00:35:49,580 --> 00:35:56,700
for practical compromises to allow it to run in more systems, use certain data sources,

483
00:35:56,700 --> 00:35:57,700
or what have you.

484
00:35:57,700 --> 00:36:05,260
They're not necessarily necessary for what we're evaluating when we say open source AI,

485
00:36:05,260 --> 00:36:10,620
but they're practically needed to allow people to actually use these.

486
00:36:10,620 --> 00:36:12,220
And that's how we apply it.

487
00:36:12,220 --> 00:36:18,100
It still gets us to binary, it's still criteria, but I mean -- sorry, I'm just trying to really

488
00:36:18,100 --> 00:36:19,100
think about real world.

489
00:36:19,100 --> 00:36:20,100
>> Great question.

490
00:36:20,100 --> 00:36:21,100
Yeah.

491
00:36:21,100 --> 00:36:22,100
>> Josh, I hear you.

492
00:36:22,100 --> 00:36:32,980
And in fact, you know, probably as veterans of the open source and free software, I think

493
00:36:32,980 --> 00:36:37,380
you recognize that there is a piece above the checklist.

494
00:36:37,380 --> 00:36:39,500
The checklist is very specific.

495
00:36:39,500 --> 00:36:43,720
And let's say it's a sort of experiment.

496
00:36:43,720 --> 00:36:46,420
That's why it's going through the validation phase.

497
00:36:46,420 --> 00:36:50,440
But what really, in my mind, what really counts is what's above that.

498
00:36:50,440 --> 00:36:57,180
And above that, there is the definition that looks pretty much like the free software foundation,

499
00:36:57,180 --> 00:37:02,900
the free software definition, not just the four freedoms, but also what's written below

500
00:37:02,900 --> 00:37:07,780
as the preferred form to make modifications to a machine learning system.

501
00:37:07,780 --> 00:37:11,460
Those pieces are the ones that in my mind count a lot more.

502
00:37:11,460 --> 00:37:17,580
Because in those pieces, we can have that flexibility to judge and evaluate.

503
00:37:17,580 --> 00:37:21,860
From a distance, we're going to be able to see, hey, do I know enough about the prominence

504
00:37:21,860 --> 00:37:27,260
of this data so that I can say how you've trained your system and therefore I can say

505
00:37:27,260 --> 00:37:33,220
I can replicate it and that will tell me that it's really an open source AI.

506
00:37:33,220 --> 00:37:36,740
And if not, like, dude, I don't even need to go through the checklist.

507
00:37:36,740 --> 00:37:40,020
Like, you know, it's very quick and clear.

508
00:37:40,020 --> 00:37:45,340
But if I have plenty of information, then if it's skipping one of the elements of those

509
00:37:45,340 --> 00:37:53,020
checklists, I can probably say confidently, like, yeah, this is open source enough.

510
00:37:53,020 --> 00:37:54,740
I can live with it.

511
00:37:54,740 --> 00:37:57,380
Because I know that I can do this, this, and that.

512
00:37:57,380 --> 00:38:02,580
I can modify, I can study, share with confidence.

513
00:38:02,580 --> 00:38:10,340
So a lot of it, remember that this is it took 20 plus years to go from the free software

514
00:38:10,340 --> 00:38:12,340
definition to the open source definition.

515
00:38:12,340 --> 00:38:19,180
Like, that required that generate I mean, those 20 years generated a huge amount of

516
00:38:19,180 --> 00:38:27,100
code licenses, there was plenty to draw from to write those 10 points for the Debian free

517
00:38:27,100 --> 00:38:30,500
software guidelines.

518
00:38:30,500 --> 00:38:31,700
We don't have that luxury.

519
00:38:31,700 --> 00:38:37,220
We're really flying and building the plane at the same time.

520
00:38:37,220 --> 00:38:41,060
But yeah, the experience is really valuable.

521
00:38:41,060 --> 00:38:47,860
And if you share with me that cafe that you mentioned that via email, I'm really curious

522
00:38:47,860 --> 00:38:49,580
to see what that is.

523
00:38:49,580 --> 00:38:51,660
Yeah, I'll do that.

524
00:38:51,660 --> 00:38:55,860
I've written it up for sharing to a co worker somewhat recently.

525
00:38:55,860 --> 00:38:57,460
And I'll adapt that.

526
00:38:57,460 --> 00:38:59,260
Just a quick little follow up.

527
00:38:59,260 --> 00:39:00,260
Oh, sorry.

528
00:39:00,260 --> 00:39:01,260
I didn't capitalize.

529
00:39:01,260 --> 00:39:02,260
Yep, yep.

530
00:39:02,260 --> 00:39:04,260
Look, Peter, I need to just absolutely.

531
00:39:04,260 --> 00:39:05,260
Sorry.

532
00:39:05,260 --> 00:39:09,620
So what I'm going to do is I'm going to go to Dan's question.

533
00:39:09,620 --> 00:39:12,800
And then I'll do a last call for questions.

534
00:39:12,800 --> 00:39:17,060
And I think it might make sense, Josh to take the any continuing, continuing conversation

535
00:39:17,060 --> 00:39:18,060
into email.

536
00:39:18,060 --> 00:39:22,020
Just so that we can can wrap up the meeting.

537
00:39:22,020 --> 00:39:26,180
But yeah, so I'll take Dan's question, then we'll do the last call.

538
00:39:26,180 --> 00:39:35,100
So Dan is asking, how will OSI partnerships work, particularly a OS, OSI AI partnerships.

539
00:39:35,100 --> 00:39:39,860
So I think we might need more clarification, but Stefano, do you have enough to respond

540
00:39:39,860 --> 00:39:40,860
to that?

541
00:39:40,860 --> 00:39:46,780
Yeah, I it's not clear to me the concept of partnership, because we don't have partners

542
00:39:46,780 --> 00:39:54,740
now we have the OSI has affiliate organizations, which are other nonprofits that support the

543
00:39:54,740 --> 00:39:56,500
mission of the OSI.

544
00:39:56,500 --> 00:40:04,140
We have individual members who donate to us and decide donate or not, but decide to support

545
00:40:04,140 --> 00:40:10,860
the mission of OSI with money or just by following our activities.

546
00:40:10,860 --> 00:40:19,180
And we have individual sponsors, sorry, corporate sponsors and and but we don't have partners.

547
00:40:19,180 --> 00:40:20,700
So I'm not sure.

548
00:40:20,700 --> 00:40:26,540
We don't envision to have AI partnerships.

549
00:40:26,540 --> 00:40:32,460
So if you want to type in there, Dan, what you're clarifying follow up, then we are happy

550
00:40:32,460 --> 00:40:38,540
to respond to your questions.

551
00:40:38,540 --> 00:40:47,020
And then I think I will just do one last call for comments, actually for questions.

552
00:40:47,020 --> 00:40:53,500
Yes, and then and then yeah, we can take the conversation Josh, the quick conversation

553
00:40:53,500 --> 00:40:57,220
offline to continue if you'd like.

554
00:40:57,220 --> 00:41:00,740
So as I scroll to the last, thank you, slide.

555
00:41:00,740 --> 00:41:02,740
I'm not seeing any.

556
00:41:02,740 --> 00:41:05,180
Okay, cool.

557
00:41:05,180 --> 00:41:07,540
All right.

558
00:41:07,540 --> 00:41:09,020
So then thank you so much.

559
00:41:09,020 --> 00:41:10,020
Anna.

560
00:41:10,020 --> 00:41:12,060
I see I see a raised hand from Anna.

561
00:41:12,060 --> 00:41:15,460
I think it might be a clapping hand from Anna.

562
00:41:15,460 --> 00:41:16,940
Oh, I see.

563
00:41:16,940 --> 00:41:17,940
Okay.

564
00:41:17,940 --> 00:41:18,940
Thank you, Anna.

565
00:41:18,940 --> 00:41:19,940
We appreciate it.

566
00:41:19,940 --> 00:41:20,940
Yeah, we met on at PyCon.

567
00:41:20,940 --> 00:41:21,940
Oh, just clapping.

568
00:41:21,940 --> 00:41:22,940
Okay.

569
00:41:22,940 --> 00:41:23,940
All right.

570
00:41:23,940 --> 00:41:24,940
We appreciate it.

571
00:41:24,940 --> 00:41:25,940
Okay, bye.

572
00:41:25,940 --> 00:41:26,940
Thank you, everyone.

573
00:41:26,940 --> 00:41:27,940
And hang out in the forum.

574
00:41:27,940 --> 00:41:28,940
That's where we share all our updates and opportunities for interaction and feedback.

575
00:41:28,940 --> 00:41:29,940
Thank you.

576
00:41:29,940 --> 00:41:29,940
Thanks.

577
00:41:29,940 --> 00:41:30,940
Bye.

578
00:41:30,940 --> 00:41:30,940
Bye.

579
00:41:30,940 --> 00:41:35,940
Bye.

580
00:41:35,940 --> 00:41:37,940
Thanks.

### End of last town hall held on 2024-05-31 ###

### Start of next town hall held on 2024-06-14 ###
--- Presentation for 2024-06-14 ---
OPEN SOURCE AI DEFINITION
Online public townhall
June 14, 2024
last updated: June 11, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

OSI’s objective for 2024

Open Source AI Definition

3

Open Source AI Deﬁnition

Current Version
OSAID v.0.0.8

4

Open
Source AI
Definition

Preamble
4 Freedoms

v.0.0.8

Legal Checklist

-

Open Source AI Deﬁnition

Key Feedback
OSAID v.0.0.8

6

Open
Source AI
Definition
Data
Information
v.0.0.8

Requiring only data
information…

…instead of training
datasets is the
greatest point of
debate now.

Open
Source AI
Definition
Other
Components
v.0.0.8

Others have
proposed…
… removing data
pre-processing
code requirement if
training data is not
required.

…requiring a model
card
… and data card to
standardize system
documentation.

Open Source
AI Definition
Describing Legal
Requirements
v.0.0.8

In contrast to the clear
“OSI-approved”
licenses available for
code components…
… the “OSD
compliant”
requirement for data
information...
…and “OSD
conformant”
requirement for
model parameters
have been
challenging for
reviewers to
interpret.

Preferred form to make modiﬁcations
Data information

Code

Model

Sufficiently detailed
information about
the data used to
train the system, so
that a skilled person
can recreate a
substantially
equivalent system
using the same or
similar data.

The source code
used to train and run
the system.

The model
parameters
(weights and biases)

Data Information Explained
◦ The intention of Data information is to allow
developers to recreate a substantially
equivalent system using the same or similar
data.
◦ Came out of the systems review process, with
votes by volunteers.

11

Zooming in on the issues with datasets
◦ The Pile taken down after an alleged copyright
infringement in the US. But legal in Japan. Maybe
legal in EU
◦ DOLMA, initially had a restrictive license. Later
switched to a permissive one. Suffers from the
same legal uncertainties of the Pile, however the
Allen Institute has not been sued, yet.
◦ Training techniques that preserve privacy like
federated learning don’t create datasets.
12

Alternative proposals
● Use synthetic data: Experimental,
unproven technology, limited to
corner cases
● All their components must be “open
source”: This integralism ignores that
even the GNU project accepts system
library exceptions and other
compromises.

13

Open Source AI Deﬁnition

System Validation
OSAID v.0.0.8

14

Validation Reviewers
1. Arctic
1.

Jesús M.
Gonzalez-Barahona
Universidad Rey Juan
Carlos

2. BLOOM*
2.
3.

Danish Contractor
BLOOM Model Gov.
Work Group
Jaan Li University of
Tartu, One Fact
Foundation

3. Falcon
1.
2.

Casey Valk Nutanix
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France

4. Grok
1.
2.

Victor Lu independent
database consultant
Karsten Wade Open
Community Architects

We’re interested in reviewing about 10 AI systems self-described as open as part of
this definition validation phase. Those marked (*) have were reviewed in previous
phases.

5. Llama 2*
1.
2.
3.
4.

Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute of
Paris
Victor Lu independent
database consultant

9. LLM360
5.

[Team member TBD]
LLM360

8. Mistral
1.
2.
3.

Mark Collier
OpenInfra Foundation
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
Cailean Osborne
University of Oxford,
Linux Foundation

7. OLMo
4.
5.

Amanda Casari
Google
Abdoulaye Diack
Google

10. Pythia*
1.
2.
3.

Hailey Schoelkopf
EleutherAI

4.

Aviya Skowron
EleutherAI

11. T5
5.

8. OpenCV*
We will need an
independent reviewer
for LLM360

1.

Rasim Sen Oasis
Software Technology
Ltd.

9. Phi-2
6.

Seo-Young Isabelle
Hwang Samsung

Seo-Young Isabelle
Hwang Samsung
Stella Biderman
EleutherAI

Jaan Li University of
Tartu, One Fact
Foundation

Viking
6.

Merlijn Sebrechts
Ghent University

Validation Challenges

It is hard for volunteer reviewers
to find required documents
independently..

Validation Challenges

This meant a lot of the review
analysis has been incomplete.

Validation Solutions

Having the help of
system creators to locate
documents has been
crucial.

Thank you,
Arctic!

Validation Expectations

Given current system information, our expected
review results are as follows. If we are missing
information, please let us know.

Open Source AI Deﬁnition

What’s Next?

June - October 2024

● Complete validation phase
● Resolve comments, release v. 0.0.9 after
validation
● Cut the release candidate with sufficient
endorsement
20

Complete the Validation
Phase

Thanks,
LLM360!

1. Reach out to AI system creators to
ﬁll in the blanks on their own systems
by pointing us to correct
documentation
2. Invite volunteers to also help us ﬁll in
these blanks

21

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

March

April

May

June …

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review
Meetings
Begin

Virtual System
Review
Meetings
Continue

Virtual System
Review
Meetings
END

Feedback
Informs Content
of OSI In-Person
Stakeholder
Meeting

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Townhalls +

Townhall +

Draft 0.0.5

Draft 0.0.6

Draft 0.0.7 and 8

PyCon
Workshop
May 17th,
Pittsburgh)

(≈

Draft 0.0.9

… October
Monthly Virtual
Meetings

Release stable
version

Virtual Launch
Event (date
TBD)

RC1

Stable
Version

In-Person Meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

✓ PyCon US

May 17

Europe

France

Paris

✓ OW2

June 11 - 12

North America

United States

New York

OSPOs for Good

July 9 - 11

Africa

Virtual

Virtual

Sustain Africa

July

Asia Paciﬁc

China

Hong Kong

AI_dev

August 23

Latin America

Argentina

Buenos Aires

Nerdearla

September

Europe

TBD

TBD

(data governance)

October

North America

United States

Raleigh

All Things Open

Oct 27 - 29
23

Participation Options
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
● Volunteer to help with validation (email or DM Mer
Joyce)
24

Q&A

25

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the deﬁnition.

26


--- Subtitles for 2024-06-14 --- ###
1
00:00:00,001 --> 00:00:08,040
All right, folks, thanks for joining this town hall.

2
00:00:08,040 --> 00:00:13,520
We meet every two weeks, try to keep up to date the community about the evolution of

3
00:00:13,520 --> 00:00:21,880
the draft and summarizing the conversations that we've been having on the forums and in

4
00:00:21,880 --> 00:00:29,960
conferences like the conference that I just presented at here in Paris, in Cologne, Paris,

5
00:00:29,960 --> 00:00:31,760
in a hotel.

6
00:00:31,760 --> 00:00:41,240
So hopefully the network gods support us to go through this recording.

7
00:00:41,240 --> 00:00:45,920
Just the community agreement, remember that we're trying, the one that I'm really focused

8
00:00:45,920 --> 00:00:52,640
on and love the most is the forward motion, the fact that we are trying to understand

9
00:00:52,640 --> 00:00:57,280
where the problems are and if we don't find an immediate solution, still we've marked

10
00:00:57,280 --> 00:01:03,680
the spot, we move around, we'll get back to it because we do have to find a solution.

11
00:01:03,680 --> 00:01:08,600
We do have a tremendous amount of pressure from all over the place, from politicians,

12
00:01:08,600 --> 00:01:16,800
policymakers and from the industry, from academia, from the nonprofit and civil society to provide

13
00:01:16,800 --> 00:01:23,960
some sort of guidance of what open source AI really means since everyone is using this

14
00:01:23,960 --> 00:01:26,200
term.

15
00:01:26,200 --> 00:01:32,360
And reminding that this is our objective to have by the end of the year a workable definition,

16
00:01:32,360 --> 00:01:41,880
something that is acceptable even though it may not be the most perfect test thing that

17
00:01:41,880 --> 00:01:49,960
we can imagine because the world is moving very rapidly and things might change in the

18
00:01:49,960 --> 00:01:51,880
future.

19
00:01:51,880 --> 00:01:59,840
So we need to find ways to provide solid principles that maybe will not change but allow for some

20
00:01:59,840 --> 00:02:01,960
parts to be adapted.

21
00:02:01,960 --> 00:02:07,080
We'll talk about some of the topics that emerged last week.

22
00:02:07,080 --> 00:02:16,840
So the current version of the definition is still draft 008 and this was released a little

23
00:02:16,840 --> 00:02:26,760
bit over a month ago at this point and it's made of some parts and parts that have received

24
00:02:26,760 --> 00:02:34,960
very little comments in the past months are the preamble which is the place where we define

25
00:02:34,960 --> 00:02:42,200
what we're talking about which is we decided to adopt the definition of an AI system provided

26
00:02:42,200 --> 00:02:49,960
by the Organization for Economic Collaboration and Development or ECD.

27
00:02:49,960 --> 00:02:55,480
It seems to be a widely accepted definition so we're going to use that as a reference.

28
00:02:55,480 --> 00:03:00,960
Maybe it's worth going back and remind people at this stage that the reason why we have

29
00:03:00,960 --> 00:03:08,840
a definition of an AI system is because we needed to have an anchor for the conversation.

30
00:03:08,840 --> 00:03:15,680
In the free software definition, the free software definition refers to the program

31
00:03:15,680 --> 00:03:22,560
and everyone understands what the program is or at least there is a very small margin

32
00:03:22,560 --> 00:03:27,800
for error or misinterpretations.

33
00:03:27,800 --> 00:03:34,240
But when it comes to AI there are misinterpretations and that's why there is a definition and these

34
00:03:34,240 --> 00:03:43,960
definitions are also stated in law like the AI Act mentions, defines an AI system in a

35
00:03:43,960 --> 00:03:47,240
very similar way to the OECD text.

36
00:03:47,240 --> 00:03:49,000
So that's what we have.

37
00:03:49,000 --> 00:03:55,960
And then we have in the preamble the reason why we are working on and the reason why we

38
00:03:55,960 --> 00:04:03,680
want the definition of open source AI which is a very short summary of the intention to

39
00:04:03,680 --> 00:04:13,760
give users the same rights that they have in software, the same independence, control

40
00:04:13,760 --> 00:04:22,200
of the technology to enable permissionless innovation and collaboration.

41
00:04:22,200 --> 00:04:29,120
Now the concept of user is also not defined in the free software space and this is an

42
00:04:29,120 --> 00:04:31,160
area that has received some comments.

43
00:04:31,160 --> 00:04:38,680
We want to be explicit and I think I'm working on a draft 0.9 and that will specify, will

44
00:04:38,680 --> 00:04:45,240
contain a suggestion for what the recipients of the rights should be.

45
00:04:45,240 --> 00:04:52,080
And draft 0.09 will probably mention the fact that we want end users, so anyone who is interacting

46
00:04:52,080 --> 00:04:59,400
with the system, putting input and receiving outputs and also developers and deployers

47
00:04:59,400 --> 00:05:04,120
of AI systems needs to be able to enjoy those freedoms.

48
00:05:04,120 --> 00:05:09,840
Now below this part have not received a lot of comments so they seem to be fairly stable

49
00:05:09,840 --> 00:05:10,840
to me.

50
00:05:10,840 --> 00:05:19,800
The one part that is new in draft 0.08 is the concept of preferred form to make modifications.

51
00:05:19,800 --> 00:05:27,440
That is something that is described in the free software definition as access to the

52
00:05:27,440 --> 00:05:35,320
source code that is necessary for the actions of study the software and modify the software.

53
00:05:35,320 --> 00:05:43,000
Now to study an AI system and to modify an AI system it's necessary to define what the

54
00:05:43,000 --> 00:05:49,320
preferred form to make modification is and that's where there is a new section in 0.08

55
00:05:49,320 --> 00:05:53,000
that says what the preferred form to make modification is.

56
00:05:53,000 --> 00:05:55,520
I'll go into the details later.

57
00:05:55,520 --> 00:05:58,360
And finally the bottom part is a legal checklist.

58
00:05:58,360 --> 00:06:05,600
This is what I refer to as it's based on a paper from the Linux Foundation called the

59
00:06:05,600 --> 00:06:08,280
Model Openness Framework.

60
00:06:08,280 --> 00:06:14,960
You will see often referred to with the acronym MOF.

61
00:06:14,960 --> 00:06:22,560
And this lists components of machine learning systems and in generic terms but they're defined

62
00:06:22,560 --> 00:06:23,600
in the paper.

63
00:06:23,600 --> 00:06:32,880
We use this as an ideal checklist that a future certifier or reviewer of AI systems might

64
00:06:32,880 --> 00:06:39,440
use as a reference to say if this component is available under licenses or terms or legal

65
00:06:39,440 --> 00:06:45,200
terms that allow the same grant, the same freedoms of the open source definition, the

66
00:06:45,200 --> 00:06:52,800
original one, then if the required components are available under those acceptable conditions

67
00:06:52,800 --> 00:06:56,280
then the AI system is an open source AI.

68
00:06:56,280 --> 00:06:58,840
If not, most likely it's not.

69
00:06:58,840 --> 00:07:02,080
Okay, so let's go a little bit into the details.

70
00:07:02,080 --> 00:07:09,400
The key feedback that we have received is around what is required and what is optional.

71
00:07:09,400 --> 00:07:21,960
So the pieces of optional components, I mean required

72
00:07:21,960 --> 00:07:29,760
components are the biggest conversation is about data.

73
00:07:29,760 --> 00:07:40,960
The original datasets are seen, some people have seen the training dataset into the optional

74
00:07:40,960 --> 00:07:46,360
components and I believe they may have jumped to conclusions because the, well I'll talk

75
00:07:46,360 --> 00:07:53,720
about it, but these are replaced with data information.

76
00:07:53,720 --> 00:07:55,640
So let's skip through.

77
00:07:55,640 --> 00:08:00,600
The other required components that I want to draw your attention on is the fact that

78
00:08:00,600 --> 00:08:13,520
the data processing and like one of the components is actually required and data processing and

79
00:08:13,520 --> 00:08:19,880
labeling techniques and all the disclosures about how the dataset has been built are part

80
00:08:19,880 --> 00:08:24,560
of the draft.

81
00:08:24,560 --> 00:08:32,920
And I'll go into more details later.

82
00:08:32,920 --> 00:08:43,840
So there is a very lots of confusion around this part of the definition and we need to

83
00:08:43,840 --> 00:08:46,240
spend a little bit more time to discuss.

84
00:08:46,240 --> 00:08:52,440
Other minor requests are comments that we have received that are around the fact that

85
00:08:52,440 --> 00:09:02,840
the legal requirements are described as available with terms like available under OSD, the original

86
00:09:02,840 --> 00:09:10,840
open source definition, OSD compliant terms or OSD conformant terms.

87
00:09:10,840 --> 00:09:14,040
Quick explanation of what this means.

88
00:09:14,040 --> 00:09:19,280
The data information, I mean the concept here in data information like training methodologies,

89
00:09:19,280 --> 00:09:25,480
et cetera, training, training scope, the data, the scope of the data, where the sources,

90
00:09:25,480 --> 00:09:31,400
et cetera, are in documentation are probably going to be in the form of documentation.

91
00:09:31,400 --> 00:09:39,000
The documentation uses different licenses and different agreements for sharing and distributing

92
00:09:39,000 --> 00:09:42,160
for distribution.

93
00:09:42,160 --> 00:09:49,880
However, the OSI has reviewed licenses that are not specifically targeting software.

94
00:09:49,880 --> 00:09:57,600
So we don't have a way to say right now, we don't have a list of OSI approved licenses

95
00:09:57,600 --> 00:09:58,720
for documentation.

96
00:09:58,720 --> 00:10:05,120
So we know what they look like and they conform, they comply with the open source definition.

97
00:10:05,120 --> 00:10:10,680
In other words, they allow for no discrimination of use, no discrimination of people, no field

98
00:10:10,680 --> 00:10:18,320
of use, strict restrictions, ability of preferred form to make modification to the text, like

99
00:10:18,320 --> 00:10:22,040
the source code of the documentation, et cetera.

100
00:10:22,040 --> 00:10:27,440
So like you don't distribute a PDF encrypted as documentation.

101
00:10:27,440 --> 00:10:28,440
That's not acceptable.

102
00:10:28,440 --> 00:10:30,280
So that's what these words say.

103
00:10:30,280 --> 00:10:37,500
And for the model parameters, we do talk about the OSD conformant.

104
00:10:37,500 --> 00:10:43,520
So we use a different term because most likely model parameters don't fall.

105
00:10:43,520 --> 00:10:47,280
Actually it's quite clear at this point, they don't fall into copyright law.

106
00:10:47,280 --> 00:10:58,920
So using the word licenses and license approved, OSI approved, OSI compliant terms is not really

107
00:10:58,920 --> 00:11:00,720
useful.

108
00:11:00,720 --> 00:11:05,120
We need to be using a different framework and it's probably going to be more in the

109
00:11:05,120 --> 00:11:12,120
contract law, at least that's what many of the lawyers that I talk to seem to think.

110
00:11:12,120 --> 00:11:18,760
And that's, you know, OSD conformant means you still need to give us access to the, give

111
00:11:18,760 --> 00:11:25,400
us the possibility to share, modify, give out free, without asking for permissions,

112
00:11:25,400 --> 00:11:26,400
et cetera.

113
00:11:26,400 --> 00:11:31,680
So it's a word that it will be more explicit in draft 09.

114
00:11:31,680 --> 00:11:37,280
There will be a description in that lawyers can interpret more clearly.

115
00:11:37,280 --> 00:11:40,480
So let's get into the concept of data information.

116
00:11:40,480 --> 00:11:42,520
And maybe I need to take a little bit of a step back.

117
00:11:42,520 --> 00:11:51,020
The intention of the definition is to provide in the upper part, so above the checklist,

118
00:11:51,020 --> 00:11:58,040
what we call the checklist, in the upper part of the document, the stated intentions and

119
00:11:58,040 --> 00:12:06,440
sort of a general, unmutable, general purpose reference point that can resist the test of

120
00:12:06,440 --> 00:12:07,440
time.

121
00:12:07,440 --> 00:12:11,480
That's why we have principles in the preamble.

122
00:12:11,480 --> 00:12:16,080
The definition of open source AI is synthetic in the four freedoms.

123
00:12:16,080 --> 00:12:22,000
The definition of preferred form to make modifications to a machine learning system is because machine

124
00:12:22,000 --> 00:12:31,280
learning is the place where we have the challenges today of recognizing the new artifacts in

125
00:12:31,280 --> 00:12:34,480
these AI, modern AI systems.

126
00:12:34,480 --> 00:12:40,000
And so when we get into the preferred form to make modifications, we were looking for,

127
00:12:40,000 --> 00:12:46,560
we were pushing the community to find a way to describe in generic terms, the intention.

128
00:12:46,560 --> 00:12:52,480
And the intention is to have the possibility to recreate from scratch.

129
00:12:52,480 --> 00:13:00,520
If I receive an AI system that I like and I wanted to give it to others, I need to be

130
00:13:00,520 --> 00:13:08,520
able to have all the instructions and all the tools and all the data to recreate a substantially

131
00:13:08,520 --> 00:13:12,640
equivalent system.

132
00:13:12,640 --> 00:13:14,160
Because that's important.

133
00:13:14,160 --> 00:13:21,400
That's one of the fundamental principles of open source has always been to be able to

134
00:13:21,400 --> 00:13:28,920
have the instructions and be able to share those with others.

135
00:13:28,920 --> 00:13:36,560
Now during the review process and during the co-design process, we asked volunteers to

136
00:13:36,560 --> 00:13:48,520
evaluate existing systems and to rank the importance of some of the components.

137
00:13:48,520 --> 00:13:55,200
And during that phase, a recommendation came out when they voted that many people voted

138
00:13:55,200 --> 00:14:05,880
much higher the availability of the details about how the datasets were built rather than

139
00:14:05,880 --> 00:14:07,960
the actual datasets.

140
00:14:07,960 --> 00:14:16,560
So that gave us our suggestion that maybe it was worth testing the waters and understand

141
00:14:16,560 --> 00:14:22,460
a little bit better what the issue around data is.

142
00:14:22,460 --> 00:14:30,400
From the high level perspective, when approaching the problem, we realized, I mean, all of us

143
00:14:30,400 --> 00:14:38,840
have had the same impression, data, the pipeline to build an AI system starts with data that

144
00:14:38,840 --> 00:14:44,080
gets filtered, mangled, assembled, tokenized into a dataset.

145
00:14:44,080 --> 00:14:47,360
The dataset gets fed into the training machine.

146
00:14:47,360 --> 00:14:49,640
Training is an iterative process.

147
00:14:49,640 --> 00:14:53,040
Data comes in at different stages.

148
00:14:53,040 --> 00:14:59,840
All of this is complicated but should be described and made available.

149
00:14:59,840 --> 00:15:02,880
And after the training is done, you get the parameters.

150
00:15:02,880 --> 00:15:05,640
And with the parameters, you load them into an inference engine.

151
00:15:05,640 --> 00:15:13,080
And that is what responds to, well, and you put an UI on top that gives you input and

152
00:15:13,080 --> 00:15:19,240
outputs like think of chat GPT or other systems like that.

153
00:15:19,240 --> 00:15:27,920
When you look from the distance at this whole pipeline, the intuition is that the whole

154
00:15:27,920 --> 00:15:29,840
pipeline needs to be made available.

155
00:15:29,840 --> 00:15:34,600
That whole pipeline is what you need to modify the preferred form to make modifications to

156
00:15:34,600 --> 00:15:35,600
the system.

157
00:15:35,600 --> 00:15:40,360
Now, when you start to zoom in, that's where the problem arises.

158
00:15:40,360 --> 00:15:45,880
So when I started looking into one of these systems that in my mind were the two systems

159
00:15:45,880 --> 00:15:55,080
that are the most open, the most freely licensed, freely made available, and that one is called

160
00:15:55,080 --> 00:15:59,480
Pythia from the nonprofit called Eleuther AI.

161
00:15:59,480 --> 00:16:06,000
The other one is from the research institute, ALEN AI Institute.

162
00:16:06,000 --> 00:16:11,880
And Pythia has been trained on a dataset called the pile.

163
00:16:11,880 --> 00:16:14,080
The pile is fully described.

164
00:16:14,080 --> 00:16:16,960
There is that community working on it.

165
00:16:16,960 --> 00:16:23,720
All the tools that have been used to assemble the pile, filter the pile, train Pythia using

166
00:16:23,720 --> 00:16:28,360
the pile and all that, they're all released with open source licenses.

167
00:16:28,360 --> 00:16:37,040
Now the pile has been an object of a takedown request for alleged copyright infringement

168
00:16:37,040 --> 00:16:39,080
in the United States.

169
00:16:39,080 --> 00:16:45,880
And since then, the original distributor of the pile has stopped distributing it.

170
00:16:45,880 --> 00:16:56,040
Now you can still ask the Eleuther AI group to get the dataset, but the legal status of

171
00:16:56,040 --> 00:17:01,960
the distribution of the pile is in jeopardy, or at least it's unclear in the United States.

172
00:17:01,960 --> 00:17:08,360
But as discussions on the forum have revealed, the pile is perfectly legal in Japan.

173
00:17:08,360 --> 00:17:15,480
And because it's distributed by a nonprofit and it's distributed for nonprofit uses,

174
00:17:15,480 --> 00:17:24,720
maybe it's also legal in Europe, because Europe has reformed its copyright act a few years

175
00:17:24,720 --> 00:17:36,440
ago and they have included an explicit exception for nonprofit text and data mining, which

176
00:17:36,440 --> 00:17:39,480
is what the pile does.

177
00:17:39,480 --> 00:17:45,840
So this raises a question, like what happens when you have...

178
00:17:45,840 --> 00:17:49,560
Well, Dolma is in a similar situation.

179
00:17:49,560 --> 00:17:56,280
Initially it was released with a license that doesn't obey, it's not compliant with the

180
00:17:56,280 --> 00:17:57,960
open source definition.

181
00:17:57,960 --> 00:18:05,760
It is imposing restrictions and permissions that needed to be asked to the island institute

182
00:18:05,760 --> 00:18:07,360
before it could be used.

183
00:18:07,360 --> 00:18:09,940
And then they changed the license.

184
00:18:09,940 --> 00:18:16,240
So initially it would be not an open source dataset or an open dataset.

185
00:18:16,240 --> 00:18:20,320
It has become an open dataset now with the change of license.

186
00:18:20,320 --> 00:18:27,080
But upon looking at more closely, it contains probably the same issues that the pile has.

187
00:18:27,080 --> 00:18:34,960
So someone could sue or request Dolma to be taken down for copyright infringement in the

188
00:18:34,960 --> 00:18:36,800
United States.

189
00:18:36,800 --> 00:18:46,040
So there are these legal issues around copyright, the fact that datasets can be open at a certain

190
00:18:46,040 --> 00:18:51,640
point in time and not open at a later stage or vice versa.

191
00:18:51,640 --> 00:18:59,800
And the legality of the distribution of this dataset may change over time and changes over

192
00:18:59,800 --> 00:19:01,600
geographies.

193
00:19:01,600 --> 00:19:04,520
So calling an open...

194
00:19:04,520 --> 00:19:13,520
Anchoring the definition of open source AI to something that can change so quickly and

195
00:19:13,520 --> 00:19:19,840
can change over time is challenging in many ways.

196
00:19:19,840 --> 00:19:22,400
And there is another issue that is more technical.

197
00:19:22,400 --> 00:19:29,880
There are ways to train systems without actually creating a dataset.

198
00:19:29,880 --> 00:19:32,720
And one of these is called federated learning.

199
00:19:32,720 --> 00:19:40,840
And in federated learning, each provider of datasets is more common.

200
00:19:40,840 --> 00:19:43,760
It's common or, you know, yeah, it's common.

201
00:19:43,760 --> 00:19:47,360
It's used with privacy preserving techniques.

202
00:19:47,360 --> 00:19:52,640
If you have data that is owned by a different entity and they don't want to share it with

203
00:19:52,640 --> 00:19:56,080
others, they don't want to create a pool for different reasons.

204
00:19:56,080 --> 00:20:01,320
If they cannot create a pool because law, like privacy laws for medical records, for

205
00:20:01,320 --> 00:20:07,320
example, doesn't allow hospitals to share data of their patients, then what they can

206
00:20:07,320 --> 00:20:14,120
do is to set up training engines inside their own data centers.

207
00:20:14,120 --> 00:20:20,600
And these training engines collaborate remotely in a privacy preserving way, training a model

208
00:20:20,600 --> 00:20:24,480
without the data actually ever leaving their data centers.

209
00:20:24,480 --> 00:20:31,080
So this technique creates models, parameters, but doesn't create datasets.

210
00:20:31,080 --> 00:20:38,280
So when we were keeping this in mind, then we thought, OK, so this is a challenge and

211
00:20:38,280 --> 00:20:41,280
we need to find another way to approach it.

212
00:20:41,280 --> 00:20:48,600
And that's where we came up with the definition of data information.

213
00:20:48,600 --> 00:21:00,600
Data information is, I didn't put this in the slides, but data information is described

214
00:21:00,600 --> 00:21:10,000
in the draft, which I encourage everyone to look at.

215
00:21:10,000 --> 00:21:22,240
The draft says that data information is, the intention here is to recreate an equivalent

216
00:21:22,240 --> 00:21:24,760
system.

217
00:21:24,760 --> 00:21:34,640
And the text says sufficiently detailed information about the data used to train the system.

218
00:21:34,640 --> 00:21:40,840
So that, and this is the very important part, it's not just the information about this data

219
00:21:40,840 --> 00:21:46,540
used to train the system, but the objective here needs to be taken into consideration

220
00:21:46,540 --> 00:21:57,040
so that a skilled person can recreate a substantially equivalent system using the same or similar

221
00:21:57,040 --> 00:21:58,880
data.

222
00:21:58,880 --> 00:22:04,240
And this concept of the same or similar is important.

223
00:22:04,240 --> 00:22:08,960
And I'll give you one example.

224
00:22:08,960 --> 00:22:18,360
Let's say you have received a model that the original developer trained on Reddit.

225
00:22:18,360 --> 00:22:29,400
This was an example brought up by Tom Callaway's bot from the AWS team, who doesn't agree with

226
00:22:29,400 --> 00:22:32,400
the concept of data information.

227
00:22:32,400 --> 00:22:40,600
And he said, what if someone trained on Reddit, but licensed the data from the Reddit corporations,

228
00:22:40,600 --> 00:22:42,980
let's say for $100 million.

229
00:22:42,980 --> 00:22:50,760
So you receive now, they have disclosed the data information, and they have given you

230
00:22:50,760 --> 00:22:54,840
all the code, the source code used to train and run the system.

231
00:22:54,840 --> 00:23:02,360
Now what about the, does that qualify, would that qualify as an open source AI?

232
00:23:02,360 --> 00:23:10,880
Now, my answer to that, looking at the concept of data information, I want to be able, to

233
00:23:10,880 --> 00:23:15,480
give an answer, I need to be able to know if I can build a substantially equivalent

234
00:23:15,480 --> 00:23:19,080
system using the same or similar data.

235
00:23:19,080 --> 00:23:28,840
So the same data would require me to enter into $100 million agreement with Reddit corporation.

236
00:23:28,840 --> 00:23:34,200
But if you think about it, while thinking about it, I know that a data set called Common

237
00:23:34,200 --> 00:23:42,400
Crawl contains the Reddit data already, despite the fact that Reddit tries to remove, to have

238
00:23:42,400 --> 00:23:45,040
Common Crawl remove that data.

239
00:23:45,040 --> 00:23:46,100
But that's a different story.

240
00:23:46,100 --> 00:23:49,720
So Common Crawl has the Reddit data.

241
00:23:49,720 --> 00:23:57,560
So before I can answer to the question, whether something trained on Reddit licensed data,

242
00:23:57,560 --> 00:24:05,640
I need to extract, try to extract the Reddit data from Common Crawl, run the training.

243
00:24:05,640 --> 00:24:11,520
If the model that I find, that I obtained at the end of the training, using the same

244
00:24:11,520 --> 00:24:17,040
code from the people who have given it to me initially, if I get something that behaves

245
00:24:17,040 --> 00:24:21,340
the same way, then I would say it's an open source AI.

246
00:24:21,340 --> 00:24:31,340
If the original training data set from Reddit contains some really special source that makes

247
00:24:31,340 --> 00:24:38,840
it impossible to replicate, then it would not be an open source AI.

248
00:24:38,840 --> 00:24:42,680
And so I leave it at that.

249
00:24:42,680 --> 00:24:50,620
I want to make clear that the intention at this stage of the definition, the preferred

250
00:24:50,620 --> 00:24:56,400
form to make modifications to the system, is written in generic terms to accommodate

251
00:24:56,400 --> 00:25:06,180
the fact that sometimes data sets don't exist and sometimes data sets cannot be distributed

252
00:25:06,180 --> 00:25:14,900
or they can be distributed, but only in some geographies and only at different times.

253
00:25:14,900 --> 00:25:22,620
So it's been written by lawyers and you can recognize in there some concepts like the

254
00:25:22,620 --> 00:25:27,500
concept of the word skilled person, which is a term of art.

255
00:25:27,500 --> 00:25:36,620
And it's meant to be a generic application of the principles of open source.

256
00:25:36,620 --> 00:25:42,780
Now we have received also on the forums there have been proposals from Giulio Ferraglioli

257
00:25:42,780 --> 00:25:50,700
and Tom to use something like synthetic data instead.

258
00:25:50,700 --> 00:25:58,580
It's a fascinating example, definitely, and synthetic data is data created from scratch

259
00:25:58,580 --> 00:26:02,540
by large language models and other techniques.

260
00:26:02,540 --> 00:26:04,660
But it's really an unproven technology.

261
00:26:04,660 --> 00:26:14,460
It would be really detrimental, I think, to anchor a definition to some technology that

262
00:26:14,460 --> 00:26:20,140
is unproven, that is expensive and may not work in all cases.

263
00:26:20,140 --> 00:26:23,380
Also this doesn't leave...

264
00:26:23,380 --> 00:26:30,060
Yeah, it's unproven and it may not scale.

265
00:26:30,060 --> 00:26:32,300
So what if?

266
00:26:32,300 --> 00:26:42,300
And the other position that we have read in one of the comments, I think also this was

267
00:26:42,300 --> 00:26:49,580
coming from Giulio, is that the whole pipeline must be open source before something can be

268
00:26:49,580 --> 00:26:52,260
considered an open source system.

269
00:26:52,260 --> 00:27:00,220
And I push back a little bit on this because if the GNU project, when it started and even

270
00:27:00,220 --> 00:27:12,660
today, contemplates the option of running open source and free software, however you

271
00:27:12,660 --> 00:27:17,340
want to call it, it's the same thing, on top of Windows, for example.

272
00:27:17,340 --> 00:27:19,540
You can run...

273
00:27:19,540 --> 00:27:20,540
You can...

274
00:27:20,540 --> 00:27:29,860
The concept of free software that depends on proprietary libraries because they're part

275
00:27:29,860 --> 00:27:31,180
of the system.

276
00:27:31,180 --> 00:27:40,660
So the whole concept of complete, pure open source components in a system is not something

277
00:27:40,660 --> 00:27:44,540
that exists in nature.

278
00:27:44,540 --> 00:27:45,540
We have...

279
00:27:45,540 --> 00:27:51,580
We, the communities around the world, have always accepted compromises when that led

280
00:27:51,580 --> 00:27:54,540
to having more openness.

281
00:27:54,540 --> 00:28:05,700
And I really want to have this clear and taken into consideration when we get into this debate.

282
00:28:05,700 --> 00:28:09,020
And so these are the most important points.

283
00:28:09,020 --> 00:28:13,220
There are other considerations that we have received.

284
00:28:13,220 --> 00:28:25,620
I think, if we don't strictly require datasets, what are the incentives for other corporations

285
00:28:25,620 --> 00:28:29,740
to reveal their...

286
00:28:29,740 --> 00:28:33,980
To share, to create more open, more common datasets?

287
00:28:33,980 --> 00:28:35,500
That is a very good question.

288
00:28:35,500 --> 00:28:41,900
And in fact, I think that given the status of the legal status of policies around the

289
00:28:41,900 --> 00:28:51,820
world that make the pile or dolma complicated to share is the reason why at the OSI, together

290
00:28:51,820 --> 00:28:58,300
with Creative Commons, together with Open Future, we want to have a separate conversation

291
00:28:58,300 --> 00:29:02,900
about the issue of data datasets and policies around that.

292
00:29:02,900 --> 00:29:10,140
And we're working on a conference this year, sometime in October, to get this conversation

293
00:29:10,140 --> 00:29:12,620
started because it's a very complex one.

294
00:29:12,620 --> 00:29:17,500
We do recognize that there is an issue with creating datasets and sharing them.

295
00:29:17,500 --> 00:29:18,500
And we wanted...

296
00:29:18,500 --> 00:29:25,620
But we don't have right now clarity about how to fix this issue, how to create incentives,

297
00:29:25,620 --> 00:29:35,100
how to create concepts like copyleft inside between data and training and models.

298
00:29:35,100 --> 00:29:40,700
So that, for example, aggregators of large datasets who want to do the right thing, they

299
00:29:40,700 --> 00:29:49,420
want to create more open models, they want to share their data in a way that preserves

300
00:29:49,420 --> 00:29:55,380
the possibility for society to have access to models that can be controlled and shared

301
00:29:55,380 --> 00:29:59,380
by larger groups, not just single vendors.

302
00:29:59,380 --> 00:30:06,580
We don't know what the mechanisms are because copyright, which has helped us create copyleft,

303
00:30:06,580 --> 00:30:11,820
does not help us cross the boundary between what is data and what is the trained model.

304
00:30:11,820 --> 00:30:18,820
We need to think about contract laws, maybe, and maybe even policies like new law that

305
00:30:18,820 --> 00:30:25,020
help us tie data to train models.

306
00:30:25,020 --> 00:30:32,700
It's a complex topic, and that's why I think we're having such a tremendous amount of discussions

307
00:30:32,700 --> 00:30:39,700
over two years of the investigations and then later the co-design process around the open

308
00:30:39,700 --> 00:30:41,300
source AI definition.

309
00:30:41,300 --> 00:30:48,260
All the hardest conversations have been around this concept of what about the data?

310
00:30:48,260 --> 00:30:50,260
And I think we need to...

311
00:30:50,260 --> 00:30:55,820
We really need to come to the conclusion that this is a very complex topic and that needs

312
00:30:55,820 --> 00:30:56,820
to be...

313
00:30:56,820 --> 00:31:00,220
The data issue needs to be taken in a separate trial.

314
00:31:00,220 --> 00:31:04,900
And the other thing that I want to say is that the concept of data information, if you

315
00:31:04,900 --> 00:31:10,540
read the definition and start from the end, start from the end, what is the intention?

316
00:31:10,540 --> 00:31:17,020
The intention is to recreate a substantially equivalent system, whether you are using the

317
00:31:17,020 --> 00:31:21,700
same data, exact data, because it's available, or whether you're using similar data because

318
00:31:21,700 --> 00:31:30,820
one way or another you find it, but you end up with the same model, same behavior, then

319
00:31:30,820 --> 00:31:34,060
that should be enough in the definition.

320
00:31:34,060 --> 00:31:36,580
And by the way, so let's look at what...

321
00:31:36,580 --> 00:31:37,580
Let's move on.

322
00:31:37,580 --> 00:31:42,220
Let's look at what we have done to validate this hypothesis because that's how we've been

323
00:31:42,220 --> 00:31:43,220
operating.

324
00:31:43,220 --> 00:31:44,540
Let's look at examples.

325
00:31:44,540 --> 00:31:48,300
Let's see what's happening out there in the wild.

326
00:31:48,300 --> 00:31:49,780
We validated...

327
00:31:49,780 --> 00:31:51,140
We went through a few systems.

328
00:31:51,140 --> 00:31:52,140
We asked people to...

329
00:31:52,140 --> 00:31:54,460
A few, more than...

330
00:31:54,460 --> 00:31:56,180
Now we are at 12.

331
00:31:56,180 --> 00:32:00,500
So we're trying to look at what happens with these systems.

332
00:32:00,500 --> 00:32:10,020
Some we know, they are not passing the bar, like Lama and Mistral and Falcon and Grok.

333
00:32:10,020 --> 00:32:15,500
Where does the definition fail?

334
00:32:15,500 --> 00:32:18,820
We find that it's hard to find the components, right?

335
00:32:18,820 --> 00:32:20,860
With people who don't know about it.

336
00:32:20,860 --> 00:32:26,060
So for example, if you look at Grok, Grok is one perfect example.

337
00:32:26,060 --> 00:32:35,020
It was developed by XAI, the Elon Musk thing, and they just share the model weights very

338
00:32:35,020 --> 00:32:36,380
freely and nothing else.

339
00:32:36,380 --> 00:32:41,420
Now if you don't find the other components, is it because they don't exist, which is probably

340
00:32:41,420 --> 00:32:45,100
most likely, or is it because you couldn't find it?

341
00:32:45,100 --> 00:32:49,700
The other thing is it's hard to understand that licenses sometimes if you don't have...

342
00:32:49,700 --> 00:32:55,180
I mean, the volunteers, they may have limited knowledge about the terms of use and terms

343
00:32:55,180 --> 00:32:57,620
of distribution.

344
00:32:57,620 --> 00:33:00,220
So some of these reviews have been incomplete.

345
00:33:00,220 --> 00:33:07,820
But despite this, we ended up, I think that we get a very easy sense.

346
00:33:07,820 --> 00:33:12,620
We know that Falcon is missing information about 3M technology.

347
00:33:12,620 --> 00:33:17,340
Their licenses have been modified.

348
00:33:17,340 --> 00:33:20,220
They're not really compliant with the open source definition.

349
00:33:20,220 --> 00:33:22,380
Grok, we know it's opaque.

350
00:33:22,380 --> 00:33:28,540
Lama, we know it fails because of a variety of reasons, like lack of transparency, but

351
00:33:28,540 --> 00:33:31,100
also it's missing other...

352
00:33:31,100 --> 00:33:33,020
The license is not compliant.

353
00:33:33,020 --> 00:33:37,060
But we know Alma, for example, they've been doing the right thing, and we expect it to

354
00:33:37,060 --> 00:33:41,660
be a positive, passing the bar of open source AI.

355
00:33:41,660 --> 00:33:44,660
By the way, they also released the data set.

356
00:33:44,660 --> 00:33:46,700
And the similar is for PTA.

357
00:33:46,700 --> 00:33:50,860
They're fully transparent, and they released the data set.

358
00:33:50,860 --> 00:33:55,860
You can ask for it, and you can get it also.

359
00:33:55,860 --> 00:34:02,780
But the issue remains with legal uncertainties around the world.

360
00:34:02,780 --> 00:34:05,980
The other, Bloom, we know that we have everything.

361
00:34:05,980 --> 00:34:11,500
It's transparent, but it fails because the license it uses for many of its components

362
00:34:11,500 --> 00:34:14,620
are imposing restrictions and things like that.

363
00:34:14,620 --> 00:34:23,580
So the concept of data information seems to be behaving exactly as expected.

364
00:34:23,580 --> 00:34:27,220
And it's showing also that there is a very strong correlation.

365
00:34:27,220 --> 00:34:32,460
Granted, it's a small sample, but there is a strong correlation between requiring data

366
00:34:32,460 --> 00:34:38,220
information and having access to the data set to caveat those legal issues.

367
00:34:38,220 --> 00:34:47,300
So I think it's working, and I'm not convinced that the alternative proposals are positive,

368
00:34:47,300 --> 00:34:58,700
because the alternative proposals put PTA and even Olmo outside of the approved licenses.

369
00:34:58,700 --> 00:35:01,540
And that is really not an acceptable outcome.

370
00:35:01,540 --> 00:35:11,020
We cannot go credibly to either commercial partners, academia, and policymakers and say,

371
00:35:11,020 --> 00:35:16,140
"This is the open source AI definition," and not have an answer when they say, "Okay,

372
00:35:16,140 --> 00:35:18,380
which one is open source AI?"

373
00:35:18,380 --> 00:35:26,180
And we cannot point at any examples, or we can only point at small, trivial academic

374
00:35:26,180 --> 00:35:30,220
experiments as examples.

375
00:35:30,220 --> 00:35:31,900
It's not going to work.

376
00:35:31,900 --> 00:35:39,260
And it's not going to work because the industry and policymakers are already being pushed

377
00:35:39,260 --> 00:35:44,660
to look at Lama and Mistral, and they consider those open source.

378
00:35:44,660 --> 00:35:52,820
So if we don't come up with a counterproposal quickly, we will have lost an opportunity

379
00:35:52,820 --> 00:35:54,180
here.

380
00:35:54,180 --> 00:35:55,940
So what's next?

381
00:35:55,940 --> 00:36:03,260
I really hope that we resolve these comments and we resolve this conversation around the

382
00:36:03,260 --> 00:36:11,340
concept of data information with a release in 0.9, which we get support from organizations

383
00:36:11,340 --> 00:36:17,300
that understand the principles behind it and validate it.

384
00:36:17,300 --> 00:36:26,140
We started to get some positive feedback this week at a conference in Paris from Lina Gora

385
00:36:26,140 --> 00:36:33,060
and publicly announced support for this concept, and others are coming in.

386
00:36:33,060 --> 00:36:38,740
And then between July and October, we're going to have a series of release candidates with

387
00:36:38,740 --> 00:36:41,580
trying to get more endorsements.

388
00:36:41,580 --> 00:36:45,260
So there are two ways for you to help.

389
00:36:45,260 --> 00:36:53,660
One is to look, keep on searching for systems that seem to be complying with the definition

390
00:36:53,660 --> 00:36:59,940
0.8 or not, like complete, go on with the validation phase.

391
00:36:59,940 --> 00:37:06,540
And yeah, and this is the timeline, and these are the things that we are, the places where

392
00:37:06,540 --> 00:37:12,140
we're going to be speaking next and presenting and discussing with the community.

393
00:37:12,140 --> 00:37:15,180
And as usual, try to join the forums.

394
00:37:15,180 --> 00:37:20,320
I know I may have said in the past, also give us feedback through social media channels.

395
00:37:20,320 --> 00:37:26,940
If you do that, please tag us because the algorithms on LinkedIn, some people are still

396
00:37:26,940 --> 00:37:29,240
using X, et cetera.

397
00:37:29,240 --> 00:37:31,040
We miss comments.

398
00:37:31,040 --> 00:37:32,040
We miss the discussions.

399
00:37:32,040 --> 00:37:38,400
They're very hard to find, and instead, the forums are the perfect place.

400
00:37:38,400 --> 00:37:46,160
And of course, join the town halls because it's a time when we can ask live Q&A.

401
00:37:46,160 --> 00:37:53,040
And with that, I will stop the speaking and see if there are any questions.

402
00:37:53,040 --> 00:38:02,480
Well, I see that there's been quite a jump on the written form.

403
00:38:02,480 --> 00:38:07,440
Trying to summarize.

404
00:38:07,440 --> 00:38:12,280
Is there, do you want to?

405
00:38:12,280 --> 00:38:20,600
Yeah, I think you all have voice rights.

406
00:38:20,600 --> 00:38:31,320
You should be able to speak.

407
00:38:31,320 --> 00:38:39,440
Do I have a path on the open source AI definition where Facebook's llama goes green on the list?

408
00:38:39,440 --> 00:38:40,440
Yeah.

409
00:38:41,440 --> 00:38:52,400
They release all of their training information, training data, and we can rebuild something

410
00:38:52,400 --> 00:38:55,400
similar like that.

411
00:38:55,400 --> 00:38:57,400
Yeah.

412
00:38:57,400 --> 00:39:01,320
Yeah, exactly.

413
00:39:01,320 --> 00:39:07,520
I got to say, conversations with commercial operators, I think that they tell me that

414
00:39:07,520 --> 00:39:21,040
the secret sauce is actually in the training techniques because they seem to, that's where

415
00:39:21,040 --> 00:39:22,040
the secret goes.

416
00:39:22,040 --> 00:39:27,600
It looks to me, they tell me that that's where their secrets are.

417
00:39:27,600 --> 00:39:36,400
How they score high on the leaderboards for benchmarks is how they train, and they don't

418
00:39:36,400 --> 00:39:40,600
want to share it.

419
00:39:40,600 --> 00:39:46,080
But I spoke with, for example, Lina Gora, they have this project called OpenLLM360,

420
00:39:46,080 --> 00:39:52,280
France OpenLLM360, something like that.

421
00:39:52,280 --> 00:39:57,800
And they've been training a system from scratch and they are releasing all of their information

422
00:39:57,800 --> 00:40:02,800
and data, et cetera, because they want to do the right thing and they want to [inaudible

423
00:40:02,800 --> 00:40:07,280
00:10:50] a model that is optimized for the French language.

424
00:40:07,280 --> 00:40:15,840
So that other, and then they want to generate that collaboration on top of that.

425
00:40:15,840 --> 00:40:25,120
So Honey Sabak comments that if we think the definitions settles on the training data method

426
00:40:25,120 --> 00:40:29,880
must be open as well, then we may end up with few or no open source AI models.

427
00:40:29,880 --> 00:40:31,600
It's one of the risks.

428
00:40:31,600 --> 00:40:37,560
But also I want to point out that there is a little bit more than that.

429
00:40:37,560 --> 00:40:43,160
The reason why I want to move on with the conversation, because it's a highlight how

430
00:40:43,160 --> 00:40:47,920
complicated this is and how different from software this is.

431
00:40:47,920 --> 00:40:52,960
On software, when you modify, you get access to the source code, you modify it and you

432
00:40:52,960 --> 00:40:58,200
have to rebuild before you can ship it again.

433
00:40:58,200 --> 00:41:02,400
So the concept of modification and studying are like that.

434
00:41:02,400 --> 00:41:10,160
You study, you see the source code and you modify, rebuild and ship it.

435
00:41:10,160 --> 00:41:12,280
For AI, you don't do this.

436
00:41:12,280 --> 00:41:21,880
You can study just for the purpose to see if there are bugs you don't need to rebuild

437
00:41:21,880 --> 00:41:30,880
or if there are issues, biases, et cetera, to evaluate a model around AI.

438
00:41:30,880 --> 00:41:36,280
But for modifications, you have multiple ways of achieving the same goal without having

439
00:41:36,280 --> 00:41:44,220
to retrain, which really is a much more interesting question to me than the debate about data.

440
00:41:44,220 --> 00:41:53,260
How do we treat models that are fully disclosed, share their datasets, share the techniques

441
00:41:53,260 --> 00:42:00,740
for the training when that training is fine tuning on proprietary models?

442
00:42:00,740 --> 00:42:12,580
And I'll say more, that fine tuning is so deep that every layer of the neural network

443
00:42:12,580 --> 00:42:21,100
has been rewritten so deeply that none of the behaviors and the benchmarks from the

444
00:42:21,100 --> 00:42:26,020
original model apply to the retrained, fine-tuned model.

445
00:42:26,020 --> 00:42:30,980
That is a huge question that we need to find an answer for today.

446
00:42:30,980 --> 00:42:40,140
And it's been raised on the forum with an example of an AI system developed by Mozilla

447
00:42:40,140 --> 00:42:49,620
to write captions for descriptions of images in the PDF.js tool.

448
00:42:49,620 --> 00:42:53,180
And they mixed two proprietary models.

449
00:42:53,180 --> 00:43:04,020
One is an object detection vision, computer vision model, and one is GPT-2.

450
00:43:04,020 --> 00:43:08,020
It's a large language model.

451
00:43:08,020 --> 00:43:09,540
We don't know anything about the training.

452
00:43:09,540 --> 00:43:11,580
They have biases, etc.

453
00:43:11,580 --> 00:43:18,460
Mozilla has fine-tuned these models, assembled them together to achieve a new system, a new

454
00:43:18,460 --> 00:43:19,460
behavior.

455
00:43:19,460 --> 00:43:21,980
It's that open source.

456
00:43:21,980 --> 00:43:27,660
And they have released everything that they have done in a very reproducible fashion.

457
00:43:27,660 --> 00:43:29,780
So what are we going to call this?

458
00:43:29,780 --> 00:43:33,420
Is this open source AI built on top of non-open models?

459
00:43:33,420 --> 00:43:34,420
Yes.

460
00:43:34,420 --> 00:43:35,420
Interesting question.

461
00:43:35,420 --> 00:43:36,420
Okay.

462
00:43:36,420 --> 00:43:37,420
So, let's go back to the slides.

463
00:43:37,420 --> 00:43:38,420
So, I'm going to show you a few slides.

464
00:43:38,420 --> 00:43:39,420
And I'm going to show you a few examples.

465
00:43:39,420 --> 00:43:40,420
So, let's go back to the slides.

466
00:43:40,420 --> 00:43:41,420
So, this is the first slide.

467
00:43:41,420 --> 00:43:42,420
And this is the second slide.

468
00:43:42,420 --> 00:43:43,420
And this is the third slide.

469
00:43:43,420 --> 00:43:44,420
And this is the fourth slide.

470
00:43:44,420 --> 00:43:45,420
And this is the fifth slide.

471
00:43:45,420 --> 00:43:46,420
And this is the sixth slide.

472
00:43:46,420 --> 00:43:47,420
And this is the seventh slide.

473
00:43:47,420 --> 00:43:48,420
And this is the eighth slide.

474
00:43:48,420 --> 00:43:49,420
And this is the ninth slide.

475
00:43:49,420 --> 00:43:50,420
And this is the tenth slide.

476
00:43:50,420 --> 00:43:51,420
And this is the twelfth slide.

477
00:43:51,420 --> 00:43:52,420
And this is the eighth slide.

478
00:43:52,420 --> 00:43:53,420
And this is the ninth slide.

479
00:43:53,420 --> 00:43:54,420
And this is the twelfth slide.

480
00:43:54,420 --> 00:43:55,420
And this is the tenth slide.

481
00:43:56,420 --> 00:43:57,420
And this is the eleventh slide.

482
00:43:57,420 --> 00:43:58,420
And this is the twelfth slide.

483
00:43:58,420 --> 00:43:59,420
And this is the thirteenth slide.

484
00:43:59,420 --> 00:44:00,420
And this is the fifteenth slide.

485
00:44:00,420 --> 00:44:01,420
And this is the sixteenth slide.

486
00:44:01,420 --> 00:44:02,420
And this is the seventeenth slide.

487
00:44:02,420 --> 00:44:03,420
And this is the eighteenth slide.

488
00:44:03,420 --> 00:44:04,420
And this is the twelfth slide.

489
00:44:04,420 --> 00:44:05,420
And this is the seventeenth slide.

490
00:44:05,420 --> 00:44:06,420
And this is the twelfth slide.

491
00:44:06,420 --> 00:44:07,420
And this is the eighteenth slide.

492
00:44:07,420 --> 00:44:08,420
And this is the twelfth slide.

493
00:44:08,420 --> 00:44:09,420
And this is the seventeenth slide.

494
00:44:09,420 --> 00:44:10,420
And this is the eighteenth slide.

495
00:44:10,420 --> 00:44:11,420
And this is the twelfth slide.

496
00:44:11,420 --> 00:44:12,420
And this is the eighteenth slide.

497
00:44:12,420 --> 00:44:13,420
And this is the twelfth slide.

498
00:44:13,420 --> 00:44:14,420
And this is the eighteenth slide.

499
00:44:14,420 --> 00:44:15,420
And this is the twelfth slide.

500
00:44:15,420 --> 00:44:16,420
And this is the eighteenth slide.

501
00:44:16,420 --> 00:44:37,420
And this is the twelfth slide.

502
00:44:37,420 --> 00:45:03,420
And this is the eighteenth slide.

503
00:45:03,420 --> 00:45:08,420
And this is the twelfth slide.

504
00:45:08,420 --> 00:45:09,420
And this is the eighteenth slide.

505
00:45:09,420 --> 00:45:10,420
And this is the twelfth slide.

506
00:45:10,420 --> 00:45:11,420
And this is the eighteenth slide.

507
00:45:11,420 --> 00:45:12,420
And this is the twelfth slide.

508
00:45:12,420 --> 00:45:13,420
And this is the eighteenth slide.

509
00:45:13,420 --> 00:45:14,420
And this is the twelfth slide.

510
00:45:15,420 --> 00:45:16,420
And this is the eighteenth slide.

511
00:45:16,420 --> 00:45:17,420
And this is the twelfth slide.

512
00:45:27,420 --> 00:45:34,420
And this is the twelfth slide.

513
00:45:34,420 --> 00:45:35,420
And this is the twelfth slide.

### End of last town hall held on 2024-06-14 ###

### Start of next town hall held on 2024-06-28 ###
--- Presentation for 2024-06-28 ---
OPEN SOURCE AI DEFINITION
Online public townhall
June 28, 2024
last updated: June 25, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

OSI’s objective for 2024

Open Source AI Definition

3

Open Source AI Deﬁnition

Current Version
OSAID v.0.0.8

4

Open
Source AI
Definition

Preamble
4 Freedoms

v.0.0.8

Legal Checklist

-

Open Source AI Deﬁnition

What We’re Working On
OSAID v.0.0.9

6

Open
Source AI
Definition
Preamble
v.0.0.9 plans
Clarifying that
the recipients
of the freedoms
are developers,
deployers and
end-users

Open
Source AI
Definition
Four
Freedoms
v.0.0.9 plans

Clarifying that the
four freedoms of
open source AI are
derived from the
Free Software
Definition

Open
Source AI
Definition
Four
Freedoms
v.0.0.9 plans

Underlining that
components and
systems must be
free from
encumbrances that
prevent any
developer, deployer,
or users from
exercising those
freedoms.

Open Source
AI Definition
Preferred
Form
v.0.0.9 plans

Adding definitions of…
… the “OSD
compliant”
requirement for data
information...
…and the “OSD
conformant”
requirement for model
parameters
..so legal
requirements are
clear for each
component

Open
Source AI
Definition
Checklist
v.0.0.9 plans

Checklist will be a
separate document
and process and its
components will be
updated to follow the
Model Openness
Framework (MOF)
precisely.

Open Source AI Deﬁnition

System Validation

OSAID v.0.0.8 (and soon v. 0.0.9)

12

Validation Updates

Thanks to Arctic and
LLM360 for helping identify
documentation!

13

Open Source AI Deﬁnition

What’s Next?

June - October 2024

● Complete validation phase
● Resolve comments, release v. 0.0.9 after
validation
● Cut the release candidate with sufficient
endorsement
14

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

June

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.8

July

August

September

Virtual System
Review

Virtual System
Review

Virtual System
Review Ends

Townhalls +

Townhalls +

Townhalls +

Townhalls +

- OSPOs for
Good (NYC)
- Sustain
Africa (virtual)

- AI-dev (Hong
Kong)

- Nerdearla
(Buenos Aires)

- All Things
Open (Raleigh)
- Data Workshop
(Europe TBD)

Draft 0.0.9

RC1

RC1

October

Stable
Version

In-Person Meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

✓ PyCon US

May 17

Europe

France

Paris

✓ OW2

June 11 - 12

North America

United States

New York

OSPOs for Good

July 9 - 11

Africa

Virtual

Virtual

Sustain Africa

July 15

Asia Paciﬁc

China

Hong Kong

AI_dev

August 23

Latin America

Argentina

Buenos Aires

Nerdearla

September 24 - 28

Europe

TBD

TBD

(data governance)

October

North America

United States

Raleigh

All Things Open

Oct 27 - 29
16

How to Participate :)
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
● Volunteer to help with validation (email or DM Mer
Joyce)
17

Q&A

18

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the deﬁnition.

19


--- Subtitles for 2024-06-28 --- ###
1
00:00:00,001 --> 00:00:08,440
to start the recording. Thank you. Yes. So welcome everyone to our biweekly town hall

2
00:00:08,440 --> 00:00:14,200
for the open source AI definition process. And you can hear I've got a little bit of

3
00:00:14,200 --> 00:00:22,500
a sore throat, but I'll hopefully be able to make it through. I will start. So these

4
00:00:22,500 --> 00:00:26,680
are our community agreements that we have at every meeting. Some of you may have already

5
00:00:26,680 --> 00:00:34,840
seen these. One mic, one speaker is about non-interrupting. Also when we get to the

6
00:00:34,840 --> 00:00:41,320
Q&A, if you have multiple questions, please ask your question and then pause and let at

7
00:00:41,320 --> 00:00:47,440
least one other person ask their own question before you ask a second question. Take space,

8
00:00:47,440 --> 00:00:53,320
make space is a similar spirit. You know, just to invite others to share space with

9
00:00:53,320 --> 00:01:01,760
us. And that some feel more shy and some feel more extroverted and everyone's voice matters.

10
00:01:01,760 --> 00:01:06,800
Kindness is just that the work is hard, but we don't have to be. And just to remind ourselves

11
00:01:06,800 --> 00:01:12,160
to be gentle and curious with each other, even when we disagree. Forward motion means

12
00:01:12,160 --> 00:01:18,520
focusing on what's possible and not letting obstacles prevent the process from moving

13
00:01:18,520 --> 00:01:24,680
forward. Similarly, solution seeking, just this work is very complex and it's vulnerable

14
00:01:24,680 --> 00:01:30,960
to suggest a solution, but that is how we move forward. And anything else that people

15
00:01:30,960 --> 00:01:37,320
would like to add to our community agreements for this meeting? You can say it in the chat

16
00:01:37,320 --> 00:01:47,400
if you want. Okay. I'll continue. So yes, we are creating an open source AI definition

17
00:01:47,400 --> 00:01:57,600
this year. And the current version is still 0.0.8. That's been the case for about a month.

18
00:01:57,600 --> 00:02:06,240
And these are the pieces of it. Maybe Nick could drop the link to the HackMD page into

19
00:02:06,240 --> 00:02:13,520
the chat because this is very hard to read. It just shows you the parts kind of as a map.

20
00:02:13,520 --> 00:02:21,680
So we have a preamble. We have the four freedoms, studies, modify, and share as applied to AI.

21
00:02:21,680 --> 00:02:29,120
And then we have the it's not circled, but we have preferred form for data information,

22
00:02:29,120 --> 00:02:34,200
code and model. There's some description there. And then the current version also has a legal

23
00:02:34,200 --> 00:02:41,360
thank you, Nick, has a legal checklist of what the licenses would be on the specific

24
00:02:41,360 --> 00:02:49,120
components that would be required in those three categories of data information, code

25
00:02:49,120 --> 00:02:56,160
and model. And what we're working on is versions. Yes. And we're open to comments. That's right.

26
00:02:56,160 --> 00:03:04,720
And you can actually comment on that document. HackMD is a commenting platform. So what we're

27
00:03:04,720 --> 00:03:12,840
working on right now is 0.0.9. And there will be some changes, a few changes. So in the

28
00:03:12,840 --> 00:03:19,080
preamble, we are clarifying that recipients of the freedoms are developers, players, and

29
00:03:19,080 --> 00:03:27,200
end users. So those freedoms of study, use, modification, and sharing. We are crediting

30
00:03:27,200 --> 00:03:35,760
the Free Software Foundation for initially developing these four freedoms because crediting

31
00:03:35,760 --> 00:03:42,120
people is a good thing to do. And so we're adding that in. And you can see in this larger

32
00:03:42,120 --> 00:03:51,200
box how those freedoms are enumerated or described. The language should not be surprising to anyone.

33
00:03:51,200 --> 00:03:56,400
And we did develop these four freedoms in a series of co-design workshops at the end

34
00:03:56,400 --> 00:04:08,320
of last year. Also in the four freedoms, we are going to, in 0.0.9, underline that the

35
00:04:08,320 --> 00:04:17,120
components must be free from encumbrance. That prevents any of those three user types,

36
00:04:17,120 --> 00:04:22,400
developer, deployer, or end user from exercising the freedoms. So just underlining that, yes,

37
00:04:22,400 --> 00:04:28,080
the four freedoms must be respected. And also if you're a little confused, you're not seeing

38
00:04:28,080 --> 00:04:34,600
0.0.9 in these images. You're seeing 0.0.8. But I'm just indicating where the changes

39
00:04:34,600 --> 00:04:40,640
will be. Also in preferred form, we are going to add

40
00:04:40,640 --> 00:04:48,480
definitions which will be just a phrase, not a sentence, for the terms OSD compliant, which

41
00:04:48,480 --> 00:04:53,120
is a requirement of data information, and OSD conformant, which is a requirement of

42
00:04:53,120 --> 00:05:00,900
model parameters so that the legal requirements are clear. And the code components just need

43
00:05:00,900 --> 00:05:08,880
to be licensed under an OSI approved license. So that's very straightforward. The code or

44
00:05:08,880 --> 00:05:14,480
software can exist under those licenses. And the licensing and legal requirements are slightly

45
00:05:14,480 --> 00:05:20,360
different for those other types of components. And so we're going to define what these terms

46
00:05:20,360 --> 00:05:26,760
mean, compliant and conformant, the next version. And then, oh, someone unmuted themselves.

47
00:05:26,760 --> 00:05:30,760
Please just stand by until the Q&A. If you have a question that you'd like to ask before

48
00:05:30,760 --> 00:05:39,520
the Q&A, you can drop it in the chat. Okay. So the checklist. The checklist, we are in

49
00:05:39,520 --> 00:05:46,240
the next version going to actually move it into a separate document. We realized that

50
00:05:46,240 --> 00:05:51,440
in trying to create the definition and also to operationalize the definition in the same

51
00:05:51,440 --> 00:05:57,880
process was a bit like jogging and juggling at the same time. And so we thought, let's

52
00:05:57,880 --> 00:06:03,640
just focus on the definition. Let's basically the definition will stop at preferred form.

53
00:06:03,640 --> 00:06:08,320
And then obviously operationalization of the definition is quite important. And that is

54
00:06:08,320 --> 00:06:15,720
the checklist. And we're just going to separate those documents and also those processes.

55
00:06:15,720 --> 00:06:21,600
One change that you will see in the checklist in version 9, and I guess we'll figure out

56
00:06:21,600 --> 00:06:26,000
how to label the versions of these documents as well. But if you're not, we will figure

57
00:06:26,000 --> 00:06:32,880
that out. It will be updated so that all the components are from the model openness framework.

58
00:06:32,880 --> 00:06:38,800
So right now, if you look on the left, the data information components are coming from

59
00:06:38,800 --> 00:06:47,320
the EU AI Act. So training methodologies and techniques, training, data scope and characteristics,

60
00:06:47,320 --> 00:06:52,560
training data providence, etc. Those are not coming from the model openness framework,

61
00:06:52,560 --> 00:06:58,120
which is a list of components from the Linux Foundation. It's coming from the EU AI Act.

62
00:06:58,120 --> 00:07:04,400
And we just because of all the great work that the Linux Foundation is doing to create

63
00:07:04,400 --> 00:07:13,240
an online compendium of AI systems and the openness of their components, we really want

64
00:07:13,240 --> 00:07:19,880
to be able to rely on that for our own definition. And so we are going to use their component

65
00:07:19,880 --> 00:07:26,680
list exclusively in our in our checklist. And I'll just read Stefano's comment, the

66
00:07:26,680 --> 00:07:33,440
data information piece is going to look the same in 0.0.9. Oh, not because it's decided,

67
00:07:33,440 --> 00:07:42,600
but because the topic is still being discussed. Okay, got it. So okay, so we're not yet. So

68
00:07:42,600 --> 00:07:52,400
eventually, we will be transitioning over to model openness framework components. That's

69
00:07:52,400 --> 00:08:01,480
not happening in 0.0.9. Thank you for the clarification. Okay, system validation. It's

70
00:08:01,480 --> 00:08:07,360
pretty much the same as last time. So thank you to Arctic and LLM 360 for helping to identify

71
00:08:07,360 --> 00:08:14,840
documentation, we found that it's really crucial to have creators help out with identifying

72
00:08:14,840 --> 00:08:20,560
the legal documents describing the rights and permissions associated with the components

73
00:08:20,560 --> 00:08:31,000
of their systems. So this screenshot of the progress and validation based on the process

74
00:08:31,000 --> 00:08:39,000
of the review, which is being done by volunteers. And also the the results of that review so

75
00:08:39,000 --> 00:08:49,360
far have not have not changed since last time. And yeah, we just find that we do need creators

76
00:08:49,360 --> 00:08:56,800
to help provide documentation in order to to know whether a system would meet the requirements

77
00:08:56,800 --> 00:09:04,720
of the open source AI definition. And Stefano's typing. I'll just wait for that if it's on

78
00:09:04,720 --> 00:09:15,760
validation. Okay, pause. I'll read I'll read the comments if it comes up. Okay, so what's

79
00:09:15,760 --> 00:09:21,440
next June, which we're almost at the end of through October, we do we want to obviously

80
00:09:21,440 --> 00:09:29,680
complete the validation phase resolve comments and release versions 0.7.9. And then cut the

81
00:09:29,680 --> 00:09:42,320
release candidate with sufficient endorsement, organizational endorsement. Okay, just keep

82
00:09:42,320 --> 00:09:49,680
going. And this is our timeline, we had to update our timeline, because it ended at June,

83
00:09:49,680 --> 00:09:56,920
I believe. And now we have to think through to the end of the project in October. So we

84
00:09:56,920 --> 00:10:06,200
will be at hospitals for good in New York, both the UN event and the side event. We will

85
00:10:06,200 --> 00:10:16,760
this is in July, I'll be doing a virtual event for sustain Africa. And this is to share the

86
00:10:16,760 --> 00:10:26,680
OSAID and also to get further feedback. Let me pause and read what Stefano is writing.

87
00:10:26,680 --> 00:10:32,200
The 0.0.9 draft includes a lot of small changes accumulated over two months since the release

88
00:10:32,200 --> 00:10:37,120
of the previous draft, but all the pieces of 0.0.9 can still change based on community

89
00:10:37,120 --> 00:10:49,840
feedback. Okay, that makes sense. Okay, so August, we'll be in Hong Kong for AI dev. And

90
00:10:49,840 --> 00:10:56,800
September. Gosh, let's see if I can pronounce this correctly with my cold. Merdeala. Okay,

91
00:10:56,800 --> 00:11:04,240
that's pretty good. And Buenos Aires. And then October, we will be launching the stable

92
00:11:04,240 --> 00:11:11,080
version of the definition at all things open in Raleigh in the US. And then we will also

93
00:11:11,080 --> 00:11:19,840
be doing a data workshop in Europe, in a city TBD. And the focus of that workshop, it will

94
00:11:19,840 --> 00:11:27,440
be to write a policy paper to try to resolve some of these challenges with the sharing

95
00:11:27,440 --> 00:11:41,240
of data sets. Yeah. Yeah. And as Stefano is saying, nothing is set in stone yet. Okay.

96
00:11:41,240 --> 00:11:46,920
So yes, and actually, you've actually already seen most of these. Yep. The different events

97
00:11:46,920 --> 00:11:55,240
that we're going to and when they are. So how to participate in this process. We do

98
00:11:55,240 --> 00:12:06,680
share updates on the process and opportunities for volunteering also discuss issues of you

99
00:12:06,680 --> 00:12:17,360
know, disagreement and difference of opinion on the public forum, which is discussed on

100
00:12:17,360 --> 00:12:23,400
open source.org, which you can join for free, you do have to sign up just to prevent spam.

101
00:12:23,400 --> 00:12:29,160
And then we have these bi weekly town halls, both for the this is one for the Europe, North

102
00:12:29,160 --> 00:12:37,320
America and our Americas time zone. And then we have a second one. That is Europe, Africa

103
00:12:37,320 --> 00:12:44,320
and Asia. So we cycle back between those two times. And then yes, if you would like to

104
00:12:44,320 --> 00:12:49,600
volunteer a validation, particularly I think about volunteering we need at this point is

105
00:12:49,600 --> 00:12:59,160
if you are the creator of a an AI system and would like to, you know, show that it is open

106
00:12:59,160 --> 00:13:05,660
according to the definition that we have now, that that would be the most valuable type

107
00:13:05,660 --> 00:13:13,080
of volunteer just because those are those are the those individuals that have this documentation

108
00:13:13,080 --> 00:13:21,280
information most at hand. Yes, there's also a blog that we update and I can share that

109
00:13:21,280 --> 00:13:27,800
and their summaries are shared every Monday on the blog. And yes, we highly recommend

110
00:13:27,800 --> 00:13:37,760
the weekly summaries. So yeah, we will now do a Q&A. And what you can do is, I think,

111
00:13:37,760 --> 00:13:42,880
raise, raise your hand and you can come off mute or you can just ask in the chat and I'll

112
00:13:42,880 --> 00:14:09,160
read it like I've been doing throughout the meeting up to you. Love to hear your thoughts.

113
00:14:09,160 --> 00:14:14,160
And for all those who who are not as familiar with our organization, Stefano, whose shots

114
00:14:14,160 --> 00:14:20,040
I was reading is the executive director of the OSI. So if anyone was wondering why, why

115
00:14:20,040 --> 00:14:24,880
is she reading aloud all these comments from this one participant? That is why he can't

116
00:14:24,880 --> 00:14:37,520
participate live today. But that's why I was reading his comments. So yeah, I will just

117
00:14:37,520 --> 00:14:44,360
be shy for a bit. Yeah, don't be shy. Yeah, don't be shy. You can actually click on the

118
00:14:44,360 --> 00:14:53,720
microphone and speak or you can just chat as well. It's OK. Yes. OK. Oh, you're welcome,

119
00:14:53,720 --> 00:15:11,040
Stefano. Yeah, thank you so much for coming. OK, yeah, go ahead, Gerardo, you can unmute

120
00:15:11,040 --> 00:15:26,240
yourself. Hi. Yes. What's your question? I've been participating in several standard committees

121
00:15:26,240 --> 00:15:37,360
on IEEE about the ethical use of AI and several AI definitions, and I'm finding that most

122
00:15:37,360 --> 00:15:52,120
of the people I met there have not yet, do not yet know about this initiative and I'm

123
00:15:52,120 --> 00:16:05,360
changing that. But I've been wondering if we don't need to push this discussion a little

124
00:16:05,360 --> 00:16:16,600
bit forward with the scientific community, especially with certain researchers that are

125
00:16:16,600 --> 00:16:30,000
dealing with this on several bases. I think I've covered most of the AI ethics discussions

126
00:16:30,000 --> 00:16:41,040
that are occurring, but it seems to me there's more going on that probably we should be a

127
00:16:41,040 --> 00:16:53,400
part of. OK, yeah, I may not be the person to respond to that. My primary role is or

128
00:16:53,400 --> 00:17:01,720
my role is running the co-design process of the definition itself within OSI. But Stefano's

129
00:17:01,720 --> 00:17:08,760
typing, but that sounds very, does sound very useful. And thank you for bringing those experiences

130
00:17:08,760 --> 00:17:14,840
you had in other organization and standards making processes here. That's very, very helpful

131
00:17:14,840 --> 00:17:23,320
to us. Gerardo, so we're working with several researchers and several organizations as well

132
00:17:23,320 --> 00:17:34,200
and we would love if you have contacts with other researchers with whom we could work

133
00:17:34,200 --> 00:17:39,720
with, that would be splendid. I just would like to highlight something because you mentioned

134
00:17:39,720 --> 00:17:47,800
ethics, right? So even though, yes, ethics is really important for us, we want to see

135
00:17:47,800 --> 00:17:57,880
open source AI being used for good, right? For the benefit of humanity. But ethics, as

136
00:17:57,880 --> 00:18:04,280
the open source AI definition is concerned, is out of scope. It doesn't mean it doesn't

137
00:18:04,280 --> 00:18:12,640
matter. It does matter that open source AI is used for the good, but it's just something

138
00:18:12,640 --> 00:18:19,280
out of scope. I'm not sure if that clarifies your question.

139
00:18:19,280 --> 00:18:31,440
Well, it's more on the way that for most of the discussions we are having, some of them

140
00:18:31,440 --> 00:18:39,960
are about the age of tools and so on. There is a need for those groups to know a little

141
00:18:39,960 --> 00:18:53,960
bit of the strains that we want to impose on these AI models to be labeled open source.

142
00:18:53,960 --> 00:19:02,960
Because it's something, when you're talking about the ethics of using AI, that also brings

143
00:19:02,960 --> 00:19:14,080
in the fact that the open source approach is more ethical in the terms of the way things

144
00:19:14,080 --> 00:19:22,120
are constructed, in terms of the transparency, the sharing of knowledge. And one of the concerns

145
00:19:22,120 --> 00:19:35,000
most of all of these groups are in is the issue of explainability. And probably that's

146
00:19:35,000 --> 00:19:41,520
something else, but probably something that we should be addressing. The explainability

147
00:19:41,520 --> 00:19:51,640
becomes easier when something is really transparent and clear and open source, more or less forces

148
00:19:51,640 --> 00:20:00,480
you into this. And so it's more the other way around. It's not that this issue depends

149
00:20:00,480 --> 00:20:10,280
on them. It's more that they, that's my part, they have to be aware of all of this.

150
00:20:10,280 --> 00:20:17,120
That's a very good point. In fact, we are in touch with a few researchers around explainable

151
00:20:17,120 --> 00:20:25,720
AI. And it's really, really interesting. So happy to connect with you. Thank you.

152
00:20:25,720 --> 00:20:33,360
Thank you. Thank you. Does anyone else have a different question? Thank you, Gerardo and

153
00:20:33,360 --> 00:20:36,360
Rick.

154
00:20:36,360 --> 00:20:48,920
Hello, Anastasia. So I see a comment about how the data information piece is going to

155
00:20:48,920 --> 00:20:55,920
stay the same in 0.9. Is that talking about how you're not going to change how you address

156
00:20:55,920 --> 00:21:02,560
the topic of data and the availability of information about the data? And I guess I'm

157
00:21:02,560 --> 00:21:09,040
curious about the general thinking since open data may not always be realistic, but it is

158
00:21:09,040 --> 00:21:18,240
this potentially important piece. So I'm curious about the focus or lack of focus on open data.

159
00:21:18,240 --> 00:21:20,240
I guess just in general.

160
00:21:20,240 --> 00:21:26,360
Yeah, I would actually, Stefano, if you want to chat or come off mute, I would love for

161
00:21:26,360 --> 00:21:35,720
you to answer that since it seems like you I mean, Stefano is is the leader on that particular

162
00:21:35,720 --> 00:21:53,000
element of this. If he is on the chat. He is. Oh, but he is not. I see his. So I'm I

163
00:21:53,000 --> 00:21:59,560
in the absence of Stefano, who made a correction on something in my slide. So obviously he

164
00:21:59,560 --> 00:22:06,880
has information I don't. I actually can't give anything other than what he just said.

165
00:22:06,880 --> 00:22:13,560
So there's two pieces of information shared. One one is that data information will not

166
00:22:13,560 --> 00:22:21,120
change from output. Oh, Stefano is coming back on. Stefano, we were wondering if you

167
00:22:21,120 --> 00:22:31,520
could answer. OK, I would still just love for you to answer Anastasia's question.

168
00:22:31,520 --> 00:22:44,240
Did you want her to ask again? Did you hear it? OK, I know all he knows. OK, so then I'll

169
00:22:44,240 --> 00:22:50,400
just continue. So there's two pieces of information shared once that data information will not

170
00:22:50,400 --> 00:23:00,840
change from 0.0.8, the link to which Nick shared and also that the checklist will conform

171
00:23:00,840 --> 00:23:10,600
to the MOF. So the way that I take that to mean. Is that.

172
00:23:10,600 --> 00:23:16,760
Stefano, do you want to do you want to speak? I don't know if you're maybe you're not

173
00:23:16,760 --> 00:23:23,920
know that you can speak. OK, it won't change only because the discussion is not complete.

174
00:23:23,920 --> 00:23:31,000
OK, OK, OK, can't speak. OK, yes. So there will be there will be a shift to the components

175
00:23:31,000 --> 00:23:37,800
coming from the model openness framework or MOF. And there may be I guess there may be

176
00:23:37,800 --> 00:23:44,520
other changes as well, because Stefano is saying the discussion is not complete.

177
00:23:44,520 --> 00:23:50,920
So maybe I can, Ximing, mirror. So there has been a lot of discussions around the data

178
00:23:50,920 --> 00:24:00,520
information and I'll share a blog post as well that Stef wrote, which is very interesting

179
00:24:00,520 --> 00:24:08,280
and also point a discussion topic that we have here. This is something that we're receiving

180
00:24:08,280 --> 00:24:17,240
a lot of feedback and hearing the working group members also participated in those discussions

181
00:24:17,240 --> 00:24:22,680
and to try to understand the role of data and data information. And this is actually

182
00:24:22,680 --> 00:24:29,080
a crucial point of the open source CI definition. So let me just drop some links here and we

183
00:24:29,080 --> 00:24:33,400
look forward to hearing more feedback on the discussion forum.

184
00:24:33,400 --> 00:24:42,520
OK, thank you. And Stefano has just written, we may even remove the text altogether and

185
00:24:42,520 --> 00:24:50,160
put a placeholder instead. And thank you, Nick, for that explainer link to the explainer

186
00:24:50,160 --> 00:24:56,760
post on data information. I I'm sorry if that wasn't satisfying, Anastasia. That is that

187
00:24:56,760 --> 00:25:02,520
is the level of response that I think we can give to you right now. But thank you for asking

188
00:25:02,520 --> 00:25:24,600
the question. Does anyone else have a question? Yes, Gerardo, go ahead.

189
00:25:24,600 --> 00:25:41,720
Just one more probably comes from the model we are working with, but I have an issue with

190
00:25:41,720 --> 00:25:49,120
defining codes that trains and codes that trains a model and code that uses a model

191
00:25:49,120 --> 00:26:00,440
of the same thing. And I think we should be splitting those and make it so that it's more

192
00:26:00,440 --> 00:26:08,160
explicit to say that both parts are to be open source, first of all, because they are

193
00:26:08,160 --> 00:26:25,280
probably not the same thing and and that most of the "open" AI things usually have that

194
00:26:25,280 --> 00:26:32,320
second part of the code open and open source, the thing that uses the model to generate

195
00:26:32,320 --> 00:26:41,280
stuff. But they keep close the code that was used to generate the model. And I think it

196
00:26:41,280 --> 00:26:50,840
may be important to split and make it explicit that both parts are considered codes and they

197
00:26:50,840 --> 00:27:05,120
are and they have to be open license and open source for the whole thing to be open source.

198
00:27:05,120 --> 00:27:12,720
And something I haven't yet seen and why it's defined here as just one thing, but usually

199
00:27:12,720 --> 00:27:20,960
it can become those two codes can be developed separately by different people on different

200
00:27:20,960 --> 00:27:32,600
teams. And there could be, let us say, the temptation to license to open license one

201
00:27:32,600 --> 00:27:35,600
and not the other.

202
00:27:35,600 --> 00:27:47,360
Okay, so regarding the three kind of categories of AI systems, we have data information, code

203
00:27:47,360 --> 00:27:54,520
and model, we are basically trying to have as few categories as possible that were still

204
00:27:54,520 --> 00:27:59,520
descriptive of the different types. And obviously, there were other types of categorization we

205
00:27:59,520 --> 00:28:07,480
could have used. That one seems pretty solid. In terms of the types of code, we are using

206
00:28:07,480 --> 00:28:18,520
this maybe, maybe you're aware of this, and maybe not. This list of types of code components

207
00:28:18,520 --> 00:28:26,040
or artifacts used to create AI systems, or used in AI systems from this model openness

208
00:28:26,040 --> 00:28:33,240
framework that was developed by a group of researchers and practitioners at and affiliated

209
00:28:33,240 --> 00:28:40,080
with the Linux Foundation. And we so as not to invent the wheel, because there's so much

210
00:28:40,080 --> 00:28:46,920
that we are doing ourselves, we said, How can we rely on the brilliance of others. And

211
00:28:46,920 --> 00:28:56,400
so we are using those code components. So we are we are not going to, I believe, change

212
00:28:56,400 --> 00:29:01,160
those components as listed in this document. And maybe Nick could even share a link to

213
00:29:01,160 --> 00:29:06,440
that it's a white paper is where those components live. Now, I think the Linux Foundation is

214
00:29:06,440 --> 00:29:12,040
also spinning up some websites and landing pages. But as of now, I know that they that

215
00:29:12,040 --> 00:29:18,400
you can find those in the white paper. So we are we're, we will just rely on that work

216
00:29:18,400 --> 00:29:24,760
that they've done. Let's maybe take one more question. And then Yeah, you're welcome to

217
00:29:24,760 --> 00:29:34,320
order. And then we'll call it a day. And Stefan, I was just saying we all care about open data.

218
00:29:34,320 --> 00:29:39,160
And we're very concerned about the issue of accessing data suitable to train an AI, which

219
00:29:39,160 --> 00:29:48,680
is true. Yep, absolutely. So, um, okay. Any, any other question from someone? I would say

220
00:29:48,680 --> 00:29:56,080
another question from someone who hasn't asked one yet. And I'll just keep reading what Stefan

221
00:29:56,080 --> 00:30:01,040
was saying, because he would be presenting with me if he were available. So he's saying

222
00:30:01,040 --> 00:30:05,960
we're also planning a conference specifically on this topic, which is that event in October,

223
00:30:05,960 --> 00:30:14,760
in in Europe in the city TBD is that comments or workshop on data, the issue of data and

224
00:30:14,760 --> 00:30:23,920
AI, which is such a substantial one. Um, okay, I'm not seeing any questions. Thank you. Thank

225
00:30:23,920 --> 00:30:30,200
you to everyone who came and who did ask a question that was really useful. Yeah, thank

226
00:30:30,200 --> 00:30:35,320
you, Gerardo. Thumbs up. Yeah, so I think we can turn off the recording. And thank you

227
00:30:35,320 --> 00:30:39,040
everyone for coming and have a have a wonderful weekend and do find us

### End of last town hall held on 2024-06-28 ###

### Start of next town hall held on 2024-07-26 ###
--- Presentation for 2024-07-26 ---
OPEN SOURCE AI DEFINITION
Online public townhall
July 26, 2024
last updated: July 26, 2024 (NV)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

OSI’s objective for 2024

Open Source AI Definition

3

Open Source AI Deﬁnition

Current Version
OSAID v.0.0.8

4

Open
Source AI
Definition

Preamble
4 Freedoms

v.0.0.8

Legal Checklist

-

Open Source AI Deﬁnition

What We’re Working On
OSAID v.0.0.9

6

Open
Source AI
Definition
Preamble
v.0.0.9 plans
Clarifying that
the recipients
of the freedoms
are developers,
deployers and
end-users

Open
Source AI
Definition
Four
Freedoms
v.0.0.9 plans

Clarifying that the
four freedoms of
open source AI are
derived from the
Free Software
Definition

Open
Source AI
Definition
Four
Freedoms
v.0.0.9 plans

Underlining that
components and
systems must be
free from
encumbrances that
prevent any
developer, deployer,
or users from
exercising those
freedoms.

Open Source
AI Definition
Preferred
Form
v.0.0.9 plans

Adding definitions of…
… the “OSD
compliant”
requirement for data
information...
…and the “OSD
conformant”
requirement for model
parameters
..so legal
requirements are
clear for each
component

Open
Source AI
Definition
Checklist
v.0.0.9 plans

Checklist will be a
separate document
and process and its
components will be
updated to follow the
Model Openness
Framework (MOF)
precisely.

Open Source AI Deﬁnition

System Validation

OSAID v.0.0.8 (and soon v. 0.0.9)

12

Validation Reviewers
1. Arctic
1.

Jesús M.
Gonzalez-Barahona
Universidad Rey Juan
Carlos

2. BLOOM
2.
3.

Danish Contractor
BLOOM Model Gov.
Work Group
Jaan Li University of
Tartu, One Fact
Foundation

3. Falcon
1.
2.

Casey Valk Nutanix
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France

4. Grok
1.
2.

Victor Lu independent
database consultant
Karsten Wade Open
Community Architects

We were interested in reviewing about 10 AI systems self-described as
open to validate the definition.

5. Llama 2
1.
2.
3.
4.

Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute of
Paris
Victor Lu independent
database consultant

9. LLM360
5.

[Team member TBD]
LLM360

8. Mistral
1.
2.
3.

Mark Collier
OpenInfra Foundation
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
Cailean Osborne
University of Oxford,
Linux Foundation

7. OLMo
4.
5.

Amanda Casari
Google
Abdoulaye Diack
Google

10. Pythia*
1.
2.
3.

Hailey Schoelkopf
EleutherAI

4.

Aviya Skowron
EleutherAI

11. T5
5.

8. OpenCV*
1.

We will need an
independent
reviewer for
LLM360

Rasim Sen Oasis
Software Technology
Ltd.

9. Phi-2
6.

Seo-Young Isabelle
Hwang Samsung

Seo-Young Isabelle
Hwang Samsung
Stella Biderman
EleutherAI

Jaan Li University of
Tartu, One Fact
Foundation

Viking
6.

Merlijn Sebrechts
Ghent University

Validation Updates

Thanks to Arctic and
LLM360 for helping identify
documentation!

14

Open Source AI Deﬁnition

What’s Next?

June - October 2024

● Complete validation phase
● Resolve comments, release v. 0.0.9 after
validation
● Cut the release candidate with sufficient
endorsement
15

Board Guidance
The OSI Board requires a deﬁnition that is:
Supported by diverse
stakeholders

Provides real-life
examples

Ready by October
2024

The deﬁnition needs to
have approval by end
users, developers,
deployers and subjects of
AI, globally.

The deﬁnition must include
relevant examples of AI
systems that comply with it
at the time of approval, so
cannot have an empty set.

A usable version of the
deﬁnition needs to be
ready for approval by the
board at the October
board meeting.

Approved June 21, 2024

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

June

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.8

July

August

September

Virtual System
Review

Virtual System
Review

Virtual System
Review Ends

Townhalls +

Townhalls +

Townhalls +

Townhalls +

- OSPOs for
Good (NYC)
- Sustain
Africa (virtual)

- AI-dev (Hong
Kong)

- Nerdearla
(Buenos Aires)

- All Things
Open (Raleigh)
- Data Workshop
(Europe TBD)

Draft 0.0.9

RC1

RC1

October

Stable
Version

In-Person Meetings
Region

Country

City

Conference

Date

North America

United States

Pittsburgh

✓ PyCon US

May 17

Europe

France

Paris

✓ OW2

June 11 - 12

North America

United States

New York

✓ OSPOs for Good

July 9 - 11

Africa

Virtual

Virtual

✓ Sustain Africa

July 15

Asia Paciﬁc

China

Hong Kong

AI_dev

August 23

Latin America

Argentina

Buenos Aires

Nerdearla

September 24 - 28

Europe

France

Paris

Data governance

October

North America

United States

Raleigh

All Things Open

Oct 27 - 29
18

Co-Design Workshop in Raleigh, USA
All Things Open Conference | October 2023

19

Co-Design Workshop in Monterey, USA
Linux Foundation Member Summit | October 2023

20

Co-Design Workshop in Addis Ababa, Ethiopia
Digital Public Goods Alliance Members Meeting | November 2023

21

How to Participate :)
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
● Volunteer to help with validation (email or DM Mer
Joyce)
22

Q&A

23

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the deﬁnition.

24


--- Subtitles for 2024-07-26 --- ###
1
00:00:00,001 --> 00:00:06,640
So, first of all, let me introduce myself.

2
00:00:06,640 --> 00:00:08,320
My name is Nick Vidal.

3
00:00:08,320 --> 00:00:14,040
I'm the community manager at the OSI, the Open Source Initiative.

4
00:00:14,040 --> 00:00:22,360
And every two weeks, we organize a public town hall about the open source AI definition.

5
00:00:22,360 --> 00:00:28,680
The goal of this town hall is to give updates and news about what has been happening with

6
00:00:28,680 --> 00:00:35,840
the definition and also to hear your feedback and see how we can make it better.

7
00:00:35,840 --> 00:00:42,200
So let's jump to the rules, right?

8
00:00:42,200 --> 00:00:45,120
What are some agreements that we have here?

9
00:00:45,120 --> 00:00:51,560
I think we can all be very open and very kind.

10
00:00:51,560 --> 00:01:01,720
The idea here, even though if we have any differences, we can agree to disagree.

11
00:01:01,720 --> 00:01:07,240
And so this is especially important when I open space for questions.

12
00:01:07,240 --> 00:01:16,680
You're free to ask any questions and I'll be happy to answer them as best as possible.

13
00:01:16,680 --> 00:01:21,420
So let's talk about the open source AI definition and where we are.

14
00:01:21,420 --> 00:01:28,040
We have been at version 0.0.8 for quite some time right now.

15
00:01:28,040 --> 00:01:34,000
And the reason for that is that we received a lot of feedback and we are working those

16
00:01:34,000 --> 00:01:44,520
details because the next one, the next version will be something much more stable and really

17
00:01:44,520 --> 00:01:48,640
with very few changes going forward.

18
00:01:48,640 --> 00:01:53,960
So the current definition, what we have right now is we have the preamble, we have the four

19
00:01:53,960 --> 00:01:58,660
freedoms and a checklist.

20
00:01:58,660 --> 00:02:03,640
Now what are we working for version 0.0.9?

21
00:02:03,640 --> 00:02:10,280
There are going to be some significant changes based on this feedback and we're about to

22
00:02:10,280 --> 00:02:14,680
release version 0.0.9.

23
00:02:14,680 --> 00:02:20,960
It's just a matter of a final approval.

24
00:02:20,960 --> 00:02:33,540
And we still have the preamble and we try to clarify who are the recipients of the freedoms.

25
00:02:33,540 --> 00:02:43,080
We also have the what's open source AI and we clarify those four freedoms.

26
00:02:43,080 --> 00:02:52,640
As you know, we have the freedom to use, to study, to modify and to share the system.

27
00:02:52,640 --> 00:03:00,040
And for those who know, who have some background on the history of free software and open source,

28
00:03:00,040 --> 00:03:05,320
you know very well that those four freedoms are from the free software definition.

29
00:03:05,320 --> 00:03:07,480
And this has been very stable.

30
00:03:07,480 --> 00:03:16,200
We have been using those four freedoms as the basis throughout most of the moment.

31
00:03:16,200 --> 00:03:24,640
Now one observation that we're also making is that we are underlining that components

32
00:03:24,640 --> 00:03:35,200
and systems must be free from encumbrances that prevent any developer, deployer or user

33
00:03:35,200 --> 00:03:37,880
from exercising those freedoms.

34
00:03:37,880 --> 00:03:44,560
So this is something that we highlighted for this version as well.

35
00:03:44,560 --> 00:03:52,560
And now we made some substantial changes to the preferred form to make modifications to

36
00:03:52,560 --> 00:03:56,200
machine learning systems.

37
00:03:56,200 --> 00:04:04,840
And the reason why we did that, we received a lot of feedback and we wanted to really

38
00:04:04,840 --> 00:04:07,760
make this as precise as possible.

39
00:04:07,760 --> 00:04:13,000
So we added the definitions, right?

40
00:04:13,000 --> 00:04:21,960
Also there's the question of OSD compliance and OSD conformance for each one of those

41
00:04:21,960 --> 00:04:25,720
requirements, right?

42
00:04:25,720 --> 00:04:29,800
And also the legal requirements for each components.

43
00:04:29,800 --> 00:04:38,080
So we made this text here, the preferred form, clearer for us to understand.

44
00:04:38,080 --> 00:04:41,280
And now this is a big one.

45
00:04:41,280 --> 00:04:48,520
Since the definition is going to be something very stable, we cannot make, we shouldn't

46
00:04:48,520 --> 00:04:52,080
make many changes with every version.

47
00:04:52,080 --> 00:04:54,580
We want to have something very solid.

48
00:04:54,580 --> 00:04:59,720
So this, the definition itself should not change much.

49
00:04:59,720 --> 00:05:09,680
However, we know that the technologies are evolving very rapidly and there might be some

50
00:05:09,680 --> 00:05:16,840
changes to the legal framework or to each one of those components.

51
00:05:16,840 --> 00:05:26,520
And so what we're doing for 0.0.9 is actually we're going to make the checklist as a separate

52
00:05:26,520 --> 00:05:31,800
document because the definition is going to be stable.

53
00:05:31,800 --> 00:05:40,120
And the checklist, we might have some smaller changes, of course, but it's going to evolve,

54
00:05:40,120 --> 00:05:41,120
right?

55
00:05:41,120 --> 00:05:50,040
And I wanted to highlight that we use as the list of components, the model openness

56
00:05:50,040 --> 00:05:51,040
framework.

57
00:05:51,040 --> 00:05:58,800
This is something that was created by the Linux Foundation and they have 17 components

58
00:05:58,800 --> 00:06:01,560
and we're using that as a basis, right?

59
00:06:01,560 --> 00:06:08,280
For the checklist.

60
00:06:08,280 --> 00:06:19,240
Now we are currently validating the open source CI definition, looking at several models.

61
00:06:19,240 --> 00:06:31,760
Right now we're looking at Arctic, Bloom, Falcon, Grok, Yamachu, LLM360, Mistral, Oumu,

62
00:06:31,760 --> 00:06:41,120
OpenCV, Fitu, Pythia, T5, and also I believe Vykan, we added that as well.

63
00:06:41,120 --> 00:06:49,960
So the idea behind this is we are looking at the open source CI definition and looking

64
00:06:49,960 --> 00:06:55,160
at the different models and different AI systems that exist.

65
00:06:55,160 --> 00:07:05,760
And we are matching that, looking at the checklist and seeing if those systems are in fact, if

66
00:07:05,760 --> 00:07:10,360
they achieve this, right?

67
00:07:10,360 --> 00:07:17,040
If the open source CI definition applies well to them and we're validating this.

68
00:07:17,040 --> 00:07:25,440
And so as part of this validation updates, here are the current results so far.

69
00:07:25,440 --> 00:07:35,160
So looking at Arctic, for example, this is a large language model by Snowflake and it's

70
00:07:35,160 --> 00:07:36,540
actually pretty good.

71
00:07:36,540 --> 00:07:38,600
It's pretty open.

72
00:07:38,600 --> 00:07:46,680
And we are looking at those different systems and for Arctic, we believe that it's expected

73
00:07:46,680 --> 00:07:52,880
that it does, yes, fulfill the open source CI definition, all the checklists.

74
00:07:52,880 --> 00:07:59,640
So it's very likely that Arctic is an open source AI, right?

75
00:07:59,640 --> 00:08:08,880
Now we have other systems which can be a bit more challenging, right?

76
00:08:08,880 --> 00:08:19,640
So Bloom, Bloom is regarded as an open model by some, but at the same time, they use Rail,

77
00:08:19,640 --> 00:08:22,400
which is very restrictive.

78
00:08:22,400 --> 00:08:30,560
It does not match the open source CI definition on that regard.

79
00:08:30,560 --> 00:08:36,240
And if we look at all other models as well, we see that a few of them, the ones that we

80
00:08:36,240 --> 00:08:43,000
expect to actually be considered open, in fact, they are, they are validating.

81
00:08:43,000 --> 00:08:51,080
So for example, Omo, it's a very open and it's likely that it meets the open source

82
00:08:51,080 --> 00:08:53,840
CI definition requirements.

83
00:08:53,840 --> 00:08:59,560
And AlphaBifia, it's a confirmed yes right now.

84
00:08:59,560 --> 00:09:05,080
So this validation process is important, right?

85
00:09:05,080 --> 00:09:08,240
Now this, what's next?

86
00:09:08,240 --> 00:09:09,640
What's the timeline?

87
00:09:09,640 --> 00:09:10,760
What's happening?

88
00:09:10,760 --> 00:09:13,200
We have this validation phase.

89
00:09:13,200 --> 00:09:14,920
We are looking at the comments.

90
00:09:14,920 --> 00:09:21,480
We're about to release version 0.0.9 after this validation.

91
00:09:21,480 --> 00:09:26,920
We want to have a release candidate.

92
00:09:26,920 --> 00:09:30,880
Also there's a guidance from the board and this is really important.

93
00:09:30,880 --> 00:09:37,360
So the board has three requirements, right, for this definition.

94
00:09:37,360 --> 00:09:43,720
It has to be supported by diverse stakeholders, right?

95
00:09:43,720 --> 00:09:47,600
Also it must provide real life examples.

96
00:09:47,600 --> 00:09:57,480
We don't want a definition that there's no AI system that actually matches that.

97
00:09:57,480 --> 00:10:06,640
So it's important for us to have this, at least a few cases where those AI systems match

98
00:10:06,640 --> 00:10:08,160
the open source CI definition.

99
00:10:08,160 --> 00:10:11,480
They fulfill the open source CI definition.

100
00:10:11,480 --> 00:10:20,200
And we really want to have this stable version by October, at all things open, where we're

101
00:10:20,200 --> 00:10:22,400
going to be announcing this.

102
00:10:22,400 --> 00:10:25,620
So we have this, this restriction on time.

103
00:10:25,620 --> 00:10:30,400
We cannot just go on for this forever.

104
00:10:30,400 --> 00:10:39,040
In fact, a lot of, there's a lot going on around policies and legislation around the

105
00:10:39,040 --> 00:10:40,040
world.

106
00:10:40,040 --> 00:10:44,800
And a definition, a clear definition really is really important.

107
00:10:44,800 --> 00:10:51,720
So we're trying to run against the clock, right?

108
00:10:51,720 --> 00:10:58,180
Basically this is a timeline throughout this past month, we've been attending several events

109
00:10:58,180 --> 00:11:00,060
worldwide.

110
00:11:00,060 --> 00:11:07,020
So July, right now we were in New York City for an event called Auspice for Good that

111
00:11:07,020 --> 00:11:12,360
happened in the United Nations headquarters in New York.

112
00:11:12,360 --> 00:11:16,060
And we also participated in Sustain Africa.

113
00:11:16,060 --> 00:11:17,520
August is coming up.

114
00:11:17,520 --> 00:11:22,740
We're still going to organize those online events, especially because not everyone can

115
00:11:22,740 --> 00:11:24,820
travel, right?

116
00:11:24,820 --> 00:11:27,420
And this makes it more accessible.

117
00:11:27,420 --> 00:11:31,900
But we're going to attend AI Dev in Hong Kong.

118
00:11:31,900 --> 00:11:35,180
This is an event by the Linux Foundation.

119
00:11:35,180 --> 00:11:37,540
We're going to give a talk there.

120
00:11:37,540 --> 00:11:40,980
We're also going to be in Northern Ireland, Buenos Aires.

121
00:11:40,980 --> 00:11:46,980
So we want to make sure that we keep AI Dev will happen in Hong Kong, in Asia.

122
00:11:46,980 --> 00:11:50,500
Northern Ireland, in Buenos Aires, in Latin America.

123
00:11:50,500 --> 00:11:52,960
We just came from an event in New York.

124
00:11:52,960 --> 00:11:58,340
So we really try to make this as representative as possible.

125
00:11:58,340 --> 00:12:02,940
We're trying to organize an in-person event in Africa as well.

126
00:12:02,940 --> 00:12:06,380
And I think we're finalizing that as well.

127
00:12:06,380 --> 00:12:13,340
So we're trying to make the events as accessible as possible by making it online, but also

128
00:12:13,340 --> 00:12:19,620
in-person events and representative of the whole world, right?

129
00:12:19,620 --> 00:12:28,460
For October, then we have the All Things Open events, an important event around open source.

130
00:12:28,460 --> 00:12:34,980
And by then, we hope to announce the stable version of the definition.

131
00:12:34,980 --> 00:12:42,580
Also there's an event, a very small meeting around data.

132
00:12:42,580 --> 00:12:48,660
But this is, we've been gathering mostly nonprofit organizations to be part of this meeting.

133
00:12:48,660 --> 00:12:52,100
It's really small and it's fully booked.

134
00:12:52,100 --> 00:12:58,100
So apologies if we cannot open it up more.

135
00:12:58,100 --> 00:13:07,260
We will try to organize other events as well and invite people to be part of that.

136
00:13:07,260 --> 00:13:12,800
This is the list of some events that have been happening these past months.

137
00:13:12,800 --> 00:13:16,820
We were also in France for OW2.

138
00:13:16,820 --> 00:13:25,780
And so you can see the dates here.

139
00:13:25,780 --> 00:13:28,760
And this in-person events, they have been important.

140
00:13:28,760 --> 00:13:34,780
We have been able to hear a lot of good feedback from everyone and really try to come up with

141
00:13:34,780 --> 00:13:43,100
a definition that it has a lot of backing from people, right?

142
00:13:43,100 --> 00:13:50,340
And so I would like to invite you to participate at these events, the events in person as well.

143
00:13:50,340 --> 00:14:01,500
Also the public forum, you can go to discuss.opensource.org and comments on the drafts or any other topic

144
00:14:01,500 --> 00:14:04,340
that you like to understand.

145
00:14:04,340 --> 00:14:05,380
It's totally free.

146
00:14:05,380 --> 00:14:06,380
You can join.

147
00:14:06,380 --> 00:14:11,420
You don't have to join as an OSI paying member.

148
00:14:11,420 --> 00:14:13,840
You can join as a free member as well.

149
00:14:13,840 --> 00:14:17,140
And you have an access to the forum.

150
00:14:17,140 --> 00:14:22,300
We're going to continue organizing this biweekly virtual townhouse.

151
00:14:22,300 --> 00:14:26,940
And here is an opportunity for you to ask questions and try to understand what's really

152
00:14:26,940 --> 00:14:29,580
happening, right?

153
00:14:29,580 --> 00:14:33,460
Invite you if you want to volunteer for the validation process.

154
00:14:33,460 --> 00:14:42,940
I believe right now it's the working groups are closed, but you can still email us to

155
00:14:42,940 --> 00:14:47,260
know if there's still an opportunity.

156
00:14:47,260 --> 00:14:49,380
And that's it.

157
00:14:49,380 --> 00:14:56,220
So right now I'm going to open up spaces for questions and answers.

158
00:14:56,220 --> 00:15:03,340
You can either ask questions on chats, if you have your microphone option open as well,

159
00:15:03,340 --> 00:15:05,940
you can ask that question live.

160
00:15:05,940 --> 00:15:11,180
And I'm available to answer your questions as best as I can.

161
00:15:11,180 --> 00:15:17,180
>> I have a quick question.

162
00:15:17,180 --> 00:15:22,540
Do you think there are only a few people on this call because there's not a release of

163
00:15:22,540 --> 00:15:29,420
the .9 to discuss or sort of is that why this call is so quiet?

164
00:15:29,420 --> 00:15:31,100
>> Yes.

165
00:15:31,100 --> 00:15:35,380
So this is our 13th townhouse.

166
00:15:35,380 --> 00:15:40,020
And so it has been we do this every two weeks.

167
00:15:40,020 --> 00:15:45,940
And I think right now there's a high expectation for version 0.0.9.

168
00:15:45,940 --> 00:15:52,660
We've been discussing 0.0.8 for quite some time right now.

169
00:15:52,660 --> 00:15:59,460
And I think most of the questions around this version have been answered.

170
00:15:59,460 --> 00:16:03,300
So people are really expecting a new version.

171
00:16:03,300 --> 00:16:04,300
And highly so.

172
00:16:04,300 --> 00:16:05,300
So are we.

173
00:16:05,300 --> 00:16:15,020
We're just waiting for a final decision from the board to release the version.

174
00:16:15,020 --> 00:16:19,740
I expect that we're going to have more questions around that.

175
00:16:19,740 --> 00:16:20,740
>> Okay.

176
00:16:20,740 --> 00:16:23,420
I have another question.

177
00:16:23,420 --> 00:16:26,540
What about the events in Hong Kong and Buenos Aires?

178
00:16:26,540 --> 00:16:31,940
Are there going to be more discussions, like presentations or more discussions or are they

179
00:16:31,940 --> 00:16:32,940
panels?

180
00:16:32,940 --> 00:16:37,380
Can you tell me any more about what's planned specifically for Hong Kong?

181
00:16:37,380 --> 00:16:38,380
>> Of course.

182
00:16:38,380 --> 00:16:39,380
Yeah.

183
00:16:39,380 --> 00:16:51,900
For AI Dev Hong Kong, we're going to have a talk from Mare Joyce, who is the facilitator

184
00:16:51,900 --> 00:16:55,580
of the open source AI definition.

185
00:16:55,580 --> 00:17:04,860
And also from Annie Lai, who is the chair of the Linux Foundation AI and Data.

186
00:17:04,860 --> 00:17:08,880
So they're going to give this talk.

187
00:17:08,880 --> 00:17:15,020
And I believe they're also going to be available for any discussions.

188
00:17:15,020 --> 00:17:17,660
I don't think we have a panel there.

189
00:17:17,660 --> 00:17:26,700
Now for Buenos Aires, for Nerdiarla, there's going to be I see that there's a yes.

190
00:17:26,700 --> 00:17:33,460
So we're going to have a presentation by Mare Joyce and staff.

191
00:17:33,460 --> 00:17:39,100
And are we going to have a panel there or a I believe we're going to have a workshop

192
00:17:39,100 --> 00:17:43,220
as well.

193
00:17:43,220 --> 00:17:51,300
This is being confirmed right now by Holo, who is organizing this.

194
00:17:51,300 --> 00:17:54,220
Yeah.

195
00:17:54,220 --> 00:18:02,500
We're going to have a workshop as well.

196
00:18:02,500 --> 00:18:08,780
It's very likely we're going to have a workshop there.

197
00:18:08,780 --> 00:18:14,420
And it's this event in Buenos Aires is pretty huge.

198
00:18:14,420 --> 00:18:17,780
It's 10,000 participants.

199
00:18:17,780 --> 00:18:20,440
And very well organized events.

200
00:18:20,440 --> 00:18:28,220
So I highly recommend it as well.

201
00:18:28,220 --> 00:18:40,460
Anyone has any other questions?

202
00:18:40,460 --> 00:18:41,460
Of course.

203
00:18:41,460 --> 00:18:42,460
Yeah.

204
00:18:42,460 --> 00:18:43,460
So let me share the link here.

205
00:18:43,460 --> 00:18:44,460
Oh, thanks, Holo.

206
00:18:44,460 --> 00:18:47,460
Holo has already added that.

207
00:18:47,460 --> 00:18:52,140
In fact, Holo is one of the organizers of the events.

208
00:18:52,140 --> 00:18:54,100
The leading organizer.

209
00:18:54,100 --> 00:19:00,380
If you'd like to reach out to him directly, Holo, maybe if you could base your email

210
00:19:00,380 --> 00:19:03,260
or all right.

211
00:19:03,260 --> 00:19:04,260
Yeah.

212
00:19:04,260 --> 00:19:12,500
Feel free to reach out to Holo regarding the events.

213
00:19:12,500 --> 00:19:18,100
All right.

214
00:19:18,100 --> 00:19:23,500
So if there are no other questions, this is going to be made available, the slides and

215
00:19:23,500 --> 00:19:24,500
the recording.

216
00:19:24,500 --> 00:19:28,700
I'm going to be basing this on the forum.

217
00:19:28,700 --> 00:19:29,700
And thank you so much.

218
00:19:29,700 --> 00:19:31,780
We really appreciate it.

219
00:19:31,780 --> 00:19:34,980
And we look forward to your feedback on the forum as well.

220
00:19:34,980 --> 00:19:35,980
All right?

221
00:19:35,980 --> 00:19:36,980
Have a great weekend.

222
00:19:36,980 --> 00:19:37,980
Bye bye.

223
00:19:37,980 --> 00:19:37,980
Bye bye.

224
00:19:37,980 --> 00:19:47,500
Bye bye.

### End of last town hall held on 2024-07-26 ###

### Start of next town hall held on 2024-08-23 ###
--- Presentation for 2024-08-23 ---
OPEN SOURCE AI DEFINITION
Online public townhall
August 23, 2024
last updated: August 23, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

Open Source AI Deﬁnition

New Version
OSAID v.0.0.9

3

Unveiling the Future:
Nurturing Openness in AI Development
揭示未来: 培育人工智能开放性发展
Anni Lai

Chair, Generative AI Commons, LF
AI & Data
Head of Open Source Operations,
Futurewei

Mer Joyce

Founder
Do Big Good LLC
Co-Design Facilitator, Open Source AI
Deﬁnition
Open Source Initiative

🥳 New Today: OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open
Source AI System must include:

Open

Weights
Model weights and
parameters

Open

+ Code +
Source code used
to train and run
the system

Data

Information
The dataset or
detailed information
about the data used
to train the system

New Version: OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Weights
The model weights and
parameters, made available
under OSI-approved terms
Examples: checkpoints from key intermediate stages of training as
well as the ﬁnal optimizer state

New Version: OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Code
The source code used to train and
run the system, made available
with OSI-approved licenses
Examples: code used for pre-processing data, training, validation and testing,
supporting libraries like tokenizers and hyperparameters search code,
inference code, and model architecture.

New Version: OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Data

Information
Sufﬁciently detailed information about the data used to
train the system, so that a skilled person can recreate a
substantially equivalent system using the same or
similar data. Data information shall be made available
with licenses that comply with the Open Source
Deﬁnition.
Examples: training methodologies and
techniques, training data sets used, information
about the provenance of those data sets, their scope and characteristics, how the data
was obtained and selected, the labeling procedures and data cleaning methodologies.

Training Data in the OSAID
OSI afﬁrms the beneﬁts of full access to training data while acknowledging it
is not always possible for reasons of law, privacy norms, technical feasibility,
and cultural practice.
• Training data is valuable to study AI systems: to understand the biases that have been
learned, which can impact system behavior. But training data is not part of the preferred
form for making modiﬁcations to an existing AI system
• Data can be hard to share. Laws that permit training on data often limit the resharing of that
same data to protect copyright or other interests.
• Privacy rules also give a person the rightful ability to control their most sensitive information,
such as decisions about their health.
• Similarly, much of the world’s Indigenous knowledge is protected through mechanisms that
are not compatible with later-developed frameworks for rights exclusivity and sharing.
• Open training data (data that can be reshared) provides the best way to enable users to study
the system, along with the preferred form of making modiﬁcations.
• Public training data (data others can inspect as long as it remains available) also enables
users to study the work, along with the preferred form.

OSAID Approval Criteria
OSI Board requires a deﬁnition that is:
Supported
by diverse
stakeholders

Provides
real-life
examples

Ready by
October 2024

The deﬁnition
needs to have
approval by end
users, developers,
deployers and
subjects of AI,
globally.

The deﬁnition must
include relevant
examples of AI
systems that comply
with it at the time of
approval, so cannot
have an empty set.

A usable version
of the deﬁnition
needs to be ready
for approval by
the board at the
October board
meeting.

Approved June 21, 2024

Open Source AI Deﬁnition

What’s Next?

September - October 2024

● Resolve comments, release RC1
● Launch stable version at All Things Opens

11

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

June

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.8

July

August

September

Virtual System
Review

Virtual System
Review

Virtual System
Review Ends

October

Hi!

Townhalls +

Townhalls +

Townhalls +

Townhalls +

- OSPOs for
Good (NYC)
- OSCA (virtual)

- AI-dev (Hong
Kong)

- Deep Learning Indaba
(Dakar)
- IndiaFOSS (Bengaluru)
- Nerdearla (Buenos Aires)

- All Things Open
(Raleigh)
- OSAI Data
Workshop (Paris)

Draft 0.0.8

Draft 0.0.9

RC1

Stable
Version

How to Participate :)
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
● Volunteer to help with validation (email or DM Mer
Joyce)
13

Q&A

14

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the deﬁnition.

15


--- Subtitles for 2024-08-23 --- ###
1
00:00:00,001 --> 00:00:10,880
I don't... oh now it's being recorded. Okay. Yeah. Okay. So now we're being recorded.

2
00:00:10,880 --> 00:00:15,560
This is the Q&A session for those who are watching the recording. The

3
00:00:15,560 --> 00:00:23,600
presentation has already occurred. Yeah. Sounds good. Nick. Nick is commenting. He's

4
00:00:23,600 --> 00:00:30,180
going to record the Q&A. I believe Stefano will be in Vienna. I do not know

5
00:00:30,180 --> 00:00:35,600
if there will be a workshop or session on the OSAID, but I think Stefano will

6
00:00:35,600 --> 00:00:42,840
be in Vienna personally. Yeah. It's not on my official list, but Nick maybe you have

7
00:00:42,840 --> 00:00:48,760
some information about that. Oh he will be presenting, but not giving a workshop.

8
00:00:48,760 --> 00:01:00,200
Okay. Thanks Nick. All right. Oh okay. Yeah. She and I presented today as I

9
00:01:00,200 --> 00:01:05,440
mentioned earlier. I don't know if people in the recording can see the chat, but

10
00:01:05,440 --> 00:01:09,520
we're just saying that Stefano is going to present with Annie Lai of the Linux

11
00:01:09,520 --> 00:01:18,040
Foundation in Vienna. Yeah. I'll hang out here at least till the half hour to chat

12
00:01:18,040 --> 00:01:24,600
with him, but the formal program has ended.

13
00:01:24,600 --> 00:01:29,640
And Nick will also hang out.

14
00:01:29,640 --> 00:01:33,680
You're welcome.

15
00:01:44,320 --> 00:01:50,080
Ah could I share a bit about my presentation in Hong Kong? Yeah. So it was

16
00:01:50,080 --> 00:01:56,600
those slides that you all saw. I guess it's not I can go back so it's in the

17
00:01:56,600 --> 00:02:04,600
recording, but these are the slides. This is what it what it was called unveiling

18
00:02:04,600 --> 00:02:08,960
the future nurturing openness in AI development, which was Annie's creation.

19
00:02:08,960 --> 00:02:14,880
And Annie talked about the Linux Foundation generative AI Commons

20
00:02:14,880 --> 00:02:20,760
projects, the model openness framework, and the model openness tool. And then I

21
00:02:20,760 --> 00:02:28,720
spoke about the OSAID and specifically talked about not only version 0.0.9

22
00:02:28,720 --> 00:02:35,080
which you saw, but I also spoke about the co-design process both creating the

23
00:02:35,080 --> 00:02:41,840
for freedoms for open source AI and also identifying the required

24
00:02:41,840 --> 00:02:47,200
components using our virtual work groups. And that's been covered in past

25
00:02:47,200 --> 00:02:55,120
workshops. But that was the that was what I presented in Hong Kong. And then

26
00:02:55,120 --> 00:02:58,880
Stefano was also there and so he participated in the Q&A and that was

27
00:02:58,880 --> 00:03:10,000
great. What was the questions? I think there was there was a question about AU

28
00:03:10,000 --> 00:03:16,600
policy and the feasibility of implementation. And then there was a

29
00:03:16,600 --> 00:03:24,120
question in Chinese and Annie fortunately speaks Chinese. And I believe it was

30
00:03:24,120 --> 00:03:33,280
about there were two. One was about the legal implications of the definition. And

31
00:03:33,280 --> 00:03:40,880
the second question was about just asking for why why use open source AI?

32
00:03:40,880 --> 00:03:46,320
Why make a system open source? And so Annie reiterated the benefits of openness

33
00:03:46,320 --> 00:03:52,440
for AI. So yeah.

34
00:03:53,440 --> 00:03:59,520
Oh, loss of connection.

35
00:03:59,520 --> 00:04:12,840
I think I'm still I can still see you chatting. Okay. It said I had lost

36
00:04:12,840 --> 00:04:16,920
internet. Okay. Early impressions on the new, so I'm just going to read Martin's

37
00:04:16,920 --> 00:04:21,480
comment. My early impressions on the new data requirements in 0.0.9 have been good.

38
00:04:21,480 --> 00:04:25,920
Okay. It seems to do the best with a complex situation. We'll keep thinking

39
00:04:25,920 --> 00:04:32,200
about any loopholes. Thank you, Martin. That is basically what we have also

40
00:04:32,200 --> 00:04:39,840
found is this is a compromise. There are people that wish that the definition

41
00:04:39,840 --> 00:04:44,840
were more stringent and there are people that wish it were looser. So there's pull

42
00:04:44,840 --> 00:04:54,760
on both ends. But that's probably some symptom of a compromise. So yeah. I'm

43
00:04:54,760 --> 00:04:57,880
glad that you're seeing that too.

44
00:04:57,880 --> 00:05:09,520
Okay. If training data had been required, how many models would have met that? Yes.

45
00:05:09,520 --> 00:05:20,800
So I think there would have been a few. I think Olmo I think might have met it. I

46
00:05:20,800 --> 00:05:29,360
think the concern was about the legality of the data that there is a risk of

47
00:05:29,360 --> 00:05:36,520
lawsuits or of the data being legally shareable in certain jurisdictions but

48
00:05:36,520 --> 00:05:52,360
not others. And that we wanted to have a definition that would have a global

49
00:05:52,360 --> 00:06:01,840
could be implemented globally and that systems wouldn't be hung up on or be on

50
00:06:01,840 --> 00:06:10,040
possible legal actions. So that is my understanding. And Nick, feel free to

51
00:06:10,040 --> 00:06:22,240
yeah there you go. Olmo and Pythia. Yes. Right. Yes. Yes. So there's a nice forum

52
00:06:22,240 --> 00:06:39,680
post about yeah I could try to find it that talks about the legal

53
00:06:39,680 --> 00:06:47,320
uncertainties of training data and how that could make it helpful to people who

54
00:06:47,320 --> 00:06:54,200
want access to AI open source AI systems to simply pull that X factor out of the

55
00:06:54,200 --> 00:06:56,960
requirements.

56
00:06:56,960 --> 00:07:06,000
See if I can find those.

57
00:07:23,800 --> 00:07:35,920
Maybe. I don't think no it's not something that I wrote. I think it was I

58
00:07:35,920 --> 00:07:48,000
was something in June. Anyway I was reading it this morning. Oh yes

59
00:07:48,000 --> 00:07:50,800
explaining the concept of data information I think it might have been

60
00:07:50,800 --> 00:07:53,240
that one.

61
00:07:57,280 --> 00:08:08,720
Yeah so yeah this is some someone called Sen Ficon. People can be pseudonymous

62
00:08:08,720 --> 00:08:17,520
in the forum but it's just I'll just copy and paste it. It just felt very

63
00:08:17,520 --> 00:08:26,600
clear and well written. This is just a part of that person's post. And that's

64
00:08:26,600 --> 00:08:29,120
the link.

65
00:08:29,120 --> 00:08:38,560
It might be Felix. I don't know. Could be.

66
00:08:38,560 --> 00:08:53,080
Oh great. Thank you. Thank you to Felix. I've actually heard about Felix and

67
00:08:53,080 --> 00:08:58,280
didn't know that this was what he'd written. But yeah it's very good. Very

68
00:08:58,280 --> 00:09:07,960
well put. What else? What are people thinking about? Anyone to chat about?

69
00:09:15,960 --> 00:09:24,040
So there are a couple folks. Maybe not both. There's at least one person on the

70
00:09:24,040 --> 00:09:29,720
call who is from META but I won't ask them to represent that affiliation

71
00:09:29,720 --> 00:09:36,840
unless they want to. But so the answer is yes they're on the call today. And if

72
00:09:36,840 --> 00:09:39,640
that person wants to say anything they're welcome to but they don't have

73
00:09:39,640 --> 00:09:43,160
to say anything if they don't want to.

74
00:09:47,480 --> 00:09:59,800
Mm-hmm yeah for future LLAMA models. Yep. Yep. Do you want to ask? Yeah you can put

75
00:09:59,800 --> 00:10:07,080
the question out there and the person can answer or not. I imagine it's

76
00:10:07,080 --> 00:10:10,680
something they're discussing internally.

77
00:10:15,080 --> 00:10:20,120
Yeah anyone who hasn't asked a question or participated in the chat.

78
00:10:20,120 --> 00:10:25,560
Ralph or DB, Gerardo,

79
00:10:25,560 --> 00:10:29,880
Toka.

80
00:10:29,880 --> 00:10:35,800
Yes Nick that's yeah that's correct. Yeah the current

81
00:10:35,800 --> 00:10:39,640
version of LLAMA does not meet those but they could in the future.

82
00:10:39,640 --> 00:10:43,640
That would be great.

83
00:11:04,360 --> 00:11:09,080
I see Martin is typing. Nick.

84
00:11:09,080 --> 00:11:15,160
I guess is everyone else who's still on the call

85
00:11:15,160 --> 00:11:19,240
is there anything else that you'd like

86
00:11:19,240 --> 00:11:23,960
to me to cover? Anything else you'd like me to talk about?

87
00:11:23,960 --> 00:11:29,880
Nick is just writing a clear definition will help organizations

88
00:11:29,880 --> 00:11:33,400
to make a choice as to whether they want to release an AI system

89
00:11:33,400 --> 00:11:40,200
as an open source AI or not. Okay yeah.

90
00:11:40,200 --> 00:11:47,960
Okay Martin is saying hoping if any indications made about

91
00:11:47,960 --> 00:11:51,960
if in desiring to conform but it sees not yet and it's obviously

92
00:11:51,960 --> 00:11:59,000
fine. Yeah I imagine it's their conversations happening internally.

93
00:12:01,960 --> 00:12:05,720
Releasing smaller models that do miss the OSAID.

94
00:12:05,720 --> 00:12:09,320
That's one option. Yep.

95
00:12:09,320 --> 00:12:11,320
Yep.

96
00:12:21,000 --> 00:12:24,840
Yes that's well put. Nick is saying a clear definition will help guide

97
00:12:24,840 --> 00:12:27,400
policymakers around the world and that was

98
00:12:27,400 --> 00:12:31,160
the reason that we started this in the first place. I actually wasn't even

99
00:12:31,160 --> 00:12:34,440
there in the beginning. Nick was there and Stefano was there.

100
00:12:34,440 --> 00:12:38,760
The board was there but yeah that was the goal for doing this is to have

101
00:12:38,760 --> 00:12:43,960
something clear that policymakers can rely on globally like the

102
00:12:43,960 --> 00:12:51,400
open source definition currently. Yes and the EU AI Act is

103
00:12:51,400 --> 00:12:56,600
is one clear application of this work.

104
00:13:00,040 --> 00:13:07,800
Okay I see people dropping off which is fine. I think I will do a last

105
00:13:07,800 --> 00:13:12,120
call for questions or comments.

106
00:13:12,120 --> 00:13:15,960
And then

107
00:13:15,960 --> 00:13:21,960
okay

108
00:13:37,960 --> 00:13:44,280
Oh you posted on the forum. Kitty I'll post on the forum.

109
00:13:44,280 --> 00:13:53,320
Okay discussing a forum post.

110
00:13:53,320 --> 00:14:03,080
I'm not sure if folks have read it. I would say ideally

111
00:14:03,080 --> 00:14:09,960
this is your opportunity to click on that link that Kitty has

112
00:14:09,960 --> 00:14:14,440
just shared and take a read and comment

113
00:14:14,440 --> 00:14:18,680
on his post. I think that's probably the thing to do.

114
00:14:18,680 --> 00:14:21,960
Yes and thank you for participating in the forum.

115
00:14:21,960 --> 00:14:24,440
Thank you.

116
00:14:24,440 --> 00:14:32,840
All right I think let's call it Nick. Yes beautiful perfect.

117
00:14:32,840 --> 00:14:36,840
Yes Nick has just dropped the link to the

118
00:14:36,840 --> 00:14:43,400
forum and I guess we can turn off the recording and

119
00:14:43,400 --> 00:14:48,440
Nick if you could hang out for a minute I just wanted to ask you something.

### End of last town hall held on 2024-08-23 ###

### Start of next town hall held on 2024-09-06 ###
--- Presentation for 2024-09-06 ---
OPEN SOURCE AI DEFINITION
Online public townhall
September 6, 2024
last updated: September 4, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

Open Source AI Deﬁnition

New Version 0.0.9

Released August 22, 2024

3

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open
Source AI System must include:

Open

Weights
Model weights and
parameters

Open

+ Code +
Source code used
to train and run
the system

Data

Information
The dataset or
detailed information
about the data used
to train the system

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Weights
The model weights and
parameters, made available
under OSI-approved terms
Examples: checkpoints from key intermediate stages of training as
well as the ﬁnal optimizer state

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Code
The source code used to train and
run the system, made available
with OSI-approved licenses
Examples: code used for pre-processing data, training, validation and testing,
supporting libraries like tokenizers and hyperparameters search code,
inference code, and model architecture.

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Data

Information
Sufﬁciently detailed information about the data used to
train the system, so that a skilled person can recreate a
substantially equivalent system using the same or
similar data. Data information shall be made available
with licenses that comply with the Open Source
Deﬁnition.
Examples: training methodologies and
techniques, training data sets used, information
about the provenance of those data sets, their scope and characteristics, how the data
was obtained and selected, the labeling procedures and data cleaning methodologies.

Training Data in the OSAID
OSI afﬁrms the beneﬁts of full access to training data while acknowledging it
is not always possible for reasons of law, privacy norms, technical feasibility,
and cultural practice.
• Training data is valuable to study AI systems: to understand the biases that have been
learned, which can impact system behavior. But training data is not part of the preferred
form for making modiﬁcations to an existing AI system
• Data can be hard to share. Laws that permit training on data often limit the resharing of that
same data to protect copyright or other interests.
• Privacy rules also give a person the rightful ability to control their most sensitive information,
such as decisions about their health.
• Similarly, much of the world’s Indigenous knowledge is protected through mechanisms that
are not compatible with later-developed frameworks for rights exclusivity and sharing.
• Open training data (data that can be reshared) provides the best way to enable users to study
the system, along with the preferred form of making modiﬁcations.
• Public training data (data others can inspect as long as it remains available) also enables
users to study the work, along with the preferred form.

OSAID Approval Criteria
OSI Board requires a deﬁnition that is:
Supported
by diverse
stakeholders

Provides
real-life
examples

Ready by
October 2024

The deﬁnition
needs to have
approval by end
users, developers,
deployers and
subjects of AI,
globally.

The deﬁnition must
include relevant
examples of AI
systems that comply
with it at the time of
approval, so cannot
have an empty set.

A usable version
of the deﬁnition
needs to be ready
for approval by
the board at the
October board
meeting.

Approved June 21, 2024

Open Source AI Deﬁnition

What’s Next?

September - October 2024

● Resolve comments, release RC1
● Launch stable version at All Things Opens

10

Endorse the OSAID!
● In preparation for the ﬁnalization of RC1 at the
end of this month, we are seeking both individual
and organizational endorsements of the OSAID.
● “Endorsement” means your name and
organizational affiliation will be appended to a
press release announcing RC1.
● If you or your organization are interested in
endorsing the OSAID, email me or Mer at
Mer@dobiggood.com
11

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

June

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.8

July

August

September

October

Virtual System
Review

Virtual System
Review

Pause in Virtual
System Review

Pause in Virtual
System Review

Townhalls +

Townhalls +

Townhalls +

- OSPOs for
Good (NYC)
- OSCA (virtual)

- AI-dev (Hong
Kong)

Townhalls +
- Deep Learning Indaba
(Dakar)
- IndiaFOSS (Bengaluru)
- RegenAI (Ashland)
- Nerdearla (Buenos Aires)

Draft 0.0.8

Draft 0.0.9

RC1

- OSAI Data
Workshop (Paris)
- All Things Open
(Raleigh - launch!)

Stable
Version

How to Participate :)
● Endorse the OSAID!
○ Email Stefano or Mer
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
13

Q&A

14

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the deﬁnition.

15


--- Subtitles for 2024-09-06 --- ###
1
00:00:00,001 --> 00:00:06,040
Let's start recording our session and get started.

2
00:00:06,040 --> 00:00:07,280
Welcome, everyone.

3
00:00:07,280 --> 00:00:09,960
This is our new format--

4
00:00:09,960 --> 00:00:11,840
well, slightly new format in the sense

5
00:00:11,840 --> 00:00:15,880
that we're going to be doing these town halls now weekly,

6
00:00:15,880 --> 00:00:20,200
going into the session, going into the final stretch

7
00:00:20,200 --> 00:00:24,640
before we have the open source AI definition reviewed

8
00:00:24,640 --> 00:00:29,520
and approved by the board at the end of October.

9
00:00:29,520 --> 00:00:33,560
At the board meeting that we have scheduled then.

10
00:00:33,560 --> 00:00:37,240
So a quick reminder of our rules of engagement,

11
00:00:37,240 --> 00:00:39,800
our community agreements that we call them.

12
00:00:39,800 --> 00:00:42,160
This is our rules.

13
00:00:42,160 --> 00:00:44,440
We want to have--

14
00:00:44,440 --> 00:00:46,800
making sure that there are no overlaps

15
00:00:46,800 --> 00:00:50,520
and people keep space for speaking and listening.

16
00:00:50,520 --> 00:00:55,320
So take space if you are shy and tend not to speak.

17
00:00:55,320 --> 00:00:59,000
But also, if you tend to speak a lot, take a break.

18
00:00:59,000 --> 00:01:03,080
Consider others and ask questions.

19
00:01:03,080 --> 00:01:08,040
Give space for others to come in and give their own things.

20
00:01:08,040 --> 00:01:09,160
Be kind.

21
00:01:09,160 --> 00:01:15,840
And the ones that I look always with in mind

22
00:01:15,840 --> 00:01:19,160
is the think about the fact that we want to continue moving.

23
00:01:19,160 --> 00:01:24,680
We can't stop discussing for too long.

24
00:01:24,680 --> 00:01:26,960
We need to notice the places where

25
00:01:26,960 --> 00:01:30,440
we have disagreement or difficult, complicated issues.

26
00:01:30,440 --> 00:01:36,000
Maybe put a note in there and get back to this later.

27
00:01:36,000 --> 00:01:40,040
And always look for solutions.

28
00:01:40,040 --> 00:01:43,440
Listing the problems is OK to start the conversation,

29
00:01:43,440 --> 00:01:45,920
but moving on.

30
00:01:45,920 --> 00:01:50,240
And so let's review for-- this is the first time

31
00:01:50,240 --> 00:01:52,680
we do this at this time of the week.

32
00:01:52,680 --> 00:01:55,920
So you may have not seen the new version that

33
00:01:55,920 --> 00:02:00,440
was released a couple of weeks ago, version 0.9.

34
00:02:00,440 --> 00:02:03,360
We have presented it in--

35
00:02:03,360 --> 00:02:09,880
I mean, we were in China with Mayor Joyce when this came out.

36
00:02:09,880 --> 00:02:15,600
And basically, the principles haven't changed between 0.8

37
00:02:15,600 --> 00:02:16,640
and 0.9.

38
00:02:16,640 --> 00:02:19,880
The principles are that we want to have-- for an open source

39
00:02:19,880 --> 00:02:23,320
AI, we want to have three kind of components

40
00:02:23,320 --> 00:02:27,160
that can be grouped into three buckets.

41
00:02:27,160 --> 00:02:30,880
The weights, the parameters, the architecture,

42
00:02:30,880 --> 00:02:33,920
anything that is related to what we

43
00:02:33,920 --> 00:02:35,920
call the model in machine learning

44
00:02:35,920 --> 00:02:38,160
needs to be made available under--

45
00:02:38,160 --> 00:02:39,280
needs to be made available.

46
00:02:39,280 --> 00:02:43,760
We'll talk about the conditions for openness or availability.

47
00:02:43,760 --> 00:02:47,280
The open-- the code used to train the system

48
00:02:47,280 --> 00:02:49,480
needs to be made available also.

49
00:02:49,480 --> 00:02:56,160
And also, you need to have a way to run the model,

50
00:02:56,160 --> 00:02:58,760
run the AI system.

51
00:02:58,760 --> 00:03:01,400
And then you need the data.

52
00:03:01,400 --> 00:03:04,040
Everyone understands and agrees on the fact

53
00:03:04,040 --> 00:03:10,280
that the data is where the parameters, the weights come

54
00:03:10,280 --> 00:03:11,960
from.

55
00:03:11,960 --> 00:03:14,880
And you need the data.

56
00:03:14,880 --> 00:03:18,040
And the coalition, the group that

57
00:03:18,040 --> 00:03:22,520
has discussed for many months how to solve the--

58
00:03:22,520 --> 00:03:25,920
how to pass the big boulder of data

59
00:03:25,920 --> 00:03:31,680
being many things in different legislations of the world

60
00:03:31,680 --> 00:03:36,600
came with the description of the data, the requirement

61
00:03:36,600 --> 00:03:39,840
the data has sufficiently detailed information

62
00:03:39,840 --> 00:03:42,240
about the data used to train the system.

63
00:03:42,240 --> 00:03:45,240
And that includes either the data set itself,

64
00:03:45,240 --> 00:03:53,760
when it's possible and legally plausible to distribute safely,

65
00:03:53,760 --> 00:03:56,360
or in alternative--

66
00:03:56,360 --> 00:03:59,960
and also-- not in alternative, but also--

67
00:03:59,960 --> 00:04:03,440
also the code with the full instructions

68
00:04:03,440 --> 00:04:05,640
on how to replicate that data set.

69
00:04:05,640 --> 00:04:11,440
And that includes the scripts to download the original data

70
00:04:11,440 --> 00:04:18,160
and content, the code to run all the interesting--

71
00:04:18,160 --> 00:04:21,720
all the interesting manipulation to go from the raw data

72
00:04:21,720 --> 00:04:25,200
to the training data set.

73
00:04:25,200 --> 00:04:34,400
And so the definition itself has the text

74
00:04:34,400 --> 00:04:39,120
of what is required in a paragraph titled,

75
00:04:39,120 --> 00:04:41,720
what is the preferred form to make modifications

76
00:04:41,720 --> 00:04:44,720
for a machine learning system?

77
00:04:44,720 --> 00:04:47,920
And describes the weights and gives some high level examples.

78
00:04:47,920 --> 00:04:52,800
These are not-- this is not text that is strictly correct,

79
00:04:52,800 --> 00:04:53,520
always--

80
00:04:53,520 --> 00:04:58,440
always the-- always valid for every possible--

81
00:04:58,440 --> 00:05:02,720
very strictly prescriptive about every possible system,

82
00:05:02,720 --> 00:05:05,520
every possible kind of technology

83
00:05:05,520 --> 00:05:08,720
that we can see today or available in the future.

84
00:05:08,720 --> 00:05:12,800
These are examples that are useful to interpret

85
00:05:12,800 --> 00:05:17,160
the actual meaning of the words above.

86
00:05:17,160 --> 00:05:21,480
So for the model weights parameters,

87
00:05:21,480 --> 00:05:24,080
that might include the checkpoints, for example,

88
00:05:24,080 --> 00:05:28,160
if the training in the system is a large language model,

89
00:05:28,160 --> 00:05:29,880
for example.

90
00:05:29,880 --> 00:05:33,080
From the code perspective, it's the source code

91
00:05:33,080 --> 00:05:35,280
used to train the system.

92
00:05:35,280 --> 00:05:40,800
And this includes all the pre-processing code,

93
00:05:40,800 --> 00:05:43,440
the training, the validation, the testing,

94
00:05:43,440 --> 00:05:46,480
how it's been done, the supporting libraries,

95
00:05:46,480 --> 00:05:50,760
and all of that code software.

96
00:05:50,760 --> 00:05:56,720
And this, obviously, need to be made available

97
00:05:56,720 --> 00:06:01,840
with open source AI approved licenses.

98
00:06:01,840 --> 00:06:03,320
And for the data--

99
00:06:03,320 --> 00:06:05,880
for the database, again, there is a-- this

100
00:06:05,880 --> 00:06:08,240
is a little bit more worthy.

101
00:06:08,240 --> 00:06:12,120
The intention here is that the open source AI,

102
00:06:12,120 --> 00:06:13,840
the developers of open source AI,

103
00:06:13,840 --> 00:06:18,680
must be sharing with others all the instructions

104
00:06:18,680 --> 00:06:22,880
and all the knowledge that they have on how they built.

105
00:06:22,880 --> 00:06:24,000
That's the intention.

106
00:06:24,000 --> 00:06:28,320
The intention is to make sure that open source

107
00:06:28,320 --> 00:06:36,280
AI carries the same meaning and the same practical values

108
00:06:36,280 --> 00:06:38,760
that the open source software definition carries.

109
00:06:38,760 --> 00:06:41,360
You need to be able to understand--

110
00:06:41,360 --> 00:06:42,840
you need to be able to understand

111
00:06:42,840 --> 00:06:45,800
how the system's been built, its intention, et cetera.

112
00:06:45,800 --> 00:06:49,320
And you need to be able to learn from them,

113
00:06:49,320 --> 00:06:51,880
from the original developers, and build on top of them

114
00:06:51,880 --> 00:06:55,720
without having to reinvent the wheel or try to guess.

115
00:06:55,720 --> 00:06:57,840
Maybe this thing has been done this way,

116
00:06:57,840 --> 00:07:01,840
so I can improve it by doing this other thing.

117
00:07:01,840 --> 00:07:03,280
No, it's building on top.

118
00:07:03,280 --> 00:07:05,720
And this is where the--

119
00:07:05,720 --> 00:07:08,840
building on top of what others have built,

120
00:07:08,840 --> 00:07:10,840
one of the basic tenets of open source.

121
00:07:10,840 --> 00:07:21,440
But we do have spent--

122
00:07:21,440 --> 00:07:25,720
I mean, the community, and together with the--

123
00:07:25,720 --> 00:07:26,800
also with the board.

124
00:07:26,800 --> 00:07:28,600
And we have reviewed a lot of the comments

125
00:07:28,600 --> 00:07:31,600
that we have received in the past.

126
00:07:31,600 --> 00:07:36,320
And we have clarified also that we do have--

127
00:07:36,320 --> 00:07:39,120
there is a-- we do have-- we do care about data.

128
00:07:39,120 --> 00:07:41,440
We do care about the availability of the training

129
00:07:41,440 --> 00:07:41,920
data.

130
00:07:41,920 --> 00:07:44,640
We do know that the training data

131
00:07:44,640 --> 00:07:49,440
is valuable to understand and study the AI systems.

132
00:07:49,440 --> 00:07:54,720
And we also want to acknowledge that there

133
00:07:54,720 --> 00:07:58,360
is different kind of data.

134
00:07:58,360 --> 00:08:01,920
There is data that computers share,

135
00:08:01,920 --> 00:08:04,000
and that would be--

136
00:08:04,000 --> 00:08:08,840
make sense to always have open in terms of openly accessible

137
00:08:08,840 --> 00:08:13,080
and as open data, following the definition of-- maintained

138
00:08:13,080 --> 00:08:15,520
by the Open Knowledge Foundation.

139
00:08:15,520 --> 00:08:18,800
But there is also training data that is simply public,

140
00:08:18,800 --> 00:08:21,040
cannot be redistributed.

141
00:08:21,040 --> 00:08:26,120
And that is basically the whole of the whole internet.

142
00:08:26,120 --> 00:08:34,400
And where you have the right to crawl and build indexes on it.

143
00:08:34,400 --> 00:08:37,360
This is one of the basic rights that Google

144
00:08:37,360 --> 00:08:42,120
has had as a search engine for forever, since its existence.

145
00:08:42,120 --> 00:08:44,960
And we need to continue to acknowledge the fact

146
00:08:44,960 --> 00:08:49,960
that while search engines can crawl the internet,

147
00:08:49,960 --> 00:08:52,520
they don't have the right to redistribute

148
00:08:52,520 --> 00:08:53,680
what they have crawled.

149
00:08:53,680 --> 00:08:57,080
But they have the right to offer the public

150
00:08:57,080 --> 00:09:01,240
some new and transformative activity, actions,

151
00:09:01,240 --> 00:09:03,480
and services.

152
00:09:03,480 --> 00:09:05,960
So this is the same thing.

153
00:09:05,960 --> 00:09:08,840
This is the concept of public training data

154
00:09:08,840 --> 00:09:10,800
that is publicly available.

155
00:09:10,800 --> 00:09:13,360
And then there is private data, which is another category

156
00:09:13,360 --> 00:09:17,880
of data for which you may have the right to train on,

157
00:09:17,880 --> 00:09:22,120
but you don't have the right to redistribute.

158
00:09:22,120 --> 00:09:26,840
And so a reminder that we are working

159
00:09:26,840 --> 00:09:34,800
within the constraints of the policies

160
00:09:34,800 --> 00:09:41,120
that the board has set as criteria to have a definition.

161
00:09:41,120 --> 00:09:42,520
The board wanted to have--

162
00:09:42,520 --> 00:09:45,040
would ask us to work with the community

163
00:09:45,040 --> 00:09:47,320
to understand, to have a definition that

164
00:09:47,320 --> 00:09:51,680
is supported by a large coalition of individuals,

165
00:09:51,680 --> 00:09:55,800
organizations, and groups with globally representative--

166
00:09:55,800 --> 00:09:57,320
so a sample that is representative

167
00:09:57,320 --> 00:10:00,320
of global communities, but also represent

168
00:10:00,320 --> 00:10:01,840
various different interests.

169
00:10:01,840 --> 00:10:05,640
We're not just representing the interest of research

170
00:10:05,640 --> 00:10:07,720
and academia of individual developers,

171
00:10:07,720 --> 00:10:12,560
or large corporations, small corporations,

172
00:10:12,560 --> 00:10:15,840
European corporations, or governments

173
00:10:15,840 --> 00:10:17,480
from other parts of the country.

174
00:10:17,480 --> 00:10:19,320
But it's the whole--

175
00:10:19,320 --> 00:10:25,040
we try to be as global and diverse as possible.

176
00:10:25,040 --> 00:10:28,480
The other constraints that the board set for us

177
00:10:28,480 --> 00:10:30,640
is that we need to provide real-life examples.

178
00:10:30,640 --> 00:10:33,320
You can't really have a definition

179
00:10:33,320 --> 00:10:39,440
that defines theoretical models, theoretical systems that

180
00:10:39,440 --> 00:10:43,560
don't have any application in practice.

181
00:10:43,560 --> 00:10:47,640
It would be not acceptable by the board.

182
00:10:47,640 --> 00:10:49,920
And we also wanted to set a deadline,

183
00:10:49,920 --> 00:10:56,240
because it's a hard deadline, because otherwise--

184
00:10:56,240 --> 00:10:59,720
because the world needs this definition soon.

185
00:10:59,720 --> 00:11:01,200
And it's better to have something

186
00:11:01,200 --> 00:11:07,680
that is done rather than perfect.

187
00:11:07,680 --> 00:11:08,560
So what's next?

188
00:11:08,560 --> 00:11:11,640
In the next few months, we are working

189
00:11:11,640 --> 00:11:15,280
to really solve the comments as they come,

190
00:11:15,280 --> 00:11:19,800
and maybe release in the next weeks, couple of weeks,

191
00:11:19,800 --> 00:11:22,240
release candidate version.

192
00:11:22,240 --> 00:11:29,760
And then get a quick feedback with the top organizations,

193
00:11:29,760 --> 00:11:33,960
groups that have worked in the process

194
00:11:33,960 --> 00:11:37,120
to gain their endorsements, gather the last comments,

195
00:11:37,120 --> 00:11:41,880
and march towards a stable version for all things

196
00:11:41,880 --> 00:11:46,000
open on October 27.

197
00:11:46,000 --> 00:11:49,760
So we are seeking now--

198
00:11:49,760 --> 00:11:52,760
we're at the stage where we are seeking the comments

199
00:11:52,760 --> 00:11:55,640
from individuals and organizational endorsements

200
00:11:55,640 --> 00:11:57,120
for--

201
00:11:57,120 --> 00:12:00,040
comments and endorsements for the draft.

202
00:12:00,040 --> 00:12:03,760
So if you have--

203
00:12:03,760 --> 00:12:05,840
if you're ready to say, yeah, we're

204
00:12:05,840 --> 00:12:08,000
ready to endorse these principles,

205
00:12:08,000 --> 00:12:10,440
just send us an email.

206
00:12:10,440 --> 00:12:13,520
And you can email me or you can email Claire.

207
00:12:13,520 --> 00:12:16,440
I've been assisting our project.

208
00:12:16,440 --> 00:12:17,760
You can get in touch with Nick.

209
00:12:17,760 --> 00:12:23,160
You can even go public on the forum already as you prefer.

210
00:12:23,160 --> 00:12:26,760
We want to have--

211
00:12:26,760 --> 00:12:28,080
we need to start moving.

212
00:12:28,080 --> 00:12:31,200
We are already at the first week of September,

213
00:12:31,200 --> 00:12:34,760
and we have only six weeks of time

214
00:12:34,760 --> 00:12:40,120
basically left before we finish the process.

215
00:12:40,120 --> 00:12:45,080
And yeah, we've covered a lot of space, a lot of time

216
00:12:45,080 --> 00:12:45,720
over the months.

217
00:12:45,720 --> 00:12:47,240
We've been traveling quite a bit.

218
00:12:47,240 --> 00:12:54,040
We're trying to be presenting in many parts of the world,

219
00:12:54,040 --> 00:12:58,160
presenting and discussing with the community

220
00:12:58,160 --> 00:13:03,280
to make sure that there are the least amount of surprises

221
00:13:03,280 --> 00:13:07,480
by the time we issue the version, version 1,

222
00:13:07,480 --> 00:13:11,120
stable version in all things open.

223
00:13:11,120 --> 00:13:13,160
I can also give you a preview.

224
00:13:13,160 --> 00:13:15,280
You see here in October, there is--

225
00:13:15,280 --> 00:13:17,080
we have two events scheduled.

226
00:13:17,080 --> 00:13:20,120
One is the all things open launch,

227
00:13:20,120 --> 00:13:22,840
but also there is a workshop that we're

228
00:13:22,840 --> 00:13:27,080
organizing for around specifically about the issue

229
00:13:27,080 --> 00:13:32,200
to discuss and understand better the space of data governance

230
00:13:32,200 --> 00:13:34,160
distribution, et cetera.

231
00:13:34,160 --> 00:13:40,440
We'll have more details this coming week made available.

232
00:13:40,440 --> 00:13:43,560
And this is all thanks-- all the travel and all the--

233
00:13:43,560 --> 00:13:48,280
and the workshop that we're organizing in Paris

234
00:13:48,280 --> 00:13:54,120
is thanks to a grant, a large grant given to us

235
00:13:54,120 --> 00:13:58,880
by the Alfred P. Sloan Foundation.

236
00:13:58,880 --> 00:14:00,240
So how to participate?

237
00:14:00,240 --> 00:14:03,680
You can definitely still email me or Mer,

238
00:14:03,680 --> 00:14:07,400
the preferred for the endorsements.

239
00:14:07,400 --> 00:14:09,440
You can also send public comments

240
00:14:09,440 --> 00:14:12,080
on discuss.opensource.org.

241
00:14:12,080 --> 00:14:16,160
Can definitely signal that you appreciate

242
00:14:16,160 --> 00:14:19,000
the stewardship and the role of the Open Source

243
00:14:19,000 --> 00:14:20,960
Initiative in driving this process

244
00:14:20,960 --> 00:14:22,760
by becoming an OSI member.

245
00:14:22,760 --> 00:14:26,040
You can also donate as you become a member.

246
00:14:26,040 --> 00:14:28,400
And you can join these downloads that--

247
00:14:28,400 --> 00:14:30,360
realize this slide needs to be updated.

248
00:14:30,360 --> 00:14:33,800
Coming-- there's-- there being--

249
00:14:33,800 --> 00:14:38,600
there being-- they're opening now weekly at alternating time.

250
00:14:38,600 --> 00:14:40,240
One week is going to be at this time.

251
00:14:40,240 --> 00:14:46,400
One other week is going to be at 9 AM Central European time

252
00:14:46,400 --> 00:14:50,680
so that the Asian community can join.

253
00:14:50,680 --> 00:14:58,040
And now-- yeah, now we've got time for Q&A.

254
00:14:58,040 --> 00:14:59,960
So a comment from YouTube, an open AI

255
00:14:59,960 --> 00:15:03,840
should prove that its training data is all legally licensed

256
00:15:03,840 --> 00:15:05,000
open source data.

257
00:15:05,000 --> 00:15:07,120
Right, so this is a very interesting question

258
00:15:07,120 --> 00:15:10,520
because it's a frequently asked question, actually.

259
00:15:10,520 --> 00:15:14,080
And so it's been debated for many months.

260
00:15:14,080 --> 00:15:20,960
The short answer is that legally licensed open source data is--

261
00:15:20,960 --> 00:15:24,720
is a-- is a big--

262
00:15:24,720 --> 00:15:28,400
is a big-- is a different--

263
00:15:28,400 --> 00:15:34,360
is a different set of what you think it might be.

264
00:15:34,360 --> 00:15:37,440
And we may want to qualify.

265
00:15:37,440 --> 00:15:42,920
So think about the fact that Google Books, for example,

266
00:15:42,920 --> 00:15:47,680
has built a product built on legally--

267
00:15:47,680 --> 00:15:53,720
legally acquired books, scanned, and done object character

268
00:15:53,720 --> 00:16:00,480
recognition on it, and created a transformative work,

269
00:16:00,480 --> 00:16:02,400
and was sued.

270
00:16:02,400 --> 00:16:06,240
And then they won the lawsuit.

271
00:16:06,240 --> 00:16:10,320
Makes me-- makes us think that the concept of training data

272
00:16:10,320 --> 00:16:12,280
legally licensed open source data

273
00:16:12,280 --> 00:16:15,000
is something that needs to be--

274
00:16:15,000 --> 00:16:16,080
needs to be clarified.

275
00:16:16,080 --> 00:16:21,160
So it's a gut reaction that we all go towards.

276
00:16:21,160 --> 00:16:24,120
But we need to think about the consequences.

277
00:16:24,120 --> 00:16:27,040
It's a big-- it's a big topic.

278
00:16:27,040 --> 00:16:34,960
And that's why we're hosting that conference in Paris also.

279
00:16:34,960 --> 00:16:37,040
So Joshua.

280
00:16:37,040 --> 00:16:41,360
Hello.

281
00:16:41,360 --> 00:16:43,080
This is Joshua.

282
00:16:43,080 --> 00:16:49,160
So in the latest draft, I've been

283
00:16:49,160 --> 00:16:50,840
spending most of my time really trying

284
00:16:50,840 --> 00:16:54,480
to think through reading the definition

285
00:16:54,480 --> 00:17:00,640
from different viewpoints of users, especially

286
00:17:00,640 --> 00:17:02,360
a developer viewpoint.

287
00:17:02,360 --> 00:17:06,040
And so I've been really digging in and thinking about

288
00:17:06,040 --> 00:17:09,280
if I'm using Protobuf or TensorFlow

289
00:17:09,280 --> 00:17:19,320
or these other common frameworks and formats for AI models, what

290
00:17:19,320 --> 00:17:21,240
my world view looks like.

291
00:17:21,240 --> 00:17:25,400
And one thing that really stands out to me

292
00:17:25,400 --> 00:17:29,680
is that generally speaking, we're

293
00:17:29,680 --> 00:17:33,120
gravitating towards these binary formats as what

294
00:17:33,120 --> 00:17:36,880
we understand as the model.

295
00:17:36,880 --> 00:17:40,920
And this is different than the abstract kind of approach

296
00:17:40,920 --> 00:17:44,400
to thinking about what an AI model is in general.

297
00:17:44,400 --> 00:17:49,640
It's, in a sense, broader than the AI model,

298
00:17:49,640 --> 00:17:51,720
which is more narrow, which is counterintuitive.

299
00:17:51,720 --> 00:17:55,880
You would think it would be just the opposite.

300
00:17:55,880 --> 00:17:57,720
And the reason why it's somewhat broader

301
00:17:57,720 --> 00:18:01,280
is because these binary formats allow

302
00:18:01,280 --> 00:18:04,640
you to do a lot of things that go perhaps

303
00:18:04,640 --> 00:18:08,040
beyond what you would normally think of as part of the AI

304
00:18:08,040 --> 00:18:09,040
model.

305
00:18:09,040 --> 00:18:13,480
But when you need to identify in your code base what

306
00:18:13,480 --> 00:18:16,640
is the model, you point to that file.

307
00:18:16,640 --> 00:18:19,160
That's the model.

308
00:18:19,160 --> 00:18:20,920
And so I think that there's going

309
00:18:20,920 --> 00:18:24,880
to be some gap between a lot of people's

310
00:18:24,880 --> 00:18:27,680
intuitive understanding of what the model is

311
00:18:27,680 --> 00:18:33,920
and what the intent of the model is by OS in this definition.

312
00:18:33,920 --> 00:18:40,880
I think it would be helpful to put a little bit more

313
00:18:40,880 --> 00:18:42,560
in the definition.

314
00:18:42,560 --> 00:18:50,800
In the online feedback forum for the latest definition,

315
00:18:50,800 --> 00:18:55,920
I added some comments specifically around this idea

316
00:18:55,920 --> 00:19:00,760
that you should be encouraging you

317
00:19:00,760 --> 00:19:04,640
to be explicit about saying that you should not

318
00:19:04,640 --> 00:19:09,400
use the AI model as a way to pass through, say,

319
00:19:09,400 --> 00:19:12,760
binary blobs or object code.

320
00:19:12,760 --> 00:19:18,200
And the reason why I sort of came to that conclusion

321
00:19:18,200 --> 00:19:21,720
is because I was looking at real examples

322
00:19:21,720 --> 00:19:28,760
where people were using things like Protobuf and other--

323
00:19:28,760 --> 00:19:35,320
in TensorFlow and using variables that just

324
00:19:35,320 --> 00:19:40,320
store data effectively as part of their training.

325
00:19:40,320 --> 00:19:45,760
So they're just storing verbatim blobs of information

326
00:19:45,760 --> 00:19:48,520
that are then used as part of the program.

327
00:19:48,520 --> 00:19:52,800
But it's that interaction, but it's stored in the model

328
00:19:52,800 --> 00:19:55,040
from their perspective, from the user's perspective,

329
00:19:55,040 --> 00:19:56,760
and in the documentation.

330
00:19:56,760 --> 00:20:00,480
So I think it would be helpful to have a little bit more

331
00:20:00,480 --> 00:20:05,000
nuance in the definition to clarify that a trained model is

332
00:20:05,000 --> 00:20:10,000
not the same as merely storing files that can then be reused,

333
00:20:10,000 --> 00:20:16,240
that it should be about a little bit more detail around that,

334
00:20:16,240 --> 00:20:23,800
just to help abuse of the open source AI definition.

335
00:20:23,800 --> 00:20:26,520
I think-- yeah.

336
00:20:26,520 --> 00:20:29,800
So I'm not sure I understand exactly--

337
00:20:29,800 --> 00:20:32,760
in fact, I was emailing you to have a conversation

338
00:20:32,760 --> 00:20:34,520
to clarify what you actually meant.

339
00:20:34,520 --> 00:20:42,640
And I'm glad you came for this and explained it via voice.

340
00:20:42,640 --> 00:20:44,440
So I'm still a little bit puzzled

341
00:20:44,440 --> 00:20:49,320
by the technical details here.

342
00:20:49,320 --> 00:20:55,080
But the general principle of the definition

343
00:20:55,080 --> 00:21:02,160
is that it must resist as much as possible the test of time.

344
00:21:02,160 --> 00:21:06,520
And it needs to set high-level principles at this stage.

345
00:21:06,520 --> 00:21:13,800
If you want to have a mental map to frame the intention here,

346
00:21:13,800 --> 00:21:16,240
the open source AI definition file,

347
00:21:16,240 --> 00:21:19,960
the way you see it linked in the chat,

348
00:21:19,960 --> 00:21:23,960
is the page that the FSF, the Free Software Foundation,

349
00:21:23,960 --> 00:21:26,480
hosts and calls what is free software.

350
00:21:26,480 --> 00:21:31,120
It's the basic principles that we

351
00:21:31,120 --> 00:21:37,720
want to have represented in a view of the world, what

352
00:21:37,720 --> 00:21:39,880
needs to be achieved.

353
00:21:39,880 --> 00:21:42,520
That page has gone through multiple iterations.

354
00:21:42,520 --> 00:21:45,360
Like, if you go below and you read it,

355
00:21:45,360 --> 00:21:49,080
the initial freedoms were three, and then a fourth was added.

356
00:21:49,080 --> 00:21:52,760
And wording has been changed on that page to clarify.

357
00:21:52,760 --> 00:21:55,960
But the principles, what is free software, those three,

358
00:21:55,960 --> 00:21:58,120
and then letter four freedoms have pretty much

359
00:21:58,120 --> 00:21:59,480
remained the same.

360
00:21:59,480 --> 00:22:05,520
What has changed and has evolved and derived from that

361
00:22:05,520 --> 00:22:07,520
is the open source definition, which

362
00:22:07,520 --> 00:22:11,400
is a sort of a checklist to evaluate,

363
00:22:11,400 --> 00:22:15,880
in practical forms, software packages used by--

364
00:22:15,880 --> 00:22:20,200
in order to be included in the FTP servers at the Debian

365
00:22:20,200 --> 00:22:24,880
project first, and then became the open source definition

366
00:22:24,880 --> 00:22:27,400
that are currently used, those 10 principles,

367
00:22:27,400 --> 00:22:33,120
to evaluate the licenses and legal documents.

368
00:22:33,120 --> 00:22:36,280
We want-- we're trying to do the same thing here.

369
00:22:36,280 --> 00:22:40,600
The open source AI definition is what is free software page.

370
00:22:40,600 --> 00:22:46,680
And then the new split document, the checklist,

371
00:22:46,680 --> 00:22:50,800
is more of those 10 points practical interpretation

372
00:22:50,800 --> 00:22:53,560
of, in practice, of what will have

373
00:22:53,560 --> 00:22:58,680
to happen for an AI system to be judged, evaluated,

374
00:22:58,680 --> 00:23:02,520
to be respecting the original intention written

375
00:23:02,520 --> 00:23:04,480
in the definition.

376
00:23:04,480 --> 00:23:06,880
So what I would recommend is that we

377
00:23:06,880 --> 00:23:10,400
spend a little bit more time, collectively,

378
00:23:10,400 --> 00:23:13,400
to think about the interpretation.

379
00:23:13,400 --> 00:23:17,080
Instead of making the first document longer,

380
00:23:17,080 --> 00:23:19,120
we could be spending a little bit more time

381
00:23:19,120 --> 00:23:21,440
to refine and review the-- and we

382
00:23:21,440 --> 00:23:23,320
have more time with another set deadline

383
00:23:23,320 --> 00:23:26,240
to finish the checklist.

384
00:23:26,240 --> 00:23:31,480
Make that document a little bit more rich.

385
00:23:31,480 --> 00:23:35,500
Bentley?

386
00:23:38,800 --> 00:23:41,960
Yeah, I noticed this definition doesn't

387
00:23:41,960 --> 00:23:44,800
mention content generated by models

388
00:23:44,800 --> 00:23:47,840
under an open source license.

389
00:23:47,840 --> 00:23:50,560
This might be much a bigger question,

390
00:23:50,560 --> 00:23:55,480
but is there a plan to address the content generated

391
00:23:55,480 --> 00:23:58,600
by an open source model, how that will be licensed,

392
00:23:58,600 --> 00:24:03,360
or is that part of a much bigger discussion?

393
00:24:03,360 --> 00:24:06,200
That's a good question.

394
00:24:06,200 --> 00:24:11,040
So the jury's still out of what are the legal ramifications

395
00:24:11,040 --> 00:24:11,880
for that part.

396
00:24:11,880 --> 00:24:14,280
But definitely, the definition does not

397
00:24:14,280 --> 00:24:18,400
touch that the same way that, more or less,

398
00:24:18,400 --> 00:24:20,200
the open source software definition

399
00:24:20,200 --> 00:24:24,400
doesn't say whether you can use a C compiler to build

400
00:24:24,400 --> 00:24:29,680
malware or other things like that.

401
00:24:29,680 --> 00:24:33,440
It's a separate-- it would be a separate conversation.

402
00:24:33,440 --> 00:24:38,360
There might be legal documents that

403
00:24:38,360 --> 00:24:41,840
say you cannot use this model, this AI system,

404
00:24:41,840 --> 00:24:45,200
to create--

405
00:24:45,200 --> 00:24:47,600
to infringe on someone else's copyright

406
00:24:47,600 --> 00:24:51,280
or to invent things that have already been invented

407
00:24:51,280 --> 00:24:53,000
or do other things.

408
00:24:53,000 --> 00:24:55,280
But that's something that will have to be evaluated

409
00:24:55,280 --> 00:24:56,800
by the legal community.

410
00:24:56,800 --> 00:25:08,200
[AUDIO OUT]

411
00:25:08,200 --> 00:25:09,600
- I have a question.

412
00:25:09,600 --> 00:25:11,120
Can you hear me, Stefano?

413
00:25:11,120 --> 00:25:11,620
- Yes.

414
00:25:11,620 --> 00:25:17,240
- By way of introduction, I'm a tech attorney.

415
00:25:17,240 --> 00:25:18,840
I've been practicing for 15 years.

416
00:25:18,840 --> 00:25:21,360
And for better or for worse, I've

417
00:25:21,360 --> 00:25:24,400
done a lot of work in the open source space.

418
00:25:24,400 --> 00:25:28,560
The one question I had is, at the top of the call,

419
00:25:28,560 --> 00:25:30,400
you went through the definition.

420
00:25:30,400 --> 00:25:33,960
And you explained that there are three parts to the definition.

421
00:25:33,960 --> 00:25:36,960
Is it fair to state that the first two

422
00:25:36,960 --> 00:25:42,200
parts of that definition have not generated

423
00:25:42,200 --> 00:25:45,280
any controversy versus the third?

424
00:25:45,280 --> 00:25:48,960
- Yeah.

425
00:25:48,960 --> 00:25:51,240
Well, not completely true.

426
00:25:51,240 --> 00:25:54,120
So the first two parts, meaning the model weights

427
00:25:54,120 --> 00:25:57,240
and parameters, and the second part--

428
00:25:57,240 --> 00:25:58,320
- And the source code.

429
00:25:58,320 --> 00:26:00,240
- --the source code of--

430
00:26:00,240 --> 00:26:02,560
so there has been debate.

431
00:26:02,560 --> 00:26:04,800
And there will continue to be a little bit.

432
00:26:04,800 --> 00:26:06,720
In the legal community, I believe

433
00:26:06,720 --> 00:26:13,440
that the legal nature of the weights parameters

434
00:26:13,440 --> 00:26:16,760
is still being debated.

435
00:26:16,760 --> 00:26:20,840
Some legislations may not consider those subject

436
00:26:20,840 --> 00:26:23,560
to any exclusive right.

437
00:26:23,560 --> 00:26:26,400
And whether they should be considered

438
00:26:26,400 --> 00:26:32,480
under any IP law or other exclusive rights

439
00:26:32,480 --> 00:26:36,440
is to be debated.

440
00:26:36,440 --> 00:26:39,360
It's not ferocious debate, but it's an intellectually

441
00:26:39,360 --> 00:26:40,800
stimulating one.

442
00:26:40,800 --> 00:26:44,840
And on the code front, on the source code front,

443
00:26:44,840 --> 00:26:56,040
there is debate over whether the training code should

444
00:26:56,040 --> 00:26:58,160
be strictly required or not.

445
00:26:58,160 --> 00:27:07,040
And there is a little bit of a push and pull.

446
00:27:07,040 --> 00:27:11,000
I've heard rumors about that requirement,

447
00:27:11,000 --> 00:27:13,920
strict requirement, as being a little bit too limiting.

448
00:27:13,920 --> 00:27:14,560
Too limiting.

449
00:27:14,560 --> 00:27:19,400
- Got it.

450
00:27:19,400 --> 00:27:23,520
But just to clarify, the third component of the definition,

451
00:27:23,520 --> 00:27:25,400
with respect to the training data,

452
00:27:25,400 --> 00:27:26,760
is the most controversial.

453
00:27:26,760 --> 00:27:27,560
Is that correct?

454
00:27:27,560 --> 00:27:28,600
- Yeah.

455
00:27:28,600 --> 00:27:29,440
Correct, yes.

456
00:27:29,440 --> 00:27:34,280
Because, yes, instinctively, all of us,

457
00:27:34,280 --> 00:27:40,520
at the beginning of the process, everyone had the same thought.

458
00:27:40,520 --> 00:27:44,440
Data is where the weights come from.

459
00:27:44,440 --> 00:27:48,440
Some people use the term source, which is confusing.

460
00:27:48,440 --> 00:27:51,120
It's not source code in the same way.

461
00:27:51,120 --> 00:27:53,320
But it's where it comes from.

462
00:27:53,320 --> 00:27:55,840
If without the data, you don't have the models.

463
00:27:55,840 --> 00:27:57,840
You don't have the parameters.

464
00:27:57,840 --> 00:28:02,720
And therefore, given that requirement,

465
00:28:02,720 --> 00:28:04,320
all of the pipe, the whole pipeline

466
00:28:04,320 --> 00:28:07,560
needs to be open and open source.

467
00:28:07,560 --> 00:28:10,800
But data is not source code.

468
00:28:10,800 --> 00:28:19,160
It doesn't fall under the same easy, air quotes,

469
00:28:19,160 --> 00:28:20,960
legal framework.

470
00:28:20,960 --> 00:28:22,480
It's a whole different beast.

471
00:28:22,480 --> 00:28:25,760
And realizing that became the big boulder

472
00:28:25,760 --> 00:28:31,200
that we had to navigate around, find a way to navigate around.

473
00:28:31,200 --> 00:28:31,680
- Got it.

474
00:28:31,680 --> 00:28:32,160
Thank you.

475
00:28:32,160 --> 00:28:35,280
That's very helpful, Stefano.

476
00:28:35,280 --> 00:28:36,600
And I apologize.

477
00:28:36,600 --> 00:28:40,360
I have read a bunch of the comments on the various versions.

478
00:28:40,360 --> 00:28:42,640
But obviously, I don't have the full history here.

479
00:28:42,640 --> 00:28:46,040
And nobody knows this stuff better than yourself.

480
00:28:46,040 --> 00:28:52,800
Did OSI, at any point, consider using different words

481
00:28:52,800 --> 00:28:57,120
to describe models that would fit within this definition

482
00:28:57,120 --> 00:29:01,320
rather than open source as a way to resolve

483
00:29:01,320 --> 00:29:05,080
the complaints from various members of the community?

484
00:29:05,080 --> 00:29:06,920
Was that considered as an option?

485
00:29:06,920 --> 00:29:08,920
- That is a very awesome question.

486
00:29:08,920 --> 00:29:10,240
That is a very awesome question.

487
00:29:10,240 --> 00:29:12,200
It was one of the very first questions,

488
00:29:12,200 --> 00:29:15,280
is what we're going to call this thing.

489
00:29:15,280 --> 00:29:18,080
And yeah, I'm on the left.

490
00:29:18,080 --> 00:29:23,240
But yes, unfortunately, though, our hand

491
00:29:23,240 --> 00:29:28,280
was forced by the fact that open source AI, as a term,

492
00:29:28,280 --> 00:29:29,520
was already being used.

493
00:29:29,520 --> 00:29:35,840
And even abused by some players.

494
00:29:35,840 --> 00:29:37,600
And I can be public about it, because I've

495
00:29:37,600 --> 00:29:38,680
been public about it.

496
00:29:38,680 --> 00:29:40,960
Meta is one of the abusers of the term.

497
00:29:40,960 --> 00:29:43,560
They keep on using, referring to open source AI.

498
00:29:43,560 --> 00:29:48,760
So in order to safeguard open source, the term itself,

499
00:29:48,760 --> 00:29:54,280
we don't have another choice but to call it open source AI

500
00:29:54,280 --> 00:29:57,120
and work around it.

501
00:29:57,120 --> 00:29:57,640
- Thank you.

502
00:29:57,640 --> 00:29:59,600
That's very helpful context.

503
00:29:59,600 --> 00:30:01,480
That's all I have.

504
00:30:01,480 --> 00:30:01,980
- Yeah.

505
00:30:01,980 --> 00:30:09,680
I see Joshua typing.

506
00:30:09,680 --> 00:30:19,840
Are there any other questions from the Mozilla group or YouTube?

507
00:30:20,840 --> 00:30:21,880
Josh, I see you typing.

508
00:30:21,880 --> 00:30:22,920
Feel free to grab the mic.

509
00:30:22,920 --> 00:30:27,240
- Thanks.

510
00:30:27,240 --> 00:30:31,320
Yeah, it was just sort of a follow up.

511
00:30:31,320 --> 00:30:35,560
My point is that, in principle, we

512
00:30:35,560 --> 00:30:42,640
know that when training a model, that we

513
00:30:42,640 --> 00:30:52,880
know that when training a model, that it gets trained on code

514
00:30:52,880 --> 00:30:53,760
at times.

515
00:30:53,760 --> 00:30:56,440
Some of the data is code.

516
00:30:56,440 --> 00:30:59,160
Some of the data is object code.

517
00:30:59,160 --> 00:31:02,080
We know this because GitHub and other things

518
00:31:02,080 --> 00:31:07,240
are often used as the training data.

519
00:31:07,240 --> 00:31:13,560
And we know that models sometimes make use of just

520
00:31:13,560 --> 00:31:14,720
verbatim storage.

521
00:31:14,720 --> 00:31:21,320
Instead of taking the data and trying to learn something

522
00:31:21,320 --> 00:31:25,800
from it, sometimes you just save blobs of data in your model.

523
00:31:25,800 --> 00:31:31,120
All major models, model formats, TensorFlow, and so forth,

524
00:31:31,120 --> 00:31:33,920
have facilities for doing exactly that function.

525
00:31:36,600 --> 00:31:40,680
TensorFlow Save Models has a bunch of things.

526
00:31:40,680 --> 00:31:43,240
Protobuf has options.

527
00:31:43,240 --> 00:31:47,680
And I don't think, in principle, that is not what you're saying.

528
00:31:47,680 --> 00:31:53,560
You're not saying that training can be taking data,

529
00:31:53,560 --> 00:31:57,960
like an object code blob, storing it in the model.

530
00:31:57,960 --> 00:32:02,840
And then when you build your AI system and you use the model,

531
00:32:02,840 --> 00:32:07,480
copy that data, load it into an executable area of memory,

532
00:32:07,480 --> 00:32:09,440
and run it.

533
00:32:09,440 --> 00:32:13,600
Now, if I'm building consumer electronics, though,

534
00:32:13,600 --> 00:32:16,560
and I'm sending updates using my model, which

535
00:32:16,560 --> 00:32:21,000
is going to be an increasingly common paradigm, given

536
00:32:21,000 --> 00:32:26,240
the fact that we have just huge amounts of AI hardware being

537
00:32:26,240 --> 00:32:29,920
put out there, then it would be an attractive thing for me

538
00:32:29,920 --> 00:32:34,200
to update my platform via that AI model.

539
00:32:34,200 --> 00:32:36,880
And some of the things I'll put in there are, yes,

540
00:32:36,880 --> 00:32:38,160
they're technically trained.

541
00:32:38,160 --> 00:32:44,200
They're trained to say, here's a lookup table of what

542
00:32:44,200 --> 00:32:48,280
should be executed when the system initializes,

543
00:32:48,280 --> 00:32:49,360
the AI system.

544
00:32:49,360 --> 00:32:52,360
And I think it would be good to just draw a bright line

545
00:32:52,360 --> 00:32:58,440
and say, no, you can't just say that--

546
00:32:58,440 --> 00:33:00,680
you can't just move something technically

547
00:33:00,680 --> 00:33:05,480
into what is the model and get around these principles

548
00:33:05,480 --> 00:33:06,480
that we're making clear.

549
00:33:06,480 --> 00:33:08,000
And that's the same thing as if you

550
00:33:08,000 --> 00:33:13,800
were to move that code elsewhere in your AI model.

551
00:33:13,800 --> 00:33:15,000
I see your point.

552
00:33:15,000 --> 00:33:16,000
I see your point.

553
00:33:16,000 --> 00:33:18,480
Now, it's more clear.

554
00:33:18,480 --> 00:33:21,040
But I would recommend that we go back

555
00:33:21,040 --> 00:33:25,040
to the reason why we put, at the beginning of the document,

556
00:33:25,040 --> 00:33:28,320
the definition of what is an AI system.

557
00:33:28,320 --> 00:33:29,440
Maybe that helps.

558
00:33:29,440 --> 00:33:31,880
Because if we go back to the definition, what is the AI

559
00:33:31,880 --> 00:33:35,720
system that we are defining in this work, in this document?

560
00:33:35,720 --> 00:33:42,640
The AI system is anything that, for implicit objectives,

561
00:33:42,640 --> 00:33:46,320
given an input spits out an output.

562
00:33:46,320 --> 00:33:50,880
So whatever you call that, whether you package it

563
00:33:50,880 --> 00:33:53,880
in a binary that loads a blob and executes

564
00:33:53,880 --> 00:33:57,680
a virtual machine that executes something else,

565
00:33:57,680 --> 00:34:04,040
if what we're calling here is what we're defining

566
00:34:04,040 --> 00:34:07,560
and what we need the corresponding--

567
00:34:07,560 --> 00:34:10,000
the preferred form to make modification of

568
00:34:10,000 --> 00:34:12,480
is whatever that system is, however it's packaged,

569
00:34:12,480 --> 00:34:14,280
however it's shipped, whatever it ships.

570
00:34:14,280 --> 00:34:22,680
So when we go into the implementation,

571
00:34:22,680 --> 00:34:24,880
some reviewer who wants to--

572
00:34:24,880 --> 00:34:27,320
someone like, I don't know, the Linux Foundation

573
00:34:27,320 --> 00:34:28,640
wants to--

574
00:34:28,640 --> 00:34:31,680
may want to have a requirement some day,

575
00:34:31,680 --> 00:34:33,800
maybe will have a requirement that says,

576
00:34:33,800 --> 00:34:39,880
we only accept AI systems in our projects that are open source

577
00:34:39,880 --> 00:34:43,120
AI following the definition of DOSI.

578
00:34:43,120 --> 00:34:47,280
Then they will have a standard set for specifically

579
00:34:47,280 --> 00:34:49,840
that technology, the same way that they have the model

580
00:34:49,840 --> 00:34:52,880
openness framework now for the generative AI models.

581
00:34:52,880 --> 00:34:57,240
And in that place, they can put all of the requirements

582
00:34:57,240 --> 00:35:00,200
specifically for that technology to say,

583
00:35:00,200 --> 00:35:02,160
nope, that loading of that binary

584
00:35:02,160 --> 00:35:04,280
here is not a good thing.

585
00:35:04,280 --> 00:35:06,520
It's basically a workaround.

586
00:35:06,520 --> 00:35:08,200
It's not acceptable.

587
00:35:08,200 --> 00:35:12,960
Or we'll have to wait and see.

588
00:35:12,960 --> 00:35:15,560
I mean, another way, another approach that we could have

589
00:35:15,560 --> 00:35:16,760
is to wait and see what--

590
00:35:16,760 --> 00:35:19,680
if this threat that you have in mind

591
00:35:19,680 --> 00:35:21,080
is actually going to show up.

592
00:35:21,080 --> 00:35:24,600
It's not just a theoretical threat.

593
00:35:24,600 --> 00:35:31,120
If it will be showing up, we're thinking one of the tasks

594
00:35:31,120 --> 00:35:37,160
that we have to do by the launch is to really come up

595
00:35:37,160 --> 00:35:40,920
with suggest recommendations on how we're going to be

596
00:35:40,920 --> 00:35:44,800
monitoring the efficacy, the adoption of the open source AI

597
00:35:44,800 --> 00:35:48,000
definition, the availability of other--

598
00:35:48,000 --> 00:35:52,320
availability and reviewing of the existence of open source

599
00:35:52,320 --> 00:35:56,520
AI systems besides the initial ones that we have identified,

600
00:35:56,520 --> 00:36:03,240
which, by the way, are the ones released by the Eleuther AI,

601
00:36:03,240 --> 00:36:07,720
Alen AI Institute, LLM 360, and most likely,

602
00:36:07,720 --> 00:36:11,080
DII, we're working to understand those better.

603
00:36:11,080 --> 00:36:14,480
So those are the ones that pass the open source AI definition

604
00:36:14,480 --> 00:36:14,960
right now.

605
00:36:14,960 --> 00:36:18,600
Thank you.

606
00:36:18,600 --> 00:36:19,640
I see.

607
00:36:19,640 --> 00:36:21,200
Yeah.

608
00:36:22,040 --> 00:36:25,040
Ben?

609
00:36:25,040 --> 00:36:32,440
So when a company modifies an open source AI model

610
00:36:32,440 --> 00:36:35,480
for commercial use, at what point

611
00:36:35,480 --> 00:36:39,480
does the modification become a derivative work subject

612
00:36:39,480 --> 00:36:40,560
to the original license?

613
00:36:40,560 --> 00:36:45,760
And would existing legal tests around open source licensing

614
00:36:45,760 --> 00:36:50,560
be sufficient to determine the AI model modifications

615
00:36:50,560 --> 00:36:52,120
and the derivative work aspect of it?

616
00:36:52,120 --> 00:36:57,040
Or do you foresee a whole new level of legal guidance

617
00:36:57,040 --> 00:37:00,280
needed with regards to these type of things?

618
00:37:00,280 --> 00:37:02,640
Yeah, that's a very good question.

619
00:37:02,640 --> 00:37:09,120
Frankly, lots of it depends on what the models themselves

620
00:37:09,120 --> 00:37:12,560
are considered under laws.

621
00:37:12,560 --> 00:37:16,040
Because right now, yeah, it all depends

622
00:37:16,040 --> 00:37:23,320
on what contracts get built around them

623
00:37:23,320 --> 00:37:27,040
and how those propagate between--

624
00:37:27,040 --> 00:37:29,600
all the rights propagate between this new artifact,

625
00:37:29,600 --> 00:37:35,800
the trained weights, trained parameters, to derivatives.

626
00:37:35,800 --> 00:37:37,360
Yeah.

627
00:37:37,360 --> 00:37:37,880
Not easy.

628
00:37:45,960 --> 00:37:50,200
So there is a conversation now happening on the license

629
00:37:50,200 --> 00:37:55,600
review mailing list, which is the community of volunteers

630
00:37:55,600 --> 00:38:00,400
who've been reviewing licenses, software licenses.

631
00:38:00,400 --> 00:38:04,040
And they've been asked to consider

632
00:38:04,040 --> 00:38:08,600
how to handle licenses and other legal documents that

633
00:38:08,600 --> 00:38:13,000
are not covering software, so the traditional space.

634
00:38:13,000 --> 00:38:17,760
So if you're interested in joining that conversation,

635
00:38:17,760 --> 00:38:22,320
I would highly recommend to join the license-review mailing

636
00:38:22,320 --> 00:38:22,820
list.

637
00:38:22,820 --> 00:38:30,360
I see a question from Peter.

638
00:38:30,360 --> 00:38:33,240
Other non-profits in OSI ecosystems,

639
00:38:33,240 --> 00:38:37,120
like GNOME, Alliance Foundation, Physotek Foundation, et cetera,

640
00:38:37,120 --> 00:38:40,200
have any of them attempted to independently define

641
00:38:40,200 --> 00:38:46,160
open-source AI or provided input to OSI about OSI's definition?

642
00:38:46,160 --> 00:38:47,880
Oh, Peter, yeah, this is a great question

643
00:38:47,880 --> 00:38:50,280
because it allows us to explain a little bit more

644
00:38:50,280 --> 00:38:52,720
what the process has been.

645
00:38:52,720 --> 00:38:55,160
So this is not OSI's definition, the same way

646
00:38:55,160 --> 00:38:58,320
that the open-source definition is not OSI's definition.

647
00:38:58,320 --> 00:39:01,040
We merely maintain it for the community.

648
00:39:01,040 --> 00:39:04,840
So we have worked from the very beginning with Linux Foundation

649
00:39:04,840 --> 00:39:09,400
and FSF and others, reached out to them

650
00:39:09,400 --> 00:39:13,920
and asked them to contribute to the definition.

651
00:39:13,920 --> 00:39:15,760
This definition is not written by us.

652
00:39:15,760 --> 00:39:21,200
It's written by a process held by a co-design process.

653
00:39:21,200 --> 00:39:26,880
It's a process designed to work with the people affected

654
00:39:26,880 --> 00:39:29,000
by the decision, not for them.

655
00:39:29,000 --> 00:39:32,880
So yes, all of these organizations

656
00:39:32,880 --> 00:39:37,080
mentioned, plus many others-- Creative Commons, Eleuthera

657
00:39:37,080 --> 00:39:41,520
AI, Mozilla Foundation, many other groups

658
00:39:41,520 --> 00:39:44,360
that have participated to the list.

659
00:39:44,360 --> 00:39:46,000
And we have somewhere on the website,

660
00:39:46,000 --> 00:39:51,000
opensource.org/deepdive, we have a list of the groups

661
00:39:51,000 --> 00:39:53,360
that have been-- and the volunteers that

662
00:39:53,360 --> 00:39:56,200
have been included.

663
00:39:56,200 --> 00:40:11,920
, All right.

664
00:40:11,920 --> 00:40:23,760
Are there any more questions?

665
00:40:23,760 --> 00:40:43,680
[AUDIO OUT]

666
00:40:43,680 --> 00:40:45,160
OK, then.

667
00:40:45,160 --> 00:40:48,880
I'm going to call it a day, call it a week.

668
00:40:48,880 --> 00:40:49,600
Thanks, everyone.

669
00:40:49,600 --> 00:40:52,280
This has been very useful, very interesting for me.

670
00:40:52,280 --> 00:40:54,760
I hope it's been informative for you, too.

671
00:40:54,760 --> 00:41:00,520
And I would like Nick is saying on the chat,

672
00:41:00,520 --> 00:41:05,040
our discussions continue on our forum,

673
00:41:05,040 --> 00:41:07,640
discuss.opensource.org.

674
00:41:07,640 --> 00:41:14,400
And the license review working group is available.

675
00:41:14,400 --> 00:41:18,560
Peter, stay on the chat here.

676
00:41:18,560 --> 00:41:21,040
I will send you the link where to join it.

677
00:41:21,040 --> 00:41:22,960
It's old school, meaningless.

678
00:41:22,960 --> 00:41:29,320
Thank you, everyone.

### End of last town hall held on 2024-09-06 ###

### Start of next town hall held on 2024-09-13 ###
--- Presentation for 2024-09-13 ---
OPEN SOURCE AI DEFINITION
Online public townhall
September 6, 2024
last updated: September 4, 2024 (MJ)

1

Community agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

Open Source AI Deﬁnition

New Version 0.0.9

Released August 22, 2024

3

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open
Source AI System must include:

Open

Weights
Model weights and
parameters

Open

+ Code +
Source code used
to train and run
the system

Data

Information
The dataset or
detailed information
about the data used
to train the system

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Weights
The model weights and
parameters, made available
under OSI-approved terms
Examples: checkpoints from key intermediate stages of training as
well as the ﬁnal optimizer state

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Code
The source code used to train and
run the system, made available
with OSI-approved licenses
Examples: code used for pre-processing data, training, validation and testing,
supporting libraries like tokenizers and hyperparameters search code,
inference code, and model architecture.

OSAID v.0.0.9
The preferred form of making modiﬁcations for a machine-learning Open Source AI System must
include:

Data

Information
Sufﬁciently detailed information about the data used to
train the system, so that a skilled person can recreate a
substantially equivalent system using the same or
similar data. Data information shall be made available
with licenses that comply with the Open Source
Deﬁnition.
Examples: training methodologies and
techniques, training data sets used, information
about the provenance of those data sets, their scope and characteristics, how the data
was obtained and selected, the labeling procedures and data cleaning methodologies.

Training Data in the OSAID
OSI afﬁrms the beneﬁts of full access to training data while acknowledging it
is not always possible for reasons of law, privacy norms, technical feasibility,
and cultural practice.
• Training data is valuable to study AI systems: to understand the biases that have been
learned, which can impact system behavior. But training data is not part of the preferred
form for making modiﬁcations to an existing AI system
• Data can be hard to share. Laws that permit training on data often limit the resharing of that
same data to protect copyright or other interests.
• Privacy rules also give a person the rightful ability to control their most sensitive information,
such as decisions about their health.
• Similarly, much of the world’s Indigenous knowledge is protected through mechanisms that
are not compatible with later-developed frameworks for rights exclusivity and sharing.
• Open training data (data that can be reshared) provides the best way to enable users to study
the system, along with the preferred form of making modiﬁcations.
• Public training data (data others can inspect as long as it remains available) also enables
users to study the work, along with the preferred form.

OSAID Approval Criteria
OSI Board requires a deﬁnition that is:
Supported
by diverse
stakeholders

Provides
real-life
examples

Ready by
October 2024

The deﬁnition
needs to have
approval by end
users, developers,
deployers and
subjects of AI,
globally.

The deﬁnition must
include relevant
examples of AI
systems that comply
with it at the time of
approval, so cannot
have an empty set.

A usable version
of the deﬁnition
needs to be ready
for approval by
the board at the
October board
meeting.

Approved June 21, 2024

Relevant comments
Summary of the discussions on the forum and on hackmd

10

Clariﬁcations of the text
- Randall:

- Rename data as "source data"
- Be explicit that public data needs
enumeration
- Be explicit that one can require downstream
users to open their data if they ﬁne tune on
your model

https://discuss.opensource.org/t/welco
me-diverse-approaches-to-training-data
-within-a-uniﬁed-open-source-ai-deﬁniti
on/531
11

Clariﬁcations of the text

From just the data, it is hard to understand *why* certain tokens are included
or excluded. For this, you really need the code for the *full* data processing
pipeline.
A open-source deﬁnition proposal: the following should be released:
- Code for entire procedure
- Any part of the executed versions (data, weights) without copyright/privacy
concerns
- Pointers to all the raw data (CommonCrawl, torrent link, etc.

12

Hardware considerations
Mariana Taglio and Allison Randall
mention hardware.
“include the detailed technical
aspects of model training, such as
hardware speciﬁcations, training time
and carbon footprint (if available)”
https://discuss.opensource.org/t/sha
re-your-thoughts-about-draft-v0-0-9
/514/8
13

Map dataset’s right to distribute
Wade:
- a framework to
describe the
commitment to
releasing data
information, or the
dataset when legally
possible
https://discuss.opensourc
e.org/t/proposal-to-handl
e-data-openness-in-the-o
pen-source-ai-deﬁnition-rf
c/561
14

Open Source AI Deﬁnition

What’s Next?

September - October 2024

● Resolve comments, release RC1
● Launch stable version at All Things Opens

15

Endorse the OSAID!
● In preparation for the ﬁnalization of RC1 at the
end of this month, we are seeking both individual
and organizational endorsements of the OSAID.
● “Endorsement” means your name and
organizational affiliation will be appended to a
press release announcing RC1.
● If you or your organization are interested in
endorsing the OSAID, email me or Mer at
Mer@dobiggood.com
16

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

June

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.8

July

August

September

October

Virtual System
Review

Virtual System
Review

Pause in Virtual
System Review

Pause in Virtual
System Review

Townhalls +

Townhalls +

Townhalls +

- OSPOs for
Good (NYC)
- OSCA (virtual)

- AI-dev (Hong
Kong)

Townhalls +
- Deep Learning Indaba
(Dakar)
- IndiaFOSS (Bengaluru)
- RegenAI (Ashland)
- Nerdearla (Buenos Aires)

Draft 0.0.8

Draft 0.0.9

RC1

- OSAI Data
Workshop (Paris)
- All Things Open
(Raleigh - launch!)

Stable
Version

How to Participate :)
● Endorse the OSAID!
○ Email Stefano or Mer
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
18

Q&A

19

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the deﬁnition.

20


--- Subtitles for 2024-09-13 --- ###
1
00:00:00,001 --> 00:00:06,320
The open source AI definition came out at the end of August with a new version

2
00:00:06,320 --> 00:00:13,240
009. This is made of three main elements. It defines machine learning, open source

3
00:00:13,240 --> 00:00:19,840
AI. The preferred form for making modifications is tied to the concepts of

4
00:00:19,840 --> 00:00:27,520
machine learning. Because this is the technology that

5
00:00:27,520 --> 00:00:35,760
requires a little bit more thinking because it has

6
00:00:35,760 --> 00:00:41,880
this concept of trained models, trained weights and parameters. So the

7
00:00:41,880 --> 00:00:47,200
weights are defined as the model weights and parameters made available under

8
00:00:47,200 --> 00:00:52,160
open source initiative approved terms. And then there are

9
00:00:52,160 --> 00:01:01,400
examples. Here what's important is the fact that we're trying to be vague

10
00:01:01,400 --> 00:01:08,120
enough but signal precisely the intention. So the vagueness is because

11
00:01:08,120 --> 00:01:13,000
we want to make sure that the text can resist time. It doesn't have to be updated

12
00:01:13,000 --> 00:01:19,720
every time a new technology, a new model architecture, a new AI technology or

13
00:01:19,720 --> 00:01:26,240
technique gets pushed out and made available. But also we want to use

14
00:01:26,240 --> 00:01:32,600
the text so that the intention of the drafters is clear. The examples

15
00:01:32,600 --> 00:01:38,160
provided are just part of the definition and they need to be

16
00:01:38,160 --> 00:01:45,280
read together to understand it, to evaluate whether that signal, that

17
00:01:45,280 --> 00:01:51,120
signaling of intention is clear enough. That should be clear. So from the source

18
00:01:51,120 --> 00:01:58,000
code requirements also, this is a space that we know a lot more about.

19
00:01:58,000 --> 00:02:03,720
Like what is the source code and how to deal with all the licenses etc.

20
00:02:03,720 --> 00:02:07,720
But what's important here is that the requirement to release the source code

21
00:02:07,720 --> 00:02:14,920
used to train the system. So one needs to be able, one who releases software

22
00:02:14,920 --> 00:02:22,560
needs to be able to understand, one who releases an AI system needs to share all

23
00:02:22,560 --> 00:02:28,200
the instructions, details including all the software used for training,

24
00:02:28,200 --> 00:02:33,280
for validation, for testing. It's very important because that's the part where

25
00:02:33,280 --> 00:02:40,920
collaboration can happen and improvements can happen iteratively.

26
00:02:40,920 --> 00:02:48,880
And data information from the data front, the text here hasn't changed yet.

27
00:02:48,880 --> 00:02:54,040
Although it's still quite convoluted and complicated, it could be refined. We'll

28
00:02:54,040 --> 00:03:03,200
talk about that later. It needs to deal with the fact that laws are

29
00:03:03,200 --> 00:03:08,960
complicated. And so this, again, the intention here is to give whoever

30
00:03:08,960 --> 00:03:16,880
receives the software enough of the code itself and the datasets or the

31
00:03:16,880 --> 00:03:23,120
provenance of the data so that the science can continue. And then also

32
00:03:23,120 --> 00:03:27,160
innovation can keep on happening, building on the shoulder of the giants

33
00:03:27,160 --> 00:03:31,400
exactly like it happens with the open source software where you don't have to

34
00:03:31,400 --> 00:03:38,800
reinvent the wheel. You can build on top of someone else's work. So when

35
00:03:38,800 --> 00:03:45,320
we released the version 009 of the open source AI definition, we

36
00:03:45,320 --> 00:03:55,000
also released text to explain why the data set is considered, the release of

37
00:03:55,000 --> 00:04:00,880
the training dataset is considered a benefit and not a requirement. It's

38
00:04:00,880 --> 00:04:09,200
because training data are covered by laws that limit the resharing of the

39
00:04:09,200 --> 00:04:16,000
datasets because of privacy rules, but also copyright law and even other

40
00:04:16,000 --> 00:04:20,480
legislations like indigenous knowledge is not protected with copyright, it's

41
00:04:20,480 --> 00:04:28,160
protected by other rules. And we need to take that diversity of legislation into

42
00:04:28,160 --> 00:04:33,400
account and we need to make a difference between open training data, public

43
00:04:33,400 --> 00:04:39,520
training data, and private data. So all of these have to be covered in a

44
00:04:39,520 --> 00:04:44,280
different way, have to be treated differently because they are

45
00:04:44,280 --> 00:04:53,440
different. And so the other thing worth reminding everyone all the time is

46
00:04:53,440 --> 00:05:00,640
that the OSI board has set some basic criteria to approve the open

47
00:05:00,640 --> 00:05:04,800
source, the results of the co-design process. We presented these slides a

48
00:05:04,800 --> 00:05:12,240
couple of weeks ago in China, but the board requires that the

49
00:05:12,240 --> 00:05:19,800
definition is supported by a diverse type of stakeholder and by

50
00:05:19,800 --> 00:05:25,760
diversity it's listed not only into interest groups like users, developers,

51
00:05:25,760 --> 00:05:30,960
deployers, subject of AI, but also geographic distribution. So we don't

52
00:05:30,960 --> 00:05:36,840
want to have only Europeans or North Americans or South Americans, etc. We're

53
00:05:36,840 --> 00:05:43,760
making an effort to go around the world to disseminate this work

54
00:05:43,760 --> 00:05:50,680
and gather approval around the world and this is thanks to a grant from the

55
00:05:50,680 --> 00:05:55,440
Alfred P Sloan Foundation. The other requirement is that the definition needs

56
00:05:55,440 --> 00:06:02,280
to provide real-life examples. So once approved we need to make sure that

57
00:06:02,280 --> 00:06:10,160
we have systems that we can point at and say these are open

58
00:06:10,160 --> 00:06:16,000
source AI. And right now comfortably we can say that the

59
00:06:16,000 --> 00:06:24,160
complying systems are PTI, the set of models released by

60
00:06:24,160 --> 00:06:31,680
Yale AI Institute and LLM 360 and TII also. So non-profit research

61
00:06:31,680 --> 00:06:36,280
institutions who have released the large language models and similarly

62
00:06:36,280 --> 00:06:44,960
powerful AI systems. And we need to be ready by the October board

63
00:06:44,960 --> 00:06:52,520
meeting that will happen on October 27th in North Carolina. So let's go through

64
00:06:52,520 --> 00:06:57,280
the relevant comments that we have received this past few days on the

65
00:06:57,280 --> 00:07:04,360
forums. We had Alison Randall basically requesting to be more

66
00:07:04,360 --> 00:07:11,760
explicit about the requirements. This is a fair request. The text

67
00:07:11,760 --> 00:07:19,320
tends to, the text of the definition tends to be more vague, to leave some vague

68
00:07:19,320 --> 00:07:24,920
words in there. Like I said before, the intention is to be vague enough but

69
00:07:24,920 --> 00:07:31,560
signal clearly the intentions. So it's worth rereading the text to

70
00:07:31,560 --> 00:07:35,000
make sure that those intentions are clear. And especially one of the

71
00:07:35,000 --> 00:07:41,080
intentions that Alison points out is the requirement to be explicit about

72
00:07:41,080 --> 00:07:47,760
allowing, adding requirements the same way that the open source definition, the

73
00:07:47,760 --> 00:07:52,680
classic open source definition allows for some requirements that are considered

74
00:07:52,680 --> 00:07:59,920
to be good. Like a requirement for copyleft, you know, the persistence of

75
00:07:59,920 --> 00:08:05,840
propagating the rights to downstream users is something that needs to be

76
00:08:05,840 --> 00:08:13,840
evaluated. Whether the text that exists today allows that or if the text is

77
00:08:13,840 --> 00:08:20,640
too vague and doesn't allow for that propagation downstream.

78
00:08:20,640 --> 00:08:25,200
Technically, also legally, we need to understand how that can happen but

79
00:08:25,200 --> 00:08:32,560
that's a different story most likely. Also be explicit about the fact that if the

80
00:08:32,560 --> 00:08:39,360
training is done on public data, so data that actually can be distributed, there

81
00:08:39,360 --> 00:08:48,360
are no exclusive IP rights, no natural exclusive IP rights. Like content created

82
00:08:48,360 --> 00:08:54,440
by humans are, they always will have, I mean most parts of the world will have

83
00:08:54,440 --> 00:09:04,760
copyright or civil rights, due to in European continental and moral rights.

84
00:09:04,760 --> 00:09:11,840
But that doesn't mean that, for example, temperatures of the ocean, that

85
00:09:11,840 --> 00:09:17,640
kind of data, that information doesn't have any right naturally. So

86
00:09:17,640 --> 00:09:23,280
we want to consider that. She also suggests renaming data as source data.

87
00:09:23,280 --> 00:09:29,880
This is something that I'm personally skeptical about. I would love to see more

88
00:09:29,880 --> 00:09:36,320
people leaving comments on the forum because there is no, I mean, so data

89
00:09:36,320 --> 00:09:43,960
is not source. Source is the word that has been traditionally used to

90
00:09:43,960 --> 00:09:49,160
signal the source code or the preferred form for making modification to the

91
00:09:49,160 --> 00:09:54,360
software. It's also mentioned in the original open source definition as such

92
00:09:54,360 --> 00:10:02,000
and data is not the source of the training, training the outputs like

93
00:10:02,000 --> 00:10:08,520
the weights and parameters. So I'm less reluctant. I mean it's called

94
00:10:08,520 --> 00:10:14,440
training data in literature. So anyway I'd love to hear more comments

95
00:10:14,440 --> 00:10:22,360
about this on the forum. And speaking of that concept of source as data, an

96
00:10:22,360 --> 00:10:27,640
interesting comment received by a person, Professor Leon, he's a professor at

97
00:10:27,640 --> 00:10:36,360
Stanford University. He made a good comment on the document on the

98
00:10:36,360 --> 00:10:45,000
text. He's saying that the data is basically the data set is the output of

99
00:10:45,000 --> 00:10:50,400
the processing of the original data and he's arguing that it's a lot more

100
00:10:50,400 --> 00:10:55,560
important to get access to all the scripts used to build the data set

101
00:10:55,560 --> 00:11:01,800
rather than the data set itself because the data set is not very comprehensible

102
00:11:01,800 --> 00:11:07,760
without the code. And it's an interesting comment because it

103
00:11:07,760 --> 00:11:15,120
really gives you, changes the perception. The data set is more like a

104
00:11:15,120 --> 00:11:23,920
binary of the original data. And the source code is actually the code used

105
00:11:23,920 --> 00:11:31,160
to create those tokens. So a very interesting comment in there and one

106
00:11:31,160 --> 00:11:36,800
that might require also some refining, fine-tuning of the text of the

107
00:11:36,800 --> 00:11:41,600
definition. Then we have comments on, we have received comments on hardware

108
00:11:41,600 --> 00:11:49,160
considerations from Mariana Taglio and Alison Randall. This is an area, hardware

109
00:11:49,160 --> 00:11:53,600
is an area where the open source initiative and open source in general

110
00:11:53,600 --> 00:12:00,160
has not considered because it's a completely different layer and

111
00:12:00,160 --> 00:12:06,760
can be isolated. Even in AI I think it can be isolated. Adding

112
00:12:06,760 --> 00:12:10,880
hardware considerations would make the whole specification, the whole definition

113
00:12:10,880 --> 00:12:16,440
a lot more complicated and I'm not sure it's viable. But you know if you think it

114
00:12:16,440 --> 00:12:22,800
is, this is your time to leave your comments on the forums on this topic.

115
00:12:22,800 --> 00:12:31,920
To me getting detailed aspects like hardware specifications, the

116
00:12:31,920 --> 00:12:38,200
carbon footprint and things like that is interesting from the social benefit

117
00:12:38,200 --> 00:12:45,800
perspective. But I don't think it really helps with the concept of improving,

118
00:12:45,800 --> 00:12:53,160
collaborating, innovate on the AI systems. It's a little

119
00:12:53,160 --> 00:12:58,760
bit the considerations about ethical use. They're valuable but they need to be put

120
00:12:58,760 --> 00:13:04,920
at a different level, like a legislation level for example. But your comments, I

121
00:13:04,920 --> 00:13:08,520
mean please leave your comments in this part if you think they are part of

122
00:13:08,520 --> 00:13:15,120
the, if they're important for studying, share, modify and distribute AI

123
00:13:15,120 --> 00:13:22,560
systems. And finally just a couple of days ago Carsten Wade made this proposal

124
00:13:22,560 --> 00:13:29,280
to map visually the rights to distribute data set. I think it's an interesting

125
00:13:29,280 --> 00:13:36,880
representation in a quadrant type of way where if you're

126
00:13:36,880 --> 00:13:45,040
building on data, I mean he has this quadrant with two axes. One is on the

127
00:13:45,040 --> 00:13:50,720
vertical axe, you have the IP intellectual property rights, present or

128
00:13:50,720 --> 00:13:58,600
absent. And then if you want to have an integrity of the

129
00:13:58,600 --> 00:14:08,080
pipeline stack, just another axis. So if you're

130
00:14:08,080 --> 00:14:15,000
training something on public data, then basically anyone have high integrity

131
00:14:15,000 --> 00:14:21,880
then you must release everything including the

132
00:14:21,880 --> 00:14:30,520
original data set because basically why shouldn't you? Since there are no, I mean

133
00:14:30,520 --> 00:14:37,040
since you should have all the IP rights. But if you're training on

134
00:14:37,040 --> 00:14:43,800
private data then it's open source AI minus the data, D minus.

135
00:14:43,800 --> 00:14:49,480
I think it's an interesting visual on this. Or if you don't have

136
00:14:49,480 --> 00:14:57,640
the, if you choose not to release the data then it's

137
00:14:57,640 --> 00:15:05,760
definitely going to be a closed, not open source AI. So let's talk about what's

138
00:15:05,760 --> 00:15:12,200
next and that is we're going to be reviewing the text,

139
00:15:12,200 --> 00:15:18,080
close some of the comments, most of the comments and release a release

140
00:15:18,080 --> 00:15:24,000
candidate, have a release candidate version by probably in the next, within a

141
00:15:24,000 --> 00:15:28,940
couple of weeks. So if you have more comments, please do it as soon as

142
00:15:28,940 --> 00:15:32,960
possible. Although there will still be time to change after the release

143
00:15:32,960 --> 00:15:38,920
candidate. And then if you're ready to endorse the open source AI definition, we

144
00:15:38,920 --> 00:15:43,760
are looking for individuals and organizations who can endorse it and

145
00:15:43,760 --> 00:15:49,920
that means that your name or your organization affiliation will be

146
00:15:49,920 --> 00:15:55,760
appended to the press release and the announcement page of the open source AI

147
00:15:55,760 --> 00:16:03,280
definition. So if you want to be part of this release, please email me or Mer,

148
00:16:03,280 --> 00:16:11,960
the email address is on the deck. And just to give a timeline, we are

149
00:16:11,960 --> 00:16:17,720
in September now, we gave a few talks around the world. We are going to

150
00:16:17,720 --> 00:16:28,120
Buenos Aires next week, we Ashland in Oregon this weekend, we've been in

151
00:16:28,120 --> 00:16:36,400
Daba and Bangalore to present this definition and we're going to

152
00:16:36,400 --> 00:16:42,800
be holding more town halls every week until basically until the end of

153
00:16:42,800 --> 00:16:48,360
October for the official launch in North Carolina at All Things Open.

154
00:16:48,360 --> 00:16:55,600
We're going to be holding a special data workshop on data

155
00:16:55,600 --> 00:17:02,320
in Paris also in October thanks to the Alfred P. Sloan Foundation grant.

156
00:17:02,320 --> 00:17:10,480
Alright, so the way to participate is to endorse, so you can email me or

157
00:17:10,480 --> 00:17:19,200
Stefano or Mer and you can keep on commenting on the forum. And with that,

158
00:17:19,200 --> 00:17:27,680
time for Q&A. So take the floor as you want, open your mic or type

159
00:17:27,680 --> 00:17:31,360
questions if you prefer.

160
00:17:31,360 --> 00:17:46,960
I can see Ted comment saying not sure why hardware is needed.

161
00:17:46,960 --> 00:17:54,880
Yeah, go ahead Ted. Yes, Stefano, thank you for sharing, this is really helpful.

162
00:17:54,880 --> 00:18:06,240
Two questions, one is can you share the slides that we can distribute to many

163
00:18:06,240 --> 00:18:13,120
people who cannot attend this town hall? Yes, so the session is recorded, I'm

164
00:18:13,120 --> 00:18:19,600
going to cut the part without the audio and we're going to be sharing the deck, yes.

165
00:18:19,600 --> 00:18:26,000
Okay, so you will share through email? Yeah, all of them are

166
00:18:26,000 --> 00:18:30,720
shared on the forum, so I will put the link on where we're going to be

167
00:18:30,720 --> 00:18:37,880
putting it so you can subscribe and get that one. But yeah, I can send it to you via email.

168
00:18:37,880 --> 00:18:48,280
Okay, and secondly that endorsement, so it's just an email, is there any

169
00:18:48,280 --> 00:18:58,720
template or just a simple email that endorses the RC1 unstable version?

170
00:18:58,720 --> 00:19:05,120
Just an email saying, hey, I'm interested, my organization is

171
00:19:05,120 --> 00:19:12,520
interested in endorsing this and we're going to be putting you in the

172
00:19:12,520 --> 00:19:19,720
loop, so every release candidate will make sure that you have it and

173
00:19:19,720 --> 00:19:25,760
once we get closer to the press release time, we will ask you formally, this

174
00:19:25,760 --> 00:19:31,760
is probably going to be at the beginning of October, for a quote. Okay, can I

175
00:19:31,760 --> 00:19:42,120
suggest that we put a link on the relevant website, so that I

176
00:19:42,120 --> 00:19:50,480
can propagate or send to whoever is interested in endorsing it.

177
00:19:50,480 --> 00:19:56,920
Just one click, one click link, so whatever individual or

178
00:19:56,920 --> 00:20:02,880
organizations, we can hit on the quick list of names or organizations and

179
00:20:02,880 --> 00:20:09,600
that would be it, instead of just a separate email.

180
00:20:09,600 --> 00:20:14,280
Oh no, absolutely, yes, we're thinking about also creating a landing page with

181
00:20:14,280 --> 00:20:19,280
the text and the button below that says, yes, I endorse it, name and

182
00:20:19,280 --> 00:20:24,920
affiliation. Yeah, okay, all right, that'd be great, thank you, I have no more

183
00:20:24,920 --> 00:20:33,720
questions. Thank you. Anyone else, any doubts, ideas?

184
00:20:33,720 --> 00:20:44,400
Overall, it looks good to you?

185
00:20:44,400 --> 00:20:58,200
All right then, oh, Jay, is someone typing?

186
00:21:12,480 --> 00:21:18,920
All right, if there are no more questions, I mean, if you have more questions, you

187
00:21:18,920 --> 00:21:26,280
can always email me or ask directly on the forum, really feel free to use all

188
00:21:26,280 --> 00:21:32,120
the resources we have. I want to thank everyone and

### End of last town hall held on 2024-09-13 ###

### Start of next town hall held on 2024-09-20 ###
--- Presentation for 2024-09-20 ---
OPEN SOURCE AI DEFINITION
Online public townhall
September 20, 2024
last updated: September 18, 2024 (MJ)

1

Community Agreements
●
●

●

●

●

●

One Mic, One Speaker -- Please allow one person to speak at a time.
Take Space, Make Space -- If you tend to talk more, we invite you to
make space for others to share. If you tend not to share, we invite you
to speak up.
Kindness -- This work is hard, but we don't have to be. Gentleness and
curiosity help. Those who use insults or hate speech will need to leave
the meeting.
Forward Motion -- We advance by focusing on what is possible in the
moment and doing it. Obstacles are marked for later discussion, not
used to stop the process. If we hit a boulder, we note it on the map and
keep walking. We'll come back and unearth it later on.
Solution-Seeking -- This work is so complex that focusing on what
won't work will stop it. Suggesting new ideas, options, and proposals is
vulnerable, but crucial. All of us are needed to make this work.
Anything else?

2

Open Source AI Deﬁnition

Version 0.0.9

Released Aug. 22, 2024

3

OSAID: 4 Freedoms

An Open Source AI is an AI system made available under
terms and in a way that grant the freedoms to:

1. Use the system for any purpose and without having
to ask for permission.
2. Study how the system works and inspect its
components.
3. Modify the system for any purpose, including to
change its output.
4. Share the system for others to use with or without
modiﬁcations, for any purpose.
4

OSAID: Preferred Form

The preferred form of making modiﬁcations for a machine-learning
Open Source AI System must include:

Open

Open

Model weights and
parameters

Source code used
to train and run
the system

Weights + Code +

Data

Information
The dataset or
detailed information
about the data used
to train the system

Open Source AI Deﬁnition

What’s Next?

September - October 2024

● Resolve comments, release RC1 this week
● Launch stable version at All Things Opens
on October 28th

6

Board Approval Criteria for the OSAID
Supported
by diverse
stakeholders

Provides
real-life
examples

Ready by
October

The deﬁnition
needs to have
approval by end
users, developers,
deployers and
subjects of AI,
globally.

The deﬁnition must
include relevant
examples of AI
systems that comply
with it at the time of
approval, so cannot
have an empty set.

A usable version of
the deﬁnition needs
to be ready for
approval by the
board at the
October board
meeting.

2024

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

April

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.8

August

September

October

Virtual System
Review

Virtual System
Review

Feedback and
review
Hardening WG

Feedback and
review
Hardening WG

Townhalls +

Townhalls +

- OSPOs for
Good (NYC)
- OS Community
Africa (virtual)

- AI-dev (Hong
Kong)
- OSC (Beijing)

July

Draft 0.0.8

Draft 0.0.9

Weekly townhalls
- DL Indaba (Dakar)
- FOSSIndia (Bangalore)
- RegenAI (Ashland)
- LF Europe (Vienna)
- Nerdearla (Buenos
Aires)

RC1

Weekly townhalls
- All Things
Open (Raleigh)
- Data Workshop
(Paris)

Stable
Version

Open Source AI Deﬁnition

Get Involved!

Endorse The OSAID

9

Call for
Public Participation

Endorse the OSAID!
opensource.org/osaid-endorse

Endorse the OSAID!
● In preparation for next month’s launch, we are
seeking both individual and organizational
endorsements of the OSAID.
● “Endorsement” means your name and
organizational affiliation will be appended to a
press release announcing the OSAID.
● Use OSI’s webform to give your endorsement.
● Email or DM Mer or Nick with any questions.
11

How to Participate :)
● Endorse the OSAID!
○ opensource.org/osaid-endorse
● Public forum: discuss.opensource.org
● Become an OSI member
○ Free or or full
○ SSO with other OSI websites
● Biweekly virtual townhalls… like this one!
12

Q&A

13

Thank you
We realize this is difficult work and we appreciate
your help and openness in improving the deﬁnition.

14


--- Subtitles for 2024-09-20 --- ###
1
00:00:00,001 --> 00:00:13,120
Welcome everyone to our town hall. We're actually going to start having these town halls every week

2
00:00:13,120 --> 00:00:21,040
until the OSAID is launched on October 28th. So we'll be seeing more of you. These are the

3
00:00:21,040 --> 00:00:26,960
community agreements which you're familiar with if you've been here before. One mic, one speaker,

4
00:00:26,960 --> 00:00:32,000
so allow one person to speak at a time, to take space, make space, if you tend to talk more,

5
00:00:32,000 --> 00:00:37,600
pause to let others share, if you tend not to share we invite you to speak up and share your

6
00:00:37,600 --> 00:00:42,880
opinions, to be kind to each other, just not acknowledging that the work is hard, but we can

7
00:00:42,880 --> 00:00:47,920
still be gentle with each other. Obviously hate speech is not permitted in the meeting, that

8
00:00:47,920 --> 00:00:55,920
hopefully is not a surprise, that we're trying to move forward, obviously, that obstacles are marked

9
00:00:55,920 --> 00:01:01,840
to be discussed, but that we try to continue to make progress even when we find those, oh

10
00:01:01,840 --> 00:01:10,400
Nick, I am seeing my whole slide, it's okay, that we seek solutions,

11
00:01:10,400 --> 00:01:15,840
just because there are so many ways that this could not work and finding those ways that this

12
00:01:15,840 --> 00:01:22,160
can work is needed. Does anyone have any other community agreements that they'd like us to

13
00:01:22,800 --> 00:01:26,640
use as we conduct this meeting? You can share it in the chat.

14
00:01:26,640 --> 00:01:39,760
Drinking a bubbly water, okay. Thank you Nick. So we're on version 0.0.9 which was released at

15
00:01:39,760 --> 00:01:45,840
the end of last month, if you use that QR code you can view it online, I believe this is the

16
00:01:45,840 --> 00:01:54,080
HackMD instance with the commenting function, and I'll go through that now. Next. Yeah, so the

17
00:01:54,080 --> 00:02:02,320
OpenSource AI definition has two parts in addition to the preamble, it affirms the four freedoms

18
00:02:02,320 --> 00:02:11,280
as initially enunciated by the Free Software Foundation, but now applied to AI systems, so

19
00:02:12,720 --> 00:02:18,800
grants the freedom to use the system for any purpose without asking permission, to study the

20
00:02:18,800 --> 00:02:24,240
system and inspect its components, to modify the system for any purpose, including to change its

21
00:02:24,240 --> 00:02:30,400
output, and to share the system again with or without modification and for any purpose.

22
00:02:30,400 --> 00:02:37,600
Next. And then the second part of the definition is the preferred form

23
00:02:38,640 --> 00:02:44,240
to make modifications, and again this is for machine learning, OpenSource AI system, right,

24
00:02:44,240 --> 00:02:51,680
so the technology is part of what defines the preferred form, the particular methodology.

25
00:02:51,680 --> 00:02:58,800
So first it requires open weights, so that's model weights and parameters, requires open code,

26
00:02:58,800 --> 00:03:06,000
which means source code used to train and run the system, and it requires data information,

27
00:03:06,000 --> 00:03:10,960
which is the data set or detailed information about the data used to train the system,

28
00:03:10,960 --> 00:03:16,560
and that actually we should change that particular way we're phrasing that,

29
00:03:16,560 --> 00:03:21,920
because the detailed information about the data used to train and run the system is required,

30
00:03:21,920 --> 00:03:31,840
whether or not a data set is also required. Okay, thank you. So what is next, September and

31
00:03:31,840 --> 00:03:39,040
October? We're resolving the comments. We intended to release our RC1 this week. It is still being

32
00:03:39,040 --> 00:03:44,640
edited for reasons some of you may have heard the conversation is having with Gerardo, but there's

33
00:03:44,640 --> 00:03:51,280
still some edits being made to RC1, the release candidate one. Our goal is that we will launch

34
00:03:51,280 --> 00:03:58,240
that next week though, at the Nerdeala conference in Argentina, and then we will have a staple

35
00:03:58,240 --> 00:04:06,160
version launched at All Things Open on October 8th, 28th at the end of next month. Next.

36
00:04:06,160 --> 00:04:13,120
And also just so you know the requirements that we have internally for this launch,

37
00:04:13,120 --> 00:04:18,800
that the board requires that the open source AI definition or OSAID be supported by diverse

38
00:04:18,800 --> 00:04:28,160
stakeholders and specifically for stakeholder types, end users, deployers, developers, and

39
00:04:28,160 --> 00:04:35,920
subjects. So I think in this slide there are not definitions of that, but we can define that if

40
00:04:35,920 --> 00:04:42,640
there's interest, and that that be global representation. Secondly, that there are real

41
00:04:42,640 --> 00:04:49,280
life examples of these AI systems, so it is not, the definition does not result in an empty set.

42
00:04:49,280 --> 00:04:55,680
And finally, that we are able to complete this process by that

43
00:04:55,680 --> 00:05:00,400
deadline that I gave previously of the end of October. Next.

44
00:05:00,400 --> 00:05:06,480
So this is just to let you know where we are. We're in September.

45
00:05:07,600 --> 00:05:14,480
We have done a lot of, we and actually our partners, have done a lot of sharing

46
00:05:14,480 --> 00:05:22,000
the OSAID internationally this month. We had Rahmat, who is a organizational endorser,

47
00:05:22,000 --> 00:05:28,000
which I'll talk about later, and co-designer presented the OSAID in Dakar at the Deep Learning

48
00:05:28,000 --> 00:05:35,760
in Dhaba. We had Taranima Prabhakar, who's a, I'd say a friend, a friend of the OSAID,

49
00:05:36,560 --> 00:05:41,920
and the founder of Taddle, an open source AI system, presented at FOSS India in Bangalore.

50
00:05:41,920 --> 00:05:47,280
I presented in the US at Regen AI, Ashland's in the state of Oregon.

51
00:05:47,280 --> 00:05:55,920
Stefano presented the OSAID in Vienna at Lung's Foundation Europe, and then I will,

52
00:05:55,920 --> 00:06:04,720
inshallah, present in Buenos Aires next week at NERD-ER-LA. And then next week, next month, we

53
00:06:04,720 --> 00:06:12,560
will present at All Things Open, which is in Raleigh, and then we have a data workshop, which

54
00:06:12,560 --> 00:06:20,880
will, which is a data policy paper that deals with, I would say, data questions emerging from the

55
00:06:20,880 --> 00:06:24,400
open source AI definition, but is not specifically working on the definition,

56
00:06:24,400 --> 00:06:32,080
and that will generate a white paper. Okay, yes, get involved. So the big ask,

57
00:06:33,760 --> 00:06:38,720
do you want to do the ask, Nick, the endorse the OSAID ask, because you did, you did this web form.

58
00:06:38,720 --> 00:06:47,280
So do you want to do it, or shall I? Yes, so right now, if you join the website,

59
00:06:47,280 --> 00:06:55,840
and if you look under the drafts, there's a form now for endorsements. So if you'd like to join

60
00:06:55,840 --> 00:07:05,520
that, and I'll paste the link here as well to the form, you can endorse that either as an individual

61
00:07:05,520 --> 00:07:15,040
or as an organization. So let me just drop the link to the form, and it's very simple. You just

62
00:07:15,040 --> 00:07:24,320
add your name, your role, your institution, and if you're endorsing this as an individual or

63
00:07:24,320 --> 00:07:32,800
as an institution, or both, that's an option as well. You can add your comments, and once we

64
00:07:32,800 --> 00:07:41,840
are going to be releasing this, the new release candidates, we'll also showcase some quotes,

65
00:07:41,840 --> 00:07:49,120
and if you add a message there, that should appear as well. Thanks so much. Next slide.

66
00:07:49,120 --> 00:07:57,680
So this is also just sharing with you the definition, and well, you can go back to that

67
00:07:58,640 --> 00:08:11,680
colorful one. Okay, well, they, oh, oh, oh, oh, oh, yeah. So basically, just to say this is a call,

68
00:08:11,680 --> 00:08:21,280
another call for public participation, that anyone who is on the web and would like to endorse the

69
00:08:21,280 --> 00:08:27,840
OSAID is welcome to do so. We do, as Nick said, have the option for organizations to endorse the

70
00:08:27,840 --> 00:08:35,600
OSAID, which is in a more formal process, but also any individual can choose to endorse the OSAID,

71
00:08:35,600 --> 00:08:44,800
and what that means is that when we do have our official launch in October, that your name and

72
00:08:44,800 --> 00:08:50,640
affiliation will be listed on the landing page as an endorser, kind of like the signature at

73
00:08:50,640 --> 00:09:03,600
the end of a document. So, yes, thank you. Next. Hi. So I think that gave, yeah, I think that

74
00:09:03,600 --> 00:09:08,880
gave that information that I was just describing. We can put it up in text still, go back. Some

75
00:09:08,880 --> 00:09:16,160
people like to read as well as to hear information. Could you go back to that previous slide, Nick?

76
00:09:19,520 --> 00:09:24,240
Thank you. Yeah, so this is just affirming that there's individual and organizational,

77
00:09:24,240 --> 00:09:29,440
and it's clarifying what endorsement means, which is that your name is going on, it says

78
00:09:29,440 --> 00:09:35,440
your press release, but it's a public statement that will be on our website, announcing the OSAID,

79
00:09:35,440 --> 00:09:40,800
the web form, and then you can email or DM Nick or I if you have any questions about this,

80
00:09:40,800 --> 00:09:45,760
in addition to the Q&A we're about to have. Now we can go on to the final slide.

81
00:09:46,400 --> 00:09:50,880
Yep, so that's our request. We would love your endorsement of the OSAID,

82
00:09:50,880 --> 00:09:59,280
and to participate in our public forum, which has always been available, and attend weekly town

83
00:09:59,280 --> 00:10:06,080
halls are the ways to currently participate in the process, and that QR code is to the forum,

84
00:10:06,080 --> 00:10:10,720
I believe. So if you go to the next slide.

85
00:10:15,280 --> 00:10:21,360
I will open it up to Q&A, and please, please raise your hand, and I or Nick will call on you,

86
00:10:21,360 --> 00:10:28,560
and allowing everyone to have a comment before we have double up on comments from any one person,

87
00:10:28,560 --> 00:10:33,760
and you can also text in the chat if you would like to do that instead of speaking aloud.

88
00:10:33,760 --> 00:10:40,240
But yeah, I would love to have your comments and questions.

89
00:10:42,240 --> 00:10:48,240
All right.

90
00:10:48,240 --> 00:10:53,920
All right. Hello, or Nick, are there any comments or questions that you feel might be interesting

91
00:10:53,920 --> 00:10:59,520
for us to talk through? Is there anything, anything that I can clarify?

92
00:10:59,520 --> 00:11:03,120
I know that you're going to be talking about the OSAID, and I'm going to be talking about

93
00:11:03,120 --> 00:11:07,520
the OSAID, and I'm going to be talking about the OSAID, and I'm going to be talking about

94
00:11:07,520 --> 00:11:12,720
the OSAID, and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

95
00:11:12,720 --> 00:11:16,320
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

96
00:11:16,320 --> 00:11:19,680
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

97
00:11:19,680 --> 00:11:22,480
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

98
00:11:25,280 --> 00:11:27,280
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

99
00:11:29,280 --> 00:11:35,040
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

100
00:11:35,040 --> 00:11:37,040
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

101
00:11:47,040 --> 00:12:09,040
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

102
00:12:09,040 --> 00:12:11,040
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID,

103
00:12:17,040 --> 00:12:19,040
and I'm going to be talking about the OSAID, and I'm going to be talking about the OSAID.

104
00:12:19,040 --> 00:12:21,040
And I'm going to be talking about the OSAID.

### End of last town hall held on 2024-09-20 ###

### Start of next town hall held on 2024-09-27 ###
--- Presentation for 2024-09-27 ---
Open Source Initiative

Deﬁning
Open Source AI
Mer Joyce | September 27, 2024

Supported by:

Why Deﬁne Open Source AI
Now?

Mer Joyce

Frontier of OS

Shaping Regulation

Combat Open-Washing

Deﬁning Open Source AI is
the most signiﬁcant
challenge facing the open
source movement.

Government regulations
have begun in the EU, US,
and elsewhere. We have
the opportunity to share
these new policies and
laws by deﬁning OSAI.

Companies are calling AI
systems “open source”
even though their licenses
contain restrictions that go
against the accepted
principles and freedoms of
open source.

Beneﬁts of Open Source AI
by Lea Gimpel

Mer Joyce

Transparency + Safety

Market Deconcentration +
AI Polyculture

OSAI provides information
essential for auditing
systems and to mitigate
bias, ensures
accountability and
transparency of data
sources, and accelerates AI
safety research

OSAI makes more models
available, spurs innovation
and quality due to
increased competition and
tackles AI monoculture by
providing more
stakeholders access to
foundational technology.

Diverse Applications
OSAI gives developers
access to resources crucial
for developing contextspeciﬁc, localized applications that are representative of cultural and
linguistic diversity and allow
for model aligned with
different value systems.

https://opensource.org/deepdive/we
binars

2022-2023 research
Interviews (Podcast)

Panel discussions

Webinar series

Four panels with 4 experts covering 4
area:
●
Business
●
Society
●
Legal
●
Academia

18 experts of different disciplines
from all over the world dissected
issues from data governance, privacy,
labor laws, software development and
more.

https://deepdive.opensource.org/report/
We’ve known that the availability of training data was THE ISSUE

https://opensource.org/deepdive/webinars

Co-Designing
the OSAID
How we created the Open Source AI
Deﬁnition through global consultation.

Co-Designing the OSAID
A Global Snapshot
Our co-design process included in-person
workshops on five continents – South America,
North America, Africa, Europe, and Asia – and
virtual participants from more than 35 countries.

OSAID Co-Design Question

1
Mer Joyce

Use • Study • Modify • Share
What should these open
source principles mean for
artiﬁcial intelligence?

Open Source
AI Definition
Four
Freedoms
v.0.0.9

1. Use the system for any
purpose and without having to
ask for permission.
2. Study how the system works
and inspect its components.
3. Modify the system for any
purpose, including to change
its output.
4. Share the system for others to
use with or without
modiﬁcations, for any purpose.

OSAID Co-Design Question

2
Mer Joyce

What components must be
open in order for an AI
system to be used, studied,
modiﬁed, and shared?

Virtual
Workgroups
Llama 2 Group
1.
2.
3.
4.
5.
6.
7.
8.

Bastien Guerry
DINUM, French public
administration
Ezequiel Lanza Intel
Roman Shaposhnik
Apache Software
Foundation
Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic Institute of
Paris
Mo Zhou Debian,
Johns Hopkins
University
Victor Lu independent
database consultant

Members agreed to make their names and affiliations public to support the
transparency of the co-design process.

BLOOM Group
1.
2.
3.
4.
5.
6.
7.
8.

George C. G. Barbosa
Fundação Oswaldo Cruz
Daniel Brumund GIZ
FAIR Forward - AI for all
Danish Contractor
BLOOM Model Gov. WG
Abdoulaye Diack
Google
Jaan Li University of
Tartu, Phare Health
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
Ofentse Phuti WiMLDS
Gaborone
Caleb Fianku Quao
Kwame Nkrumah
University of Science and
Technology, Kumasi

Pythia Group
1.
2.

3.
4.

Seo-Young Isabelle
Hwang Samsung
Cailean Osborne
University of Oxford,
Linux Foundation
Stella Biderman
EleutherAI
Justin Colannino
Microsoft

5.

Hailey Schoelkopf
EleutherAI

6.

Aviya Skowron
EleutherAI

Over 50% of participants
are People of Color, 30%
are Black, and 25% are
women, trans, and
nonbinary.

OpenCV Group
1.
2.
3.
4.
5.
6.
7.

8.
9.
10.

Rahmat Akintola
Cubeseed Africa
Ignatius Ezeani
Lancaster University
Kevin Harerimana CMU
Africa
Satya Mallick OpenCV
David Manset ITU
Phil Nelson
OpenCV
Tlamelo Makati
WiMLDS Gaborone,
Technological University
Dublin
Minyechil Alehegn
Tefera Mizan Tepi
University
Akosua Twumasi
Ghana Health Service
Rasim Sen Oasis
Software Technology Ltd.

Selecting the required components described in the

Voting for Requirements checklist and preferred form section
1. Workgroup
Votes

2. Public Vote
Compilation
3. Public Results
Report on Forum

4. Deﬁnition v.0.0.6
+ Checklist

components from
Model Openness
Framework (MOF)

March 1, 2024
March 10, 2024

OSAID Co-Design Question

3
Mer Joyce

Which AI systems
meet the criteria of
the OSAID?

We were interested in reviewing about 10 AI systems self-described as

Validation Reviewers open to validate the definition. Work began May 1, 2024.
1. Arctic
1.

Jesús M.
Gonzalez-Barahona
Universidad Rey Juan
Carlos

2. BLOOM
2.
3.

Danish Contractor
BLOOM Model Gov.
Work Group
Jaan Li University of
Tartu, One Fact
Foundation

3. Falcon
1.
2.

Casey Valk Nutanix
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France

4. Grok
1.
2.

Victor Lu
independent
database consultant
Karsten Wade Open
Community
Architects

5. Llama 2
1.
2.
3.
4.

Davide Testuggine
Meta
Jonathan Torres
Meta
Stefano Zacchiroli
Polytechnic
Institute of Paris
Victor Lu
independent
database consultant

6. LLM360
5.

[Team member
TBD] LLM360

7. Phi-2
6.

Seo-Young Isabelle
Hwang Samsung

8. Mistral
1.
2.
3.

Mark Collier
OpenInfra
Foundation
Jean-Pierre Lorre
LINAGORA,
OpenLLM-France
Cailean Osborne
University of Oxford,
Linux Foundation

9. OLMo
4.
5.

Amanda Casari
Google
Abdoulaye Diack
Google

10. OpenCV
1.

Rasim Sen Oasis
Software
Technology Ltd.

11. Pythia
1.
2.

Seo-Young Isabelle
Hwang Samsung
Stella Biderman
EleutherAI

3.

Hailey Schoelkopf
EleutherAI

4.

Aviya Skowron
EleutherAI

12. T5
5.

Jaan Li University of
Tartu, One Fact
Foundation

13. Viking
6.

Merlijn Sebrechts
Ghent University

Validation Review

Each system is reviewed on a public form, to maximize transparency.

Validation is ongoing. We have found that creator participation is in

Validation Progress most cases necessary to identify all the legal documents needed to
ascertain openness. Last updated in June, 2024.

Current Version: 0.0.9

Open Source AI Deﬁnition
4 Freedoms
● Use
● Study
● Modify
● Share

+

Open Weights

● Model
weights and
parameters

+

Open Code

● Source code
used to train
and run the
system

+

Data Info

● Dataset or
detailed
information
about the
data used to
train the
system

The AI conundrums
“If we assume, for example, that the definition requires full
release of datasets, one thing is certain: in Julia’s words, it
would be “a definition for which few existing systems qualify.”
(OSI note: also less powerful and limited to specific domains
https://redmonk.com/sogrady/2024/07/03/ai-conundrums/

A long history of
exceptions
“The GNU C library uses a special kind of copyleft called the GNU
Library General Public License, which gives permission to link
proprietary software with the library. Why make this exception? [...]
It is not a matter of principle;[...] but strategically it seems that
disallowing them would do more to discourage use of the GNU
system than to encourage development of free applications
Richard Stallman
https://www.gnu.org/philosophy/fsfs/rms-essays.pdf

Board Guidance
The OSI Board requires a deﬁnition that is:

Supported by
diverse stakeholders

Provides real-life
examples

Ready by
October 2024

The deﬁnition
needs to have
approval by end
users, developers,
deployers and
subjects of AI,
globally.

The deﬁnition must
include relevant
examples of AI
systems that comply
with it at the time of
approval, so cannot
have an empty set.

A usable version of
the deﬁnition
needs to be ready
for approval by the
board at the
October board
meeting.

Approved June 21, 2024

Open Source
AI Definition
The general structure of
the document

Basic concepts

The Open Source
AI Definition

The definition of
preferred form to
make modifications
to machine learning

Clarifications

Unlikely to
change in the
future

Most likely to
change in the
future

New FAQ entry (preview)
Open training data: data that can be copied, preserved and reshared. It provides the best way to enable users to study the
system.
Public training data: data that others can inspect as long as it remains available. This also enables users to study the work.
However, this data can degrade as links or references are lost or removed from network availability. To obviate this, different
communities will have to work together to define standards, procedures, tools and governance models to overcome this risk,
and Data Information is required in case the data becomes later unavailable..
Obtainable training data: data that can be obtained, including for a fee. This information provides transparency and is
similar to a purchasable component in an open hardware system. The Data Information provides a means of understanding
this data other than obtaining or purchasing it. This is an area that is likely to change rapidly and will need careful monitoring
to protect Open Source AI developers.
Unshareable non-public training data: data that cannot be shared for explainable reasons, like Personally Identifiable
Information (PII). For this class of data, the ability to study some of the system's biases demands a detailed description of the
data – what it is, how it was collected, its characteristics, and so on – so that users can understand the biases and
categorization underlying the system.

The Board requires that the definition have approval by end users,

Stakeholder Groups developers, deployers and subjects of AI, globally.
Stakeholder

Description

Example

Vols

1. Developer

Makes AI system and/or
component that will be studied,
used, modiﬁed, or shared
through an open source license

ML researcher in academia or
industry

31%

2. Deployer

Seeks to study, use modify, or
share an open source AI system

AI engineer in industry, health
researcher in academia

46%

3. End User

Consumes a system output, but
does not seek to study, use,
modify, or share the system

Student using a chatbot to write
a report, artist creating an image

≈ 90%

4. Subject

Affected upstream or
downstream by a system output
without interacting with it
intentionally + advocates for this
group.

Photographer who ﬁnds their
image in training dataset
(upstream), mortgage applicant
evaluated by a bank’s AI system
(downstream)

≈ 100%
32

32

System testing work stream

2024 Timeline

Stakeholder consultation work stream
Release schedule

February

April

Call For Volunteers
+ Activity
Feedback and
Revision

Virtual System
Review

Bi-Weekly
Virtual
Public
Townhalls

Bi-Weekly
Virtual
Public
Townhalls

Draft 0.0.5

Draft 0.0.8

July

August

September

October

Virtual System
Review

Virtual System
Review

Feedback and
review
Hardening WG

Feedback and
review
Hardening WG

Hola 👋
Townhalls +

Townhalls +

- OSPOs for
Good (NYC)
- OS Community
Africa (virtual)

- AI-dev (Hong
Kong)
- OSC (Beijing)

Draft 0.0.8

Draft 0.0.9

Weekly townhalls
- DL Indaba (Dakar)
- FOSSIndia (Bangalore)
- RegenAI (Ashland)
- LF Europe (Vienna)
- Nerdearla (Buenos
Aires)

RC1

Weekly townhalls
- All Things
Open (Raleigh)
- Data Workshop
(Paris)

Version 1.0

Call for Public Participation

Endorse
the
OSAID!
Mer Joyce

opensource.org/osaid-endorse

Endorse the OSAID!
● In preparation for next month’s launch, we are
seeking both individual and organizational
endorsements of the OSAID.
● “Endorsement” means your name and
organizational aﬃliation will be appended to a press
release announcing the OSAID.
● Use OSI’s webform to give your endorsement.
● Email or DM me on the forum with any questions.
35

How to Participate
● Endorse the OSAID!
○ opensource.org/osaid-endorse
● Public forum: discuss.opensource.org
● Comment on RC1 next week
● Become an OSI Member (free + paid)
● Weekly virtual town halls

Thank You,
Co-Designers!
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

The following individuals volunteered as workgroup members or system reviewers
during the co-design process. This group of 36 volunteers represents 23 countries by
birth and residence. African participants are highlighted.

Rahmat Akintola Cubeseed Africa
George C. G. Barbosa Fundação Oswaldo Cruz
Stella Biderman EleutherAI
Amanda Casari Google
Justin Colannino Microsoft
Mark Collier OpenInfra Foundation
Daniel Brumund GIZ FAIR Forward - AI for All
Danish Contractor BLOOM Model Gov. WG
Abdoulaye Diack Google
Ignatius Ezeani Lancaster University
Jesús M. Gonzalez-Barahona Universidad Rey
Juan Carlos
Bastien Guerry DINUM, French public
administration
Kevin Harerimana CMU Africa
Seo-Young Isabelle Hwang Samsung
Ezequiel Lanza Intel
Jaan Li University of Tartu, One Fact Foundation
Jean-Pierre Lorre LINAGORA, OpenLLM-France
Victor Lu independent database consultant

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

Tlamelo Makati WiMLDS Gaborone, Technological
University Dublin
Satya Mallick OpenCV
David Manset ITU
Phil Nelson OpenCV
Cailean Osborne University of Oxford,
Linux Foundation
Ofentse Phuti WiMLDS Gaborone
Caleb Fianku Quao Kwame Nkrumah University of
Science and Technology, Kumasi
Hailey Schoelkopf EleutherAI
Rasim Sen Oasis Software Technology Ltd.
Roman Shaposhnik Apache Software Foundation
Aviya Skowron EleutherAI
Minyechil Alehegn Tefera Mizan Tepi University
Davide Testuggine Meta
Jonathan Torres Meta
Akosua Twumasi Ghana Health Service
Casey Valk Nutanix
Stefano Zacchiroli Polytechnic Institute of Paris
Mo Zhou Debian, Johns Hopkins University

Q+A

Mer Joyce


--- Subtitles for 2024-09-27 --- ###
1
00:00:00,001 --> 00:00:07,000
All right, thanks everyone for joining this day in the slide access.

2
00:00:07,000 --> 00:00:12,720
I'm Stefano Maffulli, I'm the Executive Director of the Open Source Initiative.

3
00:00:12,720 --> 00:00:18,760
And a reminder, we've been doing this work thanks to a grant from Alfred Sloan Foundation

4
00:00:18,760 --> 00:00:25,000
and a donation from MercatoLibre, which has joined us as sponsors.

5
00:00:25,000 --> 00:00:32,000
Okay, so let's go through quickly a little bit of the history of how we got here

6
00:00:32,000 --> 00:00:37,000
and what we were doing and why we're doing this.

7
00:00:37,000 --> 00:00:43,000
So, why we're defining the Open Source AI now is because

8
00:00:43,000 --> 00:00:51,000
these technologies, since the introduction of chat GPTs or the Open AI launches,

9
00:00:51,000 --> 00:01:00,000
have significantly, significantly impacted how the IT environment in the software spaces

10
00:01:00,000 --> 00:01:08,000
has operated and worked and changed a lot of the fundamentals of our understanding of software

11
00:01:08,000 --> 00:01:11,000
and the Open Source definition.

12
00:01:11,000 --> 00:01:19,000
At the same time, we've seen an incredibly fast reaction from governments around the world,

13
00:01:19,000 --> 00:01:28,000
especially the United States, Europe, but also Japan, China, have started to write laws directly.

14
00:01:28,000 --> 00:01:35,000
And some of these laws, especially the AI Act is very explicit, mentions Open Source AI

15
00:01:35,000 --> 00:01:43,000
and Open Source systems and models without having a specific understanding of what that means.

16
00:01:43,000 --> 00:01:50,000
And the reason for the regulators to do that is to allow for special carve-outs for research and innovation.

17
00:01:50,000 --> 00:01:55,000
Like having seen how the Open Source definition worked for software

18
00:01:55,000 --> 00:02:00,000
and allowed for a thriving ecosystem that is worth apparently 8.8 trillion,

19
00:02:00,000 --> 00:02:09,000
or in any case, it's a huge sum of money, and it's underlying the whole digital infrastructure.

20
00:02:09,000 --> 00:02:15,000
So regulators want to keep everything safe, keep society safe, but at the same time,

21
00:02:15,000 --> 00:02:20,000
they want to make sure that the risks are reduced.

22
00:02:20,000 --> 00:02:28,000
And trying to find that balance revolves around the concept of Open Source AI.

23
00:02:28,000 --> 00:02:34,000
And third is this open washing thing that we've seen a lot of companies,

24
00:02:34,000 --> 00:02:41,000
and especially one abusing of the term Open Source and calling out meta in this space,

25
00:02:41,000 --> 00:02:50,000
for releasing models and releasing artifacts that are not Open Source by any stretch of imagination,

26
00:02:50,000 --> 00:02:55,000
especially their licensing in terms of conditions or not.

27
00:02:55,000 --> 00:03:03,000
But because there is a lack of clarity, they can claim whatever they want,

28
00:03:03,000 --> 00:03:14,000
because there is no easy way to push back and point at a widely agreed upon definition of what Open Source AI actually means.

29
00:03:14,000 --> 00:03:21,000
And so we asked them, I mean, this is just to show the Digital Public Goods Alliance

30
00:03:21,000 --> 00:03:33,000
have given a little, illustrated what they think the benefits of Open Source AI is in general.

31
00:03:33,000 --> 00:03:37,000
So without further ado, let me start from the history.

32
00:03:37,000 --> 00:03:45,000
What we've done is that in late 2021, when I joined the OSI,

33
00:03:45,000 --> 00:03:54,000
we knew it was immediately after Copilot was launched, and it was immediately clear that the word was not going to be the same with that.

34
00:03:54,000 --> 00:03:58,000
So we launched this research that we call Deep Dive AI.

35
00:03:58,000 --> 00:04:00,000
We wanted to understand the space.

36
00:04:00,000 --> 00:04:08,000
So in 2022 and 2023, we ran, if you want, research endeavor.

37
00:04:08,000 --> 00:04:11,000
We interviewed experts. We started to ask them questions.

38
00:04:11,000 --> 00:04:15,000
We published them in a form of deep podcast.

39
00:04:15,000 --> 00:04:20,000
And I recommend you to listen to it because it's really, if you're new to the space,

40
00:04:20,000 --> 00:04:28,000
it gives very good introduction to the legal aspect, the technical aspects, copyright, business and society security.

41
00:04:28,000 --> 00:04:36,000
Also, there's an interview with DARPA program manager for AI security, very eye opening.

42
00:04:36,000 --> 00:04:47,000
Next year, we've done, I mean, in the same year, we also run four panels with four experts covering business, society, legal and academic views.

43
00:04:47,000 --> 00:04:55,000
And we published their report. So if you don't want to listen to the podcast, you don't want to listen to the panel recordings, look at the report.

44
00:04:55,000 --> 00:05:04,000
It summarizes the issue. And together with the webinar series, with interviews and presentations from experts from different parts of the world,

45
00:05:04,000 --> 00:05:16,000
we knew at the beginning or in the middle of 2023, we knew that the issue that we needed to solve was that of the availability of training data.

46
00:05:16,000 --> 00:05:24,000
We knew that that was the problem that we needed to solve in some way, shape or form.

47
00:05:24,000 --> 00:05:26,000
Because it was the most controversial one.

48
00:05:26,000 --> 00:05:34,000
So we thought about using a co-design method to come up with a solution, a plausible solution.

49
00:05:34,000 --> 00:05:43,000
So we went around the world, we asked people questions and we also set up online meetings in order to have more variety.

50
00:05:43,000 --> 00:05:52,000
And we asked them a series of three questions. The first one, what you should be, what should the words or use the verbs,

51
00:05:52,000 --> 00:05:58,000
use, study, modify, share, mean for in the realm, in the domain of artificial intelligence.

52
00:05:58,000 --> 00:06:16,000
And after a few meetings in person and online, it was not too complicated to reproduce the basic principles of the free software definition and port them to the domain of AI.

53
00:06:16,000 --> 00:06:26,000
Then we needed to understand how to implement those freedoms. If you want to use, if you want to study an AI system, what do you need?

54
00:06:26,000 --> 00:06:32,000
So we collected virtual working groups online to analyze four types of systems.

55
00:06:32,000 --> 00:06:40,000
One is Lama, which is completely proprietary. The other one is Bloom, which is very open science, but with a bad license.

56
00:06:40,000 --> 00:06:53,000
PTA is a very open science LLM and OpenCV group analyzed the computer vision neural networks, zoo models, models in the zoo.

57
00:06:53,000 --> 00:07:02,000
So to, to, to find the analysis, to, to look at it from, from also the angle of something that is not an LLM.

58
00:07:02,000 --> 00:07:12,000
And we run some votes, not scientific. I, it was never the intention of being scientific. I've seen conversations on the forum.

59
00:07:12,000 --> 00:07:20,000
The idea was to gather a little bit of feedback that will allow us to move on and check an hypothesis.

60
00:07:20,000 --> 00:07:34,000
The hypothesis here was given the, that known, it showed that the groups were not unanimous in saying that the training data was always necessary to study and modify, especially for modify.

61
00:07:34,000 --> 00:07:39,000
For studying it, it was, that component was a little bit higher.

62
00:07:39,000 --> 00:07:49,000
But we have an hypothesis and the hypothesis to solve the problem of data was to see, okay, if we don't require it, what happens?

63
00:07:49,000 --> 00:08:01,000
So we asked the third questions. Let's go outside, look at the, look at the world, look at existing systems and check which ones would comply if we don't require strictly the training data.

64
00:08:01,000 --> 00:08:08,000
But we require the other components, which is the data processing code and the training code.

65
00:08:08,000 --> 00:08:19,000
So we, we threw, we widened the search to more models to, for this validation phase.

66
00:08:19,000 --> 00:08:35,000
And in fact, we ended up with the expected results. In other words, none of the systems that we knew were bad, like LLAMA or,

67
00:08:35,000 --> 00:08:41,000
or Bloom, for example, we knew that those ones would not pass for different reasons.

68
00:08:41,000 --> 00:08:47,000
So that was that signal. It was just a signal that there might be a way to solve the issue.

69
00:08:47,000 --> 00:08:52,000
And the result was this, right? We, the definition is structured this way.

70
00:08:52,000 --> 00:09:00,000
There is the four freedoms and then the requirements for the preferred form of making modifications to machine learning into these three pieces.

71
00:09:00,000 --> 00:09:10,000
You have to have the weights, the parameters need to be available with freedom respecting licenses and legal terms.

72
00:09:10,000 --> 00:09:18,000
Software needs to be complete for the training, the training and for the data processing.

73
00:09:18,000 --> 00:09:26,000
And then you need a very detailed description of what the data we use for training actually is.

74
00:09:26,000 --> 00:09:32,000
So Redmonk put it quite nicely and he called this the AI conundrums, right?

75
00:09:32,000 --> 00:09:39,000
And he quotes Giulio Ferraioli here. And this is a very interesting quote.

76
00:09:39,000 --> 00:09:48,000
So it says that if we assume that the definition requires full release of datasets, one thing is certain.

77
00:09:48,000 --> 00:09:53,000
It would be a definition for which very few existing systems qualify.

78
00:09:53,000 --> 00:09:59,000
And also those that qualify are less powerful and limited to specific domains.

79
00:09:59,000 --> 00:10:08,000
And then because then the other thing to solve, I mean, the other approach to think about this conundrum is that on the other hand,

80
00:10:08,000 --> 00:10:16,000
we have in the free software and open source movement a long history of making exceptions and finding ways to solve the problems

81
00:10:16,000 --> 00:10:27,000
in order to have more freedom and more open source software.

82
00:10:27,000 --> 00:10:40,000
Now, the other constraints that we have in this whole process is that the board, the OSI board, required three basic pillars, the elements.

83
00:10:40,000 --> 00:10:46,000
We want the definition to be supported by diverse stakeholders.

84
00:10:46,000 --> 00:10:50,000
That means not only end users, not only developers, but all of them.

85
00:10:50,000 --> 00:10:55,000
So end users, developers, deployers of AI, subjects of AI.

86
00:10:55,000 --> 00:11:06,000
And they want to have global representation of diverse interests in the list of endorsers and support.

87
00:11:06,000 --> 00:11:14,000
They also require, the board also requires that the definition must include relevant examples of AI systems.

88
00:11:14,000 --> 00:11:18,000
And this is for a specific tactical reason.

89
00:11:18,000 --> 00:11:32,000
The regulators will not consider the open source AI definition as an interesting one or something to follow and imitate if it is not a general one,

90
00:11:32,000 --> 00:11:39,000
if it doesn't cover general cases and if it doesn't cover and if it excludes clearly anything.

91
00:11:39,000 --> 00:11:44,000
I mean, if it really limited to only small domains.

92
00:11:44,000 --> 00:11:50,000
And it needs to be ready by October. And again, the main reason for this is timing.

93
00:11:50,000 --> 00:11:56,000
The AI Act is already out there and is already law.

94
00:11:56,000 --> 00:12:03,000
And it's already the legislation process, the normative process has already started.

95
00:12:03,000 --> 00:12:21,000
This means that there is a hyperactivity in Brussels, but also in D.C. and in California to convince and educate policymakers that open source just means access to open weights, access to the weights.

96
00:12:21,000 --> 00:12:39,000
And that's all they need to care about. And maybe a little bit of description of transparency in the form, if you're familiar with that data cards or other formats that are pretty lightweight to describe in metadata sense, the data set used for the training.

97
00:12:39,000 --> 00:12:49,000
And which is not sufficient. So there is an urgency to come up with a working definition.

98
00:12:49,000 --> 00:12:59,000
OK, so we're working on Release Candidate 1, which has some clarifications and basic concepts.

99
00:12:59,000 --> 00:13:04,000
This is just a screenshot. It's not really don't don't refer to it too much.

100
00:13:04,000 --> 00:13:08,000
It's going to change between now and when Release Candidate is released.

101
00:13:08,000 --> 00:13:17,000
But I wanted to show here the structure of the document to illustrate a major feature that has been here in its intention from the beginning.

102
00:13:17,000 --> 00:13:25,000
There are probably two main, two macro parts. The top part includes the basic concept, like what is a definition?

103
00:13:25,000 --> 00:13:31,000
What is it that we are defining? Which is it? What is AI? What is an AI system?

104
00:13:31,000 --> 00:13:45,000
And we added here, we may add based on feedback from the forum, the machine learning definition so that we have a list of framework reference of the topics that are covered below.

105
00:13:45,000 --> 00:13:57,000
Then there is the actual open source AI definition, what it is, right? The preamble, why we want it and what is open source AI, which includes the four freedoms.

106
00:13:57,000 --> 00:14:14,000
Now, the four freedoms, as you know, even in the free software world, the free software definition, the freedom to study and the freedom to modify are followed by a sentence that says precondition for this is access to the source code.

107
00:14:14,000 --> 00:14:28,000
And further, the source code is defined as the preferred form of making modification to the program, which is later clarified also in other documents, including the licenses, etc.

108
00:14:28,000 --> 00:14:38,000
Now, we need a frame here to understand what we're talking about, what we need, what is the preferred form of making modifications to an AI system.

109
00:14:38,000 --> 00:14:47,000
And in order to do that, we need to limit the scope. We cannot talk about all of the AI. We need to talk about machine learning in this specific case.

110
00:14:47,000 --> 00:14:56,000
It's the most pressing and urgent matter. Let's finish this part. So the second part, now we enter in the second macro part of the definition.

111
00:14:56,000 --> 00:15:04,000
This part is one that is most likely to change and evolve in future versions of the definition.

112
00:15:04,000 --> 00:15:19,000
So the preferred form of making modifications here lists the three major blocks of an AI system, a machine learning system, which are data piece, the code piece, and the parameters weights and sets conditions for those.

113
00:15:19,000 --> 00:15:38,000
And then we added, which is already in 0.9, space for clarifications. These clarifications serve the purpose of clarifying that whether you call, when you want to call and use the term open source,

114
00:15:38,000 --> 00:15:54,000
refer to anything that spits an output based on an input, whether you call it an open source AI, an open source AI system, open source model, open source weights, open source model or open source weights.

115
00:15:54,000 --> 00:16:05,000
Basically, these are all equivalent terms. You cannot skip the line to some extent and say, well, it's an open source model only.

116
00:16:05,000 --> 00:16:23,000
So, you know, the rest of the definition doesn't apply. No, if you want to call it open source model or open source weights, you have to comply with, we have to provide the preferred form of making modifications to that system.

117
00:16:23,000 --> 00:16:42,000
Now we're also working on a new FAQ and the new FAQ entry will clarify further the four types of training data that are plausible and possible in this world.

118
00:16:42,000 --> 00:16:52,000
And that is clearly, you know, open training data, public training data, obtainable training data and unshareable non-public training data.

119
00:16:52,000 --> 00:17:02,000
And we believe that we believe that all of these four types of data can be part of an open source AI.

120
00:17:02,000 --> 00:17:26,000
We don't want to limit the open source AI only to open training data or open and public training data, because that would limit the capabilities of open source systems and would limit the domain of open source systems.

121
00:17:26,000 --> 00:17:41,000
So open training data typically is something that, you know, you have, you can fairly assume that there is global coverage, global possibility to distribute that data.

122
00:17:41,000 --> 00:17:49,000
And that is a collection of facts like time series of wind speed and ocean temperatures.

123
00:17:49,000 --> 00:17:58,000
And it may seem safe, but I discovered that when trying to immigrate to China, going to China for a business trip,

124
00:17:58,000 --> 00:18:21,000
I noticed that among the reasons for Chinese government to search on entry and exit of the country to search digital devices is for the availability, for the presence of weather information, weather data on the devices.

125
00:18:21,000 --> 00:18:29,000
Meaning this means that even that kind of data is not necessarily easy to circulate around the world, like one can assume.

126
00:18:29,000 --> 00:18:39,000
Public training data is data that others can download, like, you know, the archive of my blog posts, for example.

127
00:18:39,000 --> 00:18:46,000
And this is also this is subject to availability, right? Links go stale and everything like that.

128
00:18:46,000 --> 00:18:56,000
But we can work around that with tools like common crawl or Internet archive and software heritage and places like that where they keep, they can keep the archive.

129
00:18:56,000 --> 00:19:07,000
But that's the public training data. The obtainable training data is data that you can buy or data that you can, you know, they can be obtained if you ask.

130
00:19:07,000 --> 00:19:22,000
Things like the ImageNet data set, for example, and the unshareable non-public training data is private information, that kind of like medical, medical, medical data.

131
00:19:22,000 --> 00:19:27,000
This kind of data can never be asked for, can never be distributed.

132
00:19:27,000 --> 00:19:34,000
And so we need to be careful, but we want to have open source AI medical systems.

133
00:19:34,000 --> 00:19:38,000
So we need to find a way to work around this, this piece.

134
00:19:38,000 --> 00:19:46,000
So I can give a very quick overview of where we're thinking that the release candidate might go to explain things a little bit better.

135
00:19:46,000 --> 00:19:56,000
So in the data information piece, we're trying to make it clear that the requirements are a must.

136
00:19:56,000 --> 00:20:01,000
Like the, we noticed the confusion between the work that we create.

137
00:20:01,000 --> 00:20:06,000
So we're probably going to use, suggesting is to use the word build instead.

138
00:20:06,000 --> 00:20:16,000
Substantially equivalent systems stays together with, but we are adding the fact that the data information needs to work with the code below.

139
00:20:16,000 --> 00:20:19,000
You cannot just say data information alone.

140
00:20:19,000 --> 00:20:23,000
You cannot look at data information alone and think that you can be replicating.

141
00:20:23,000 --> 00:20:38,000
And this is, if you saw online, there was a comment from Professor Perciviere, who highlighted how much the knowing about the data and knowing how it's been processed is the crucial, crucial piece.

142
00:20:38,000 --> 00:20:59,000
And specifies here in particular, if used, so for example, if one uses training data that is publicly available, it needs to be made available.

143
00:20:59,000 --> 00:21:10,000
And for the code specification, there is a clarification that the code must be complete.

144
00:21:10,000 --> 00:21:14,000
You cannot just say, hey, here's the code. No, I need to be able to run it.

145
00:21:14,000 --> 00:21:18,000
I need to be able to run it together with the data information.

146
00:21:18,000 --> 00:21:22,000
I need to know I need to get a working data set in some way or form.

147
00:21:22,000 --> 00:21:29,000
And there is also one line in addition that we're working on.

148
00:21:29,000 --> 00:21:33,000
It's a way to cover all three of these pieces.

149
00:21:33,000 --> 00:21:44,000
So code, data and the issue here is that code weights and data.

150
00:21:44,000 --> 00:21:48,000
The issue here is that could be contractual terms.

151
00:21:48,000 --> 00:22:00,000
And actually, there are like the responsible AI license family has in some of the they show that they played the experiment.

152
00:22:00,000 --> 00:22:08,000
And they have a way of in one contract to say, take this model, take this data and use it under these conditions.

153
00:22:08,000 --> 00:22:20,000
So what we're trying to say here is that we need to have a we need to give away to people like the groups at the OSI who will be reviewing these contracts.

154
00:22:20,000 --> 00:22:28,000
To make sure that we can evaluate the these packages that cover different artifacts,

155
00:22:28,000 --> 00:22:34,000
because if you notice the code individually, the elements code parameters and data,

156
00:22:34,000 --> 00:22:40,000
they have this provision that says code shall be made available on their OSI approved license.

157
00:22:40,000 --> 00:22:49,000
This means that, yes, individually, some elements are covered, you know, they can be covered by this license.

158
00:22:49,000 --> 00:22:53,000
But collectively, we don't have a way in this definition. So it was a missing piece.

159
00:22:53,000 --> 00:22:58,000
And that's what we're working on on this final line.

160
00:22:58,000 --> 00:23:10,000
All right. So these are the groups that we're working with to get endorsements and get different representation and timeline.

161
00:23:10,000 --> 00:23:25,000
We're still aiming to get version one ready for all things open in North Carolina in October, the end of the month.

162
00:23:25,000 --> 00:23:33,000
So for now, you know, we want to hear your voice. So endorse the concepts embedded in the draft.

163
00:23:33,000 --> 00:23:40,000
It's unlikely that they're going to be changing going forward. But the adaptations and clarifications, absolutely.

164
00:23:40,000 --> 00:23:46,000
They're coming because the intention is there.

165
00:23:46,000 --> 00:23:57,000
The intention is to make sure that we have a clear definition that is white, has white support and preserves the principles of open source.

166
00:23:57,000 --> 00:24:08,000
All right. And with that, I think we can go to open the floor for questions.

167
00:24:08,000 --> 00:24:23,000
If you have the mic, you can probably open it yourself or you can type.

168
00:24:23,000 --> 00:24:32,000
Yeah, so I am here. I've obviously shared a lot of my thoughts on on where we're at.

169
00:24:32,000 --> 00:24:41,000
My main concern here is that this is a needs to be a litmus test like the open source definition.

170
00:24:41,000 --> 00:24:50,000
And the open source definition has the benefit of being able to approve a license and then have that license self applied to millions of projects.

171
00:24:50,000 --> 00:24:55,000
Whereas in this case, we really need to look at every single project.

172
00:24:55,000 --> 00:25:02,000
This is getting applied to or somebody needs to look at it, whether it's us or more likely it's going to be the project itself.

173
00:25:02,000 --> 00:25:21,000
And so my my suggestion or my my belief is that this doesn't function as a litmus test and that words like sufficiently detailed and if I can tell you, well, I've got a skilled person here and they're going to say, well, it's not skilled enough.

174
00:25:21,000 --> 00:25:30,000
We've given you enough information. Right. So there's not there's not going to be a way for us to really tell anybody that there is not a source.

175
00:25:30,000 --> 00:25:37,000
I sort of going and trying to implement the thing ourselves based on the information that they've given us.

176
00:25:37,000 --> 00:25:43,000
So let's say that they do achieve the certification.

177
00:25:43,000 --> 00:26:03,000
They get that they get the mark. Does it protect the four freedoms? And I think it's quite clearly I don't think Steven's done a really good job of saying, get these are the kind of proofs that you would need to show like you need to show me that you can make the same modifications to a machine learning system with fine tuning.

178
00:26:03,000 --> 00:26:13,000
Right, we know access to the source data as what you can with access to the source data. I think, you know, as a practitioner, it's clear that that's not the case.

179
00:26:13,000 --> 00:26:18,000
I mean, I feel like we need something that people can self apply, which is like the OSD.

180
00:26:18,000 --> 00:26:35,000
And so it really needs to be something where you say this is the ingredients. This is the manifest. And maybe it's even an electronic manifest, like a JSON file or something that says these are the scripts, the training script, the training data, maybe optionally.

181
00:26:35,000 --> 00:26:42,000
And then with those checkboxes checked, then you get access to the mark, right?

182
00:26:42,000 --> 00:26:59,000
So yeah, I mean, I don't think unless somebody sat down and actually like, tried to size the four freedoms on systems that have been through this, like I maintain that it doesn't function as a litmus test.

183
00:26:59,000 --> 00:27:07,000
Yeah. All right. So it doesn't function as a litmus test. And that's what we tried in the validation phase.

184
00:27:07,000 --> 00:27:18,000
So we said, let's take the, at the time, the definition also had a component called the checklist, which I haven't mentioned.

185
00:27:18,000 --> 00:27:29,000
Basically, the checklist is the list of components of AI systems as described in the model openness framework, which is a paper.

186
00:27:29,000 --> 00:27:45,000
I don't know if you're familiar with it. It's a paper from the Linux Foundation. That paper lists 16, 18 components and classifies the availability of them is used to classify where they stand in three classes.

187
00:27:45,000 --> 00:27:54,000
Open class one is open model class. That's the only way around. Class three is open model class two.

188
00:27:54,000 --> 00:28:03,000
Open models, basically just the weights and model cards, some metadata, model card and data card, technical report, then open tooling.

189
00:28:03,000 --> 00:28:12,000
If they make available also data pre-processing and some other and some other inference code and some other pieces of code.

190
00:28:12,000 --> 00:28:21,000
And then a top class is open science where they provide every everything, every required component, including open training.

191
00:28:21,000 --> 00:28:27,000
I mean, the training code and the training and testing data sets.

192
00:28:27,000 --> 00:28:40,000
So one way that the validation phase work was to say which of these components are required, then one could go and use the model openness framework and see the availability of these marks.

193
00:28:40,000 --> 00:28:47,000
You put zero at this bar, which is between class two and class three and class one, between open tooling and open science.

194
00:28:47,000 --> 00:28:52,000
And that could be one way of having that litmus test.

195
00:28:52,000 --> 00:28:59,000
What we discovered is what you were saying. It requires the finding.

196
00:28:59,000 --> 00:29:08,000
This doesn't work at a glance the same way that open source software and open source license approved open source licenses work.

197
00:29:08,000 --> 00:29:21,000
You cannot look at the repository and see if there is a check the license file could check the open source initiative website and say, hap, this is open source approved or not.

198
00:29:21,000 --> 00:29:25,000
It's not because there are so many different components with different nature.

199
00:29:25,000 --> 00:29:31,000
Some documentation is on a blog post, some code is in GitHub, some or other repositories.

200
00:29:31,000 --> 00:29:35,000
Some of the models are on Agnephase or Kaggle.

201
00:29:35,000 --> 00:29:44,000
Finding all these pieces for a third party of a viewer is almost impossible or very, very complicated without the collaboration of the original developers.

202
00:29:44,000 --> 00:29:54,000
So this is an issue that we are aware of, but it's also an issue that we don't know how to solve today and it could be solved with practice tomorrow.

203
00:29:54,000 --> 00:30:07,000
In other words, there might be a space, a place in time where Agnephase maintains a community of experts or I don't know.

204
00:30:07,000 --> 00:30:27,000
There is a collaboration effort with universities and in fact we started talking with a few of them to see if they want to have, if they want to help us expand in next year the validation phase and fine tune the key indicators of what an open source AI needs.

205
00:30:27,000 --> 00:30:31,000
Because I have a theory in my mind, but it's just a silly theory.

206
00:30:31,000 --> 00:30:37,000
If you're not releasing the data processing code and if you're not releasing the training code, you're not open source.

207
00:30:37,000 --> 00:30:49,000
It's a very, in my mind so far with the samples that I've seen, it's an easy test, but it's not an easy one to solve.

208
00:30:49,000 --> 00:30:56,000
The nature of this animal of AI machine learning is that they are not software, simply.

209
00:30:56,000 --> 00:31:05,000
And even when software, there are examples where understanding if it's open source or not is complicated.

210
00:31:05,000 --> 00:31:12,000
And there are different ways of interpreting what open source means.

211
00:31:12,000 --> 00:31:22,000
In other words, if you look at SQLite, SQLite is an open source database program, but they don't have a community around it.

212
00:31:22,000 --> 00:31:26,000
They don't accept patches. They just release the code at their own will.

213
00:31:26,000 --> 00:31:40,000
And on the other side of the spectrum, you have ginormous projects like OpenStack or Kubernetes, complex governance, multiple contributors, etc.

214
00:31:40,000 --> 00:31:43,000
Different promises to the public.

215
00:31:43,000 --> 00:31:49,000
So there is a range in there.

216
00:31:49,000 --> 00:31:53,000
I go back to the principles that we want to say here.

217
00:31:53,000 --> 00:31:59,000
Do we want to have more open source AI?

218
00:31:59,000 --> 00:32:11,000
Do we want to give groups like Eleuther AI and Allen Institute for AI and maybe TII and other groups,

219
00:32:11,000 --> 00:32:23,000
do we want to give them a chance to build AI systems that are capable of competing with Lama and Gemma and the likes?

220
00:32:23,000 --> 00:32:36,000
Or do we want to stay in a corner where we can only use a limited amount of data and limited amount of kinds of data?

221
00:32:36,000 --> 00:32:42,000
And the OSI has never been about...

222
00:32:42,000 --> 00:33:00,000
Neither has the Free Software Foundation. It's never been about, let's make a limited copy of Unix because the POSIX standards are proprietaried by ISO and we protest the ISO's approach.

223
00:33:00,000 --> 00:33:04,000
Does that make sense, Sam?

224
00:33:04,000 --> 00:33:09,000
Yeah. I mean, SQLI, it's open source. It's in the public domain, right?

225
00:33:09,000 --> 00:33:16,000
It doesn't have all of the things we'd like to see in an open source project, but I can exercise the full freedoms.

226
00:33:16,000 --> 00:33:27,000
And if I have an AI system, which is derived from, and I described like, you know, the kind of toxic dump of data, things like common crawl,

227
00:33:27,000 --> 00:33:32,000
at least I can study it and I can modify it.

228
00:33:32,000 --> 00:33:38,000
And, you know, it's hosted on Amazon. There's a 501(c)(3) the Common Core Foundation that wraps around it.

229
00:33:38,000 --> 00:33:45,000
So you can build a, you know, proposed open source AI on that.

230
00:33:45,000 --> 00:33:49,000
And that's data that exists today and the systems built on it that exist today.

231
00:33:49,000 --> 00:33:53,000
Now, we may be setting too high a bar by saying it's got to be Creative Commons licensed.

232
00:33:53,000 --> 00:34:01,000
And there's a whole lot of reasons about, you know, not just copyrights, but personal rights and other things that prevent you and GDPR and the rest of it.

233
00:34:01,000 --> 00:34:04,000
And they handle the complaints about GDPR. They deal with all of that.

234
00:34:04,000 --> 00:34:08,000
So I think that maybe we are being a bit too purist.

235
00:34:08,000 --> 00:34:20,000
And by being too purist and saying it's got to be CC0 or CC5 or whatever, right, rather than saying, like, it's got to be accessible so that I can enjoy the full freedoms.

236
00:34:20,000 --> 00:34:24,000
That would be already a good start.

237
00:34:24,000 --> 00:34:33,000
If we don't, right, if we don't say we want to see the data, right, we want things like that, the lung cancer data set to be available, then it'll never be.

238
00:34:33,000 --> 00:34:35,000
No one will ever create a data set.

239
00:34:35,000 --> 00:34:37,000
I mean, I've just deleted my data from 23andMe.

240
00:34:37,000 --> 00:34:39,000
It's been sitting there for 20 years or something.

241
00:34:39,000 --> 00:34:46,000
But the reason I put it in was to help advance medical technology, right, and they're good to go private and whatever.

242
00:34:46,000 --> 00:34:47,000
So I deleted it.

243
00:34:47,000 --> 00:34:59,000
But if there was an open source DNA database, you know, I would probably, a lot of people in that community would contribute to it with a view to finding open source drugs and solutions and things.

244
00:34:59,000 --> 00:35:04,000
But if there's no incentive to do that, it's not going to happen.

245
00:35:04,000 --> 00:35:05,000
Okay. No, I see.

246
00:35:05,000 --> 00:35:07,000
I heard that argument also.

247
00:35:07,000 --> 00:35:20,000
And I tend to push back in because I do believe that the open data issue and the incentives to open data are separate and they can run in parallel.

248
00:35:20,000 --> 00:35:29,000
I don't think that there is an inherent capability of the open source definition to say to force behavior.

249
00:35:29,000 --> 00:35:33,000
It has never happened before.

250
00:35:33,000 --> 00:35:45,000
You know, Copyleft has restored what was the Copyleft and Free Software principles have restored what was available before to the research community.

251
00:35:45,000 --> 00:35:51,000
Research community was used to distribute software freely without thinking about copyright.

252
00:35:51,000 --> 00:35:55,000
And then copyright was applied all of a sudden and that created problems.

253
00:35:55,000 --> 00:36:01,000
So it's a free software movement started as a reaction to restore what was happening before.

254
00:36:01,000 --> 00:36:12,000
The open data movement needs, there are so many changes in that, so many challenges in that space that it deserves a separate conversation.

255
00:36:12,000 --> 00:36:28,000
But going back to the common crawl being the tool that is out there, one conversation I've had with Professor Liang and other experts, builders of AI, they told me one interesting thing.

256
00:36:28,000 --> 00:36:38,000
And the common crawl is everything, right? It's the whole web, more or less, we can assume.

257
00:36:38,000 --> 00:36:46,000
And the interesting part, though, is not knowing that common crawl has been used, but having access to the data processing code.

258
00:36:46,000 --> 00:36:56,000
In other words, how common crawl has been filtered, what decisions have been made to split it up into pieces, the duplicated.

259
00:36:56,000 --> 00:37:08,000
And then the other important piece of reproducing or having the freedom to fork meaningfully is to know how the training has been done.

260
00:37:08,000 --> 00:37:21,000
One other theory that I've heard about the importance of data set that is the actual data set that is relatively less important than the code used for producing it,

261
00:37:21,000 --> 00:37:34,000
is that the tendency is to accumulate more and more data. And we may get to the point where there is going to be only one giant pile of data anyway.

262
00:37:34,000 --> 00:37:53,000
But the learning moment and the innovation cycles and the collaboration seems to be most likely to happen on the code front, on the data processing and data training.

263
00:37:53,000 --> 00:38:01,000
But that's for the future. I just want to plant it there to say one thing.

264
00:38:01,000 --> 00:38:11,000
But the other thing that is today that is more relevant to your argument is that common crawl itself is not guaranteed.

265
00:38:11,000 --> 00:38:17,000
You don't have any guarantee that you can distribute it safely around the world.

266
00:38:17,000 --> 00:38:35,000
And in fact, if you look at the history of the pile, and the history of Dolma, the two data sets used by Luther AI and the Allen Institute for AI,

267
00:38:35,000 --> 00:38:43,000
both of them, they're making decisions to distribute data that for which they don't have any rights to redistribute.

268
00:38:43,000 --> 00:38:47,000
And they're also changing the conditions for their distributions. Right?

269
00:38:47,000 --> 00:39:00,000
You were saying common crawl, common zeros, 511c3. There is no guarantee that common crawl can be distributed the way it is legally, safely around the world.

270
00:39:00,000 --> 00:39:05,000
That's what we're trying to dance around. Data is a different space.

271
00:39:05,000 --> 00:39:20,000
We got really lucky that software, there was a decision, a policy decision by IBM and Apple to apply copyright to software and not other different, different exclusive rights.

272
00:39:20,000 --> 00:39:30,000
Because copyright is more or less, uniformly more or less uniformly applied worldwide.

273
00:39:30,000 --> 00:39:38,000
But I'll give you one last example, just to see how complex and how difficult this conversation is.

274
00:39:38,000 --> 00:39:42,000
We say, let's train only on public domain data.

275
00:39:42,000 --> 00:39:46,000
That concept of public domain is different around the world.

276
00:39:46,000 --> 00:39:50,000
Oh, let's talk in only our first, you know, let's, let's use only fair use.

277
00:39:50,000 --> 00:40:01,000
Let's apply the fair use doctrine. That doesn't exist outside of the United States or, and I don't know if there is, I don't know what the situation is in Australia.

278
00:40:01,000 --> 00:40:07,000
It definitely the interpretation of the courts will be different for what fair use is.

279
00:40:07,000 --> 00:40:11,000
So we can't really rely on what we relied on for software.

280
00:40:11,000 --> 00:40:20,000
We just have to change our minds. And this is one of the reasons why we're having a separate conversation and a separate workshop on data.

281
00:40:20,000 --> 00:40:26,000
This coming month to just learn a little bit more about the space.

282
00:40:26,000 --> 00:40:33,000
We're working with Open Future to produce a white paper on this, on this space.

283
00:40:33,000 --> 00:40:39,000
That will serve as a continuation, you know, as a starting point to continue the conversation next year.

284
00:40:39,000 --> 00:40:45,000
Because open data has been around for 15 years and as a movement and all.

285
00:40:45,000 --> 00:40:50,000
And now they're thinking, you know, my sense is that they've been thinking a little bit by surprise.

286
00:40:50,000 --> 00:41:03,000
The because open data has been publishing, you know, release your data, your user data and not much about making it fun, realizing that it was, it would be functional.

287
00:41:03,000 --> 00:41:11,000
And that could be used for to transform the behavior of systems.

288
00:41:11,000 --> 00:41:18,000
Yeah. Look, I mean, I think that the pragmatically the data is only required a training time.

289
00:41:18,000 --> 00:41:21,000
The inference time is 99% of the use cases at inference time.

290
00:41:21,000 --> 00:41:31,000
But but as long as you've got access to it one way or another to be able to, whether that means having a team member in the US or running it in the US or whatever it is to to resolve that.

291
00:41:31,000 --> 00:41:35,000
And then you model you can you can you can run it wherever you want.

292
00:41:35,000 --> 00:41:42,000
My concern is that we have this definition here that that is like, like I said, not like we can't apply it.

293
00:41:42,000 --> 00:41:47,000
Right. And we don't have the manpower to apply this to everybody who wants it.

294
00:41:47,000 --> 00:41:54,000
So you rely on people self applying it, which is like sufficiently detailed, sufficiently skilled.

295
00:41:54,000 --> 00:42:03,000
Anybody can apply it to anything. Right. And so there's not really any way for us to stop that, particularly without trademarks, unless you have a certification trademark and so on.

296
00:42:03,000 --> 00:42:10,000
Right. So I feel like, you know, that we're kind of going to learn that the hard way. Right.

297
00:42:10,000 --> 00:42:15,000
And then to Tom Cataway's point, right, we can't once the cat's out of the bag, you can't bring it back in again.

298
00:42:15,000 --> 00:42:19,000
You can you can start with a high bar and lower the bar.

299
00:42:19,000 --> 00:42:26,000
But if you go out with a low bar, you can't raise it later because the standard is already effectively being said.

300
00:42:26,000 --> 00:42:39,000
I think we need to measure twice and cut once here. But I feel like, you know, no matter no amount of consensus finding or discussion at this point or based on the history, really, at any point.

301
00:42:39,000 --> 00:42:43,000
I mean, we started this conversation, you and I back in March or something.

302
00:42:43,000 --> 00:42:48,000
And I see that it's been running, you know, the whole the whole year. It hasn't really got got anywhere.

303
00:42:48,000 --> 00:43:04,000
But maybe maybe the thing here is we look at those exceptions you mentioned, like the one you've got on the screen now, the library of what used to be the lesser public, lesser GPO and kind of I'm not super like I'm very impressed with the level of detail.

304
00:43:04,000 --> 00:43:13,000
So the cost has got there, but it goes to the intent. Like, you know, when you go and have a dual intent visa, right, what's your intent? You can't really test the intent, not visible.

305
00:43:13,000 --> 00:43:19,000
So it really needs to be a thing. Is the data accessible? And if it's accessible, I can exercise the full freedoms.

306
00:43:19,000 --> 00:43:25,000
I think we agree on that. What? Okay. In that case, it's not like how sorry.

307
00:43:25,000 --> 00:43:35,000
Well, OK, how do you cover the case then for federated learning? The the the the definition tries to be generic.

308
00:43:35,000 --> 00:43:40,000
And I agree with you that there is an issue with certification like that. It needs to be solved.

309
00:43:40,000 --> 00:43:46,000
But let's put that aside. Let's put that aside. We will solve it. We need the principles now.

310
00:43:46,000 --> 00:43:58,000
We need to establish the principles and the principles. I fully agree with all of you. All the requirements for open data, like the building blocks are free code data and weights.

311
00:43:58,000 --> 00:44:03,000
All three of them needs to be made available. It's such a no brainer.

312
00:44:03,000 --> 00:44:09,000
The problem here is to cover for the different cases of the different kinds of data.

313
00:44:09,000 --> 00:44:18,000
That's what we need to we to give a story. We need to have a story for. And the story can only be solved with with this approach.

314
00:44:18,000 --> 00:44:22,000
We can only be solved with this approach. We haven't seen another approach.

315
00:44:22,000 --> 00:44:27,000
We haven't seen another proposal that covers all of these different types of data.

316
00:44:27,000 --> 00:44:38,000
We can go to the politicians. We can go to policymakers and say, look, the the the the right to fork meaningfully for an AI system is

317
00:44:38,000 --> 00:44:46,000
if I have access, if we have access to complete and detailed listing of all these kinds of data with links,

318
00:44:46,000 --> 00:44:54,000
you know, the URLs, the torrent hashes and all of that to download the publicly available, obtainable and open training data.

319
00:44:54,000 --> 00:45:01,000
And in case of nonpublic, very, very detailed description like sample data sets,

320
00:45:01,000 --> 00:45:12,000
you know, sample so that we can build a data set with our own private data, like a hospital can say I can recreate the structure of that data set with my own data.

321
00:45:12,000 --> 00:45:18,000
And the training code and the data processing code.

322
00:45:18,000 --> 00:45:26,000
And the model, right. But I want to have access to I need to be able to the very basic principle that the politician needs to be here.

323
00:45:26,000 --> 00:45:30,000
They want to be here. A very simple thing. And the simple thing could be.

324
00:45:30,000 --> 00:45:37,000
We need to have training code, training, processing, processing data.

325
00:45:37,000 --> 00:45:45,000
And all the instructions to take that and build on top of it, retraining.

326
00:45:45,000 --> 00:45:51,000
Without our own data or a different data.

327
00:45:51,000 --> 00:46:03,000
Is there a way we can address that kind of batteries included versus lesser or D minus or or whatever the.

328
00:46:03,000 --> 00:46:08,000
You know that the minus the block. I mean, that's the approach. I mean, that's what we're striving to get.

329
00:46:08,000 --> 00:46:13,000
That's what we're failing to communicate in this in this.

330
00:46:13,000 --> 00:46:23,000
It when someone says like you, you and others like for simplicity, you know, grouping all training data must be open.

331
00:46:23,000 --> 00:46:35,000
The the conversation stops because there. Yes, in theory, but in practice, there are four kinds of different data.

332
00:46:35,000 --> 00:46:45,000
So how do we account with this diversity is the proposed solution is what it's been in draft since 06 to 09.

333
00:46:45,000 --> 00:46:50,000
And it is to have that data information piece together with the code requirements.

334
00:46:50,000 --> 00:46:59,000
Those two pieces. Need to be made available. Right.

335
00:46:59,000 --> 00:47:09,000
But if I can't get the data itself, then I can't even validate that what you say is I can't even validate that data is actually the data that's gone into the model and it hasn't been supplemented.

336
00:47:09,000 --> 00:47:14,000
So that's that's the problem of reproducibility. Right. Is that what you're talking about?

337
00:47:14,000 --> 00:47:21,000
Reproducing the experience? What I'm saying is anybody can say anything. Anybody can say it's open source and there's nothing I can do to say.

338
00:47:21,000 --> 00:47:30,000
We can say that is also not right. But under this, with this, but there's different rules through any claims. Right.

339
00:47:30,000 --> 00:47:36,000
Yeah, but those are two different problems. The reproducibility is one thing and recognizes whether it is open source or not.

340
00:47:36,000 --> 00:47:42,000
It's a different thing. Reproducibility, even in software, is not a solved problem.

341
00:47:42,000 --> 00:47:55,000
For the most complex systems, you don't have a way to match bit to bit the binary to the source code.

342
00:47:55,000 --> 00:48:03,000
Yeah. And especially not when you're using like CUDA and stuff.

343
00:48:03,000 --> 00:48:11,000
And there's enough randomness in that you're going to get a different binary, but it's going to perform in a substantially similar way.

344
00:48:11,000 --> 00:48:19,000
And if you run the open source system, then you should have something that answers the same question the same way.

345
00:48:19,000 --> 00:48:26,000
You can study that thing. Right. It should be similar.

346
00:48:26,000 --> 00:48:34,000
So we enter into, you know, this is a different space than software. So even in software, we don't have reproducible builds.

347
00:48:34,000 --> 00:48:50,000
Or they're not an unsolved problem. Reproducing and rebuilding from scratch a system to make sure that it behaves completely the same is still an unsolved problem and may never be solved for these machine learning, deep learning systems.

348
00:48:50,000 --> 00:48:59,000
So aiming for that, it's a mistake. That's the open science level, which has always been a separate thing from open source.

349
00:48:59,000 --> 00:49:06,000
Open source is a building block, if you want, for open science. But it's not the same thing.

350
00:49:06,000 --> 00:49:27,000
Here, we're trying to, with open source AI, the open source AI definition has different aims to enable transparency, to enable transportedness, to enable open science, to enable safe and lower risks for society, or not to be an obstacle, rather.

351
00:49:27,000 --> 00:49:41,000
But these are not the goals. The goal here is to preserve the right to fork, if you want to put it very quickly.

352
00:49:41,000 --> 00:49:48,000
Can I fork it? Can I build on top of it? What do I need to do that?

353
00:49:48,000 --> 00:50:00,000
Can I reproduce it? Sure. I should not be stopped. I mean, it's a level on top.

354
00:50:00,000 --> 00:50:13,000
Yeah, it's a bit the open source versus open weights and discussion. In one case, you're getting the weights, but you can fine tune, but you can't fully exercise the freedoms.

355
00:50:13,000 --> 00:50:23,000
But if you have source in the context of preferred form, as in the original training data.

356
00:50:23,000 --> 00:50:37,000
So what are your thoughts then on how we got to this decision and the process? And I guess there was voting done and there was a result that was interpreted, I argue misinterpreted.

357
00:50:37,000 --> 00:50:45,000
Had that decision have come out a different way, had that have been interpreted differently, had those negative votes not gone in, how would that have been handled?

358
00:50:45,000 --> 00:50:57,000
Yeah, no, no, that's a good point. I think Zak raised that issue in the early days as soon as the, Stefano Zacchiroli, as soon as they came out.

359
00:50:57,000 --> 00:51:05,000
But that was so that those results were never meant neither to be scientific, nor representative, nor democratic or anything like that.

360
00:51:05,000 --> 00:51:18,000
There were, we had, we looked at them and we know this display and we knew that requiring trained data at the time, we didn't have this analysis of this four different kinds of data.

361
00:51:18,000 --> 00:51:32,000
We knew that there were cases where data was not distributable. So strictly requiring open data or the full training data set was going to be a limiting factor.

362
00:51:32,000 --> 00:51:45,000
So we tested, we decided to test an hypothesis. It was really like a political decision, if you want, which Stefano Zacchiroli also recently reiterated.

363
00:51:45,000 --> 00:52:00,000
Let's test it. Let's see what happens if we say no training data, but we replace training data with the detailed information about it and the training code and the data processing code.

364
00:52:00,000 --> 00:52:11,000
Let's see what happens and let's test that hypothesis. And that's where we saw the path forward into two following phases, the validation phase.

365
00:52:11,000 --> 00:52:28,000
And with the conversations with the partners, like with the, there is a vocal minority, but I guarantee you there is a much larger supporting group that is not visible on the forums because they're overwhelmed.

366
00:52:28,000 --> 00:52:36,000
And because they are AI builders, they tend not to have a lot of time, the lawyers, etc. professors.

367
00:52:36,000 --> 00:52:51,000
You know, it signaled that experiments gave a signal of a possible solution to the conundrum.

368
00:52:51,000 --> 00:53:00,000
Grant, you know, keeping in mind, we're not AI experts. We were looking at what the AI experts in those working groups were saying.

369
00:53:00,000 --> 00:53:06,000
And we noticed that there was not such an overwhelming majority of requiring training data.

370
00:53:06,000 --> 00:53:10,000
So we test and we knew that requiring it was going to be a problem.

371
00:53:10,000 --> 00:53:22,000
So we tested the hypothesis of what if instead we require information, detailed information and the training code.

372
00:53:22,000 --> 00:53:36,000
Has anybody actually done, like actually done the kind of proven that the freedoms are are exercisable or it's just been a thought.

373
00:53:36,000 --> 00:53:39,000
In what sense? Rebuilt from scratch?

374
00:53:39,000 --> 00:53:50,000
Like a practitioner, somebody gone and actually modified one of these systems or studied one of these systems that without.

375
00:53:50,000 --> 00:53:56,000
Maybe that's a question I can ask you. Like, why are you using Alpaca for your for your system instead of PTI?

376
00:53:56,000 --> 00:54:00,000
PTI is or Olmo.

377
00:54:00,000 --> 00:54:11,000
So using. I'm choosing I was looking at your demo, the personal assistant, which is exactly what I had in mind three years ago.

378
00:54:11,000 --> 00:54:21,000
Like, my God, we need to find a way for the next operating system to be using these systems and they need to be powerful and capable as much as charge.

379
00:54:21,000 --> 00:54:29,000
So how do we do that? And I was watching your demo and I think I saw that you were using Alpaca in it.

380
00:54:29,000 --> 00:54:32,000
Which is the llama.

381
00:54:32,000 --> 00:54:34,000
Jumbo llama beats Jumbo.

382
00:54:34,000 --> 00:54:37,000
So I'm working on the operating system.

383
00:54:37,000 --> 00:54:46,000
And the reason I'm taking a break from that to focus on this is because I would like to have the protection of a strong, meaningful open source AI definition.

384
00:54:46,000 --> 00:54:53,000
I'm working on the operating system and the demos that you're seeing are applications of the top of the operating system.

385
00:54:53,000 --> 00:55:05,000
So like a video chatbot who is standing in for a lecturer and answering questions about quantum physics or assignment deadline or whatever the thing is.

386
00:55:05,000 --> 00:55:09,000
I don't I give you the choice. You can run what you want. Right.

387
00:55:09,000 --> 00:55:16,000
And the reality is that there are these open source models of few and far between.

388
00:55:16,000 --> 00:55:19,000
And my intention would be that there will be more of them.

389
00:55:19,000 --> 00:55:29,000
But if we don't kind of set a bar and say this is the bar you need to meet to be an open source, be part of the open source, the Linux of personal AI, if you like.

390
00:55:29,000 --> 00:55:41,000
Well, all right. So I can tell you I can share I can share with you what the I can share with you that the systems that now comply in my mind for the definition.

391
00:55:41,000 --> 00:55:49,000
I mean, my I mean, based on the results of the analysis are basically four all made by nonprofit organization.

392
00:55:49,000 --> 00:56:04,000
So the Lutheran I Allen Institute for AI, TIA Falcon and LLM 360 are these are the four groups that are releasing all of their science.

393
00:56:04,000 --> 00:56:15,000
None of the others and companies especially are hard pushing back, not about the data they're pushing back on the data processing code and training code.

394
00:56:15,000 --> 00:56:30,000
They don't want to release that. Even the ones who are more generous, like IBM, more generous about their their disclosure of their training, training details and giving the weights and very permissive licenses, etc., etc.

395
00:56:30,000 --> 00:56:35,000
They are not releasing the training code and they're not releasing the data processing code.

396
00:56:35,000 --> 00:56:41,000
And when I mean my conversations also with VC funders, they always tell me the same thing.

397
00:56:41,000 --> 00:56:45,000
That's not going to happen. The companies don't want to release those.

398
00:56:45,000 --> 00:57:03,000
So. It's another sign in my mind that this is a good working solution, it covers it gives PTA, it gives the Lutheran AI, it gives it gives a lean Agora, it gives next cloud.

399
00:57:03,000 --> 00:57:13,000
You know, these are other open source smaller companies who want to release they're working on open source AI and they want to release more of that.

400
00:57:13,000 --> 00:57:26,000
So we want to give them space to create the tools that you can embed in your system and you can ship safely knowing what what we know.

401
00:57:26,000 --> 00:57:31,000
All right, we're getting at the top of the hour. I see Giacomo join, raise the hand.

402
00:57:31,000 --> 00:57:34,000
Let's go with this question and then I need to go.

403
00:57:34,000 --> 00:57:56,000
The question is simple. If the definition does not require the data, neither its distribution nor its availability, which may be a different thing.

404
00:57:56,000 --> 00:58:02,000
So we should remove from the prelude.

405
00:58:02,000 --> 00:58:24,000
It's the claim that the definition want to grant the right, the freedom to study and the freedom to modify and say that such definition will grant the freedom to fine tune the system because that's what is happening.

406
00:58:24,000 --> 00:58:46,000
Also, why I'm pretty sure that the definition that can be used to open wash any black box, we get a lot more traction among several detectors.

407
00:58:46,000 --> 00:59:08,000
It's going to also to have to the nature of the open source solution that may actually distribute or at least show how to obtain the training data.

408
00:59:08,000 --> 00:59:14,000
So we have.

409
00:59:14,000 --> 00:59:19,000
That is all we are here about.

410
00:59:19,000 --> 00:59:43,000
We are to grant with the such definition that are the freedom to use the freedom to distribute and the freedom to fine tune or we try to actually grant the freedom that the open source usually say to want to grant and require in a way or another.

411
00:59:44,000 --> 00:59:53,000
Now, I think you're bringing up, Giacomo, I can't hear you anymore now.

412
00:59:53,000 --> 01:00:08,000
But if I understand you correctly, you're calling for a litmus test, like some saying on the chat that the that litmus test doesn't exist in software either.

413
01:00:08,000 --> 01:00:13,000
So, you know, we may pretend it does, but it doesn't.

414
01:00:13,000 --> 01:00:31,000
I was talking to Sam before explaining the difference between how complicated it is to evaluate if I can really rebuild fully in a trustful, trustworthy way, any more complex program like the reproducible build issue.

415
01:00:31,000 --> 01:00:38,000
So, why is one issue is it or maybe it's a solution.

416
01:00:38,000 --> 01:00:49,000
The problem here is that in general, the definition does not grant the freedom that he pretends to in the preamble.

417
01:00:49,000 --> 01:01:08,000
We are fine. If it's a speech, we say, OK, we can't grant the right to study and we can't grant the right to modify and we grant that we can grant the right to fine tune our system.

418
01:01:08,000 --> 01:01:11,000
So we grant that everybody would happy.

419
01:01:11,000 --> 01:01:16,000
We'll be happy. In particular, Facebook.

420
01:01:16,000 --> 01:01:19,000
I see what your point is. I see what your point is.

421
01:01:19,000 --> 01:01:30,000
I see what your point is. I do think I mean, OK, you have an issue with the fact that it's it's unclear and it's incoherent in that sense.

422
01:01:30,000 --> 01:01:34,000
I some. All right.

423
01:01:34,000 --> 01:01:42,000
Well, I take it. I think that as a as a comment and unfortunately I need to go, but we can continue the conversation online.

424
01:01:42,000 --> 01:01:48,000
One one request for you to just be mindful of the time of others.

425
01:01:48,000 --> 01:01:55,000
If you can make smaller posts and less frequent, if you want, because you made your point.

426
01:01:55,000 --> 01:02:10,000
I think you've been heard. And and let's try to give space to four others also to participate into the conversation, because if we get if they get overwhelmed, it's going to be a dialogue between two or three people.

427
01:02:10,000 --> 01:02:15,000
All right. Thanks very much. Thank you very much.

428
01:02:15,000 --> 01:02:18,000
Gotta go. Bye bye.

429
01:02:18,000 --> 01:02:20,000
Bye.

### End of last town hall held on 2024-09-27 ###

