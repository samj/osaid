1
00:00:00,001 --> 00:00:07,000
All right, thanks everyone for joining this day in the slide access.

2
00:00:07,000 --> 00:00:12,720
I'm Stefano Maffulli, I'm the Executive Director of the Open Source Initiative.

3
00:00:12,720 --> 00:00:18,760
And a reminder, we've been doing this work thanks to a grant from Alfred Sloan Foundation

4
00:00:18,760 --> 00:00:25,000
and a donation from MercatoLibre, which has joined us as sponsors.

5
00:00:25,000 --> 00:00:32,000
Okay, so let's go through quickly a little bit of the history of how we got here

6
00:00:32,000 --> 00:00:37,000
and what we were doing and why we're doing this.

7
00:00:37,000 --> 00:00:43,000
So, why we're defining the Open Source AI now is because

8
00:00:43,000 --> 00:00:51,000
these technologies, since the introduction of chat GPTs or the Open AI launches,

9
00:00:51,000 --> 00:01:00,000
have significantly, significantly impacted how the IT environment in the software spaces

10
00:01:00,000 --> 00:01:08,000
has operated and worked and changed a lot of the fundamentals of our understanding of software

11
00:01:08,000 --> 00:01:11,000
and the Open Source definition.

12
00:01:11,000 --> 00:01:19,000
At the same time, we've seen an incredibly fast reaction from governments around the world,

13
00:01:19,000 --> 00:01:28,000
especially the United States, Europe, but also Japan, China, have started to write laws directly.

14
00:01:28,000 --> 00:01:35,000
And some of these laws, especially the AI Act is very explicit, mentions Open Source AI

15
00:01:35,000 --> 00:01:43,000
and Open Source systems and models without having a specific understanding of what that means.

16
00:01:43,000 --> 00:01:50,000
And the reason for the regulators to do that is to allow for special carve-outs for research and innovation.

17
00:01:50,000 --> 00:01:55,000
Like having seen how the Open Source definition worked for software

18
00:01:55,000 --> 00:02:00,000
and allowed for a thriving ecosystem that is worth apparently 8.8 trillion,

19
00:02:00,000 --> 00:02:09,000
or in any case, it's a huge sum of money, and it's underlying the whole digital infrastructure.

20
00:02:09,000 --> 00:02:15,000
So regulators want to keep everything safe, keep society safe, but at the same time,

21
00:02:15,000 --> 00:02:20,000
they want to make sure that the risks are reduced.

22
00:02:20,000 --> 00:02:28,000
And trying to find that balance revolves around the concept of Open Source AI.

23
00:02:28,000 --> 00:02:34,000
And third is this open washing thing that we've seen a lot of companies,

24
00:02:34,000 --> 00:02:41,000
and especially one abusing of the term Open Source and calling out meta in this space,

25
00:02:41,000 --> 00:02:50,000
for releasing models and releasing artifacts that are not Open Source by any stretch of imagination,

26
00:02:50,000 --> 00:02:55,000
especially their licensing in terms of conditions or not.

27
00:02:55,000 --> 00:03:03,000
But because there is a lack of clarity, they can claim whatever they want,

28
00:03:03,000 --> 00:03:14,000
because there is no easy way to push back and point at a widely agreed upon definition of what Open Source AI actually means.

29
00:03:14,000 --> 00:03:21,000
And so we asked them, I mean, this is just to show the Digital Public Goods Alliance

30
00:03:21,000 --> 00:03:33,000
have given a little, illustrated what they think the benefits of Open Source AI is in general.

31
00:03:33,000 --> 00:03:37,000
So without further ado, let me start from the history.

32
00:03:37,000 --> 00:03:45,000
What we've done is that in late 2021, when I joined the OSI,

33
00:03:45,000 --> 00:03:54,000
we knew it was immediately after Copilot was launched, and it was immediately clear that the word was not going to be the same with that.

34
00:03:54,000 --> 00:03:58,000
So we launched this research that we call Deep Dive AI.

35
00:03:58,000 --> 00:04:00,000
We wanted to understand the space.

36
00:04:00,000 --> 00:04:08,000
So in 2022 and 2023, we ran, if you want, research endeavor.

37
00:04:08,000 --> 00:04:11,000
We interviewed experts. We started to ask them questions.

38
00:04:11,000 --> 00:04:15,000
We published them in a form of deep podcast.

39
00:04:15,000 --> 00:04:20,000
And I recommend you to listen to it because it's really, if you're new to the space,

40
00:04:20,000 --> 00:04:28,000
it gives very good introduction to the legal aspect, the technical aspects, copyright, business and society security.

41
00:04:28,000 --> 00:04:36,000
Also, there's an interview with DARPA program manager for AI security, very eye opening.

42
00:04:36,000 --> 00:04:47,000
Next year, we've done, I mean, in the same year, we also run four panels with four experts covering business, society, legal and academic views.

43
00:04:47,000 --> 00:04:55,000
And we published their report. So if you don't want to listen to the podcast, you don't want to listen to the panel recordings, look at the report.

44
00:04:55,000 --> 00:05:04,000
It summarizes the issue. And together with the webinar series, with interviews and presentations from experts from different parts of the world,

45
00:05:04,000 --> 00:05:16,000
we knew at the beginning or in the middle of 2023, we knew that the issue that we needed to solve was that of the availability of training data.

46
00:05:16,000 --> 00:05:24,000
We knew that that was the problem that we needed to solve in some way, shape or form.

47
00:05:24,000 --> 00:05:26,000
Because it was the most controversial one.

48
00:05:26,000 --> 00:05:34,000
So we thought about using a co-design method to come up with a solution, a plausible solution.

49
00:05:34,000 --> 00:05:43,000
So we went around the world, we asked people questions and we also set up online meetings in order to have more variety.

50
00:05:43,000 --> 00:05:52,000
And we asked them a series of three questions. The first one, what you should be, what should the words or use the verbs,

51
00:05:52,000 --> 00:05:58,000
use, study, modify, share, mean for in the realm, in the domain of artificial intelligence.

52
00:05:58,000 --> 00:06:16,000
And after a few meetings in person and online, it was not too complicated to reproduce the basic principles of the free software definition and port them to the domain of AI.

53
00:06:16,000 --> 00:06:26,000
Then we needed to understand how to implement those freedoms. If you want to use, if you want to study an AI system, what do you need?

54
00:06:26,000 --> 00:06:32,000
So we collected virtual working groups online to analyze four types of systems.

55
00:06:32,000 --> 00:06:40,000
One is Lama, which is completely proprietary. The other one is Bloom, which is very open science, but with a bad license.

56
00:06:40,000 --> 00:06:53,000
PTA is a very open science LLM and OpenCV group analyzed the computer vision neural networks, zoo models, models in the zoo.

57
00:06:53,000 --> 00:07:02,000
So to, to, to find the analysis, to, to look at it from, from also the angle of something that is not an LLM.

58
00:07:02,000 --> 00:07:12,000
And we run some votes, not scientific. I, it was never the intention of being scientific. I've seen conversations on the forum.

59
00:07:12,000 --> 00:07:20,000
The idea was to gather a little bit of feedback that will allow us to move on and check an hypothesis.

60
00:07:20,000 --> 00:07:34,000
The hypothesis here was given the, that known, it showed that the groups were not unanimous in saying that the training data was always necessary to study and modify, especially for modify.

61
00:07:34,000 --> 00:07:39,000
For studying it, it was, that component was a little bit higher.

62
00:07:39,000 --> 00:07:49,000
But we have an hypothesis and the hypothesis to solve the problem of data was to see, okay, if we don't require it, what happens?

63
00:07:49,000 --> 00:08:01,000
So we asked the third questions. Let's go outside, look at the, look at the world, look at existing systems and check which ones would comply if we don't require strictly the training data.

64
00:08:01,000 --> 00:08:08,000
But we require the other components, which is the data processing code and the training code.

65
00:08:08,000 --> 00:08:19,000
So we, we threw, we widened the search to more models to, for this validation phase.

66
00:08:19,000 --> 00:08:35,000
And in fact, we ended up with the expected results. In other words, none of the systems that we knew were bad, like LLAMA or,

67
00:08:35,000 --> 00:08:41,000
or Bloom, for example, we knew that those ones would not pass for different reasons.

68
00:08:41,000 --> 00:08:47,000
So that was that signal. It was just a signal that there might be a way to solve the issue.

69
00:08:47,000 --> 00:08:52,000
And the result was this, right? We, the definition is structured this way.

70
00:08:52,000 --> 00:09:00,000
There is the four freedoms and then the requirements for the preferred form of making modifications to machine learning into these three pieces.

71
00:09:00,000 --> 00:09:10,000
You have to have the weights, the parameters need to be available with freedom respecting licenses and legal terms.

72
00:09:10,000 --> 00:09:18,000
Software needs to be complete for the training, the training and for the data processing.

73
00:09:18,000 --> 00:09:26,000
And then you need a very detailed description of what the data we use for training actually is.

74
00:09:26,000 --> 00:09:32,000
So Redmonk put it quite nicely and he called this the AI conundrums, right?

75
00:09:32,000 --> 00:09:39,000
And he quotes Giulio Ferraioli here. And this is a very interesting quote.

76
00:09:39,000 --> 00:09:48,000
So it says that if we assume that the definition requires full release of datasets, one thing is certain.

77
00:09:48,000 --> 00:09:53,000
It would be a definition for which very few existing systems qualify.

78
00:09:53,000 --> 00:09:59,000
And also those that qualify are less powerful and limited to specific domains.

79
00:09:59,000 --> 00:10:08,000
And then because then the other thing to solve, I mean, the other approach to think about this conundrum is that on the other hand,

80
00:10:08,000 --> 00:10:16,000
we have in the free software and open source movement a long history of making exceptions and finding ways to solve the problems

81
00:10:16,000 --> 00:10:27,000
in order to have more freedom and more open source software.

82
00:10:27,000 --> 00:10:40,000
Now, the other constraints that we have in this whole process is that the board, the OSI board, required three basic pillars, the elements.

83
00:10:40,000 --> 00:10:46,000
We want the definition to be supported by diverse stakeholders.

84
00:10:46,000 --> 00:10:50,000
That means not only end users, not only developers, but all of them.

85
00:10:50,000 --> 00:10:55,000
So end users, developers, deployers of AI, subjects of AI.

86
00:10:55,000 --> 00:11:06,000
And they want to have global representation of diverse interests in the list of endorsers and support.

87
00:11:06,000 --> 00:11:14,000
They also require, the board also requires that the definition must include relevant examples of AI systems.

88
00:11:14,000 --> 00:11:18,000
And this is for a specific tactical reason.

89
00:11:18,000 --> 00:11:32,000
The regulators will not consider the open source AI definition as an interesting one or something to follow and imitate if it is not a general one,

90
00:11:32,000 --> 00:11:39,000
if it doesn't cover general cases and if it doesn't cover and if it excludes clearly anything.

91
00:11:39,000 --> 00:11:44,000
I mean, if it really limited to only small domains.

92
00:11:44,000 --> 00:11:50,000
And it needs to be ready by October. And again, the main reason for this is timing.

93
00:11:50,000 --> 00:11:56,000
The AI Act is already out there and is already law.

94
00:11:56,000 --> 00:12:03,000
And it's already the legislation process, the normative process has already started.

95
00:12:03,000 --> 00:12:21,000
This means that there is a hyperactivity in Brussels, but also in D.C. and in California to convince and educate policymakers that open source just means access to open weights, access to the weights.

96
00:12:21,000 --> 00:12:39,000
And that's all they need to care about. And maybe a little bit of description of transparency in the form, if you're familiar with that data cards or other formats that are pretty lightweight to describe in metadata sense, the data set used for the training.

97
00:12:39,000 --> 00:12:49,000
And which is not sufficient. So there is an urgency to come up with a working definition.

98
00:12:49,000 --> 00:12:59,000
OK, so we're working on Release Candidate 1, which has some clarifications and basic concepts.

99
00:12:59,000 --> 00:13:04,000
This is just a screenshot. It's not really don't don't refer to it too much.

100
00:13:04,000 --> 00:13:08,000
It's going to change between now and when Release Candidate is released.

101
00:13:08,000 --> 00:13:17,000
But I wanted to show here the structure of the document to illustrate a major feature that has been here in its intention from the beginning.

102
00:13:17,000 --> 00:13:25,000
There are probably two main, two macro parts. The top part includes the basic concept, like what is a definition?

103
00:13:25,000 --> 00:13:31,000
What is it that we are defining? Which is it? What is AI? What is an AI system?

104
00:13:31,000 --> 00:13:45,000
And we added here, we may add based on feedback from the forum, the machine learning definition so that we have a list of framework reference of the topics that are covered below.

105
00:13:45,000 --> 00:13:57,000
Then there is the actual open source AI definition, what it is, right? The preamble, why we want it and what is open source AI, which includes the four freedoms.

106
00:13:57,000 --> 00:14:14,000
Now, the four freedoms, as you know, even in the free software world, the free software definition, the freedom to study and the freedom to modify are followed by a sentence that says precondition for this is access to the source code.

107
00:14:14,000 --> 00:14:28,000
And further, the source code is defined as the preferred form of making modification to the program, which is later clarified also in other documents, including the licenses, etc.

108
00:14:28,000 --> 00:14:38,000
Now, we need a frame here to understand what we're talking about, what we need, what is the preferred form of making modifications to an AI system.

109
00:14:38,000 --> 00:14:47,000
And in order to do that, we need to limit the scope. We cannot talk about all of the AI. We need to talk about machine learning in this specific case.

110
00:14:47,000 --> 00:14:56,000
It's the most pressing and urgent matter. Let's finish this part. So the second part, now we enter in the second macro part of the definition.

111
00:14:56,000 --> 00:15:04,000
This part is one that is most likely to change and evolve in future versions of the definition.

112
00:15:04,000 --> 00:15:19,000
So the preferred form of making modifications here lists the three major blocks of an AI system, a machine learning system, which are data piece, the code piece, and the parameters weights and sets conditions for those.

113
00:15:19,000 --> 00:15:38,000
And then we added, which is already in 0.9, space for clarifications. These clarifications serve the purpose of clarifying that whether you call, when you want to call and use the term open source,

114
00:15:38,000 --> 00:15:54,000
refer to anything that spits an output based on an input, whether you call it an open source AI, an open source AI system, open source model, open source weights, open source model or open source weights.

115
00:15:54,000 --> 00:16:05,000
Basically, these are all equivalent terms. You cannot skip the line to some extent and say, well, it's an open source model only.

116
00:16:05,000 --> 00:16:23,000
So, you know, the rest of the definition doesn't apply. No, if you want to call it open source model or open source weights, you have to comply with, we have to provide the preferred form of making modifications to that system.

117
00:16:23,000 --> 00:16:42,000
Now we're also working on a new FAQ and the new FAQ entry will clarify further the four types of training data that are plausible and possible in this world.

118
00:16:42,000 --> 00:16:52,000
And that is clearly, you know, open training data, public training data, obtainable training data and unshareable non-public training data.

119
00:16:52,000 --> 00:17:02,000
And we believe that we believe that all of these four types of data can be part of an open source AI.

120
00:17:02,000 --> 00:17:26,000
We don't want to limit the open source AI only to open training data or open and public training data, because that would limit the capabilities of open source systems and would limit the domain of open source systems.

121
00:17:26,000 --> 00:17:41,000
So open training data typically is something that, you know, you have, you can fairly assume that there is global coverage, global possibility to distribute that data.

122
00:17:41,000 --> 00:17:49,000
And that is a collection of facts like time series of wind speed and ocean temperatures.

123
00:17:49,000 --> 00:17:58,000
And it may seem safe, but I discovered that when trying to immigrate to China, going to China for a business trip,

124
00:17:58,000 --> 00:18:21,000
I noticed that among the reasons for Chinese government to search on entry and exit of the country to search digital devices is for the availability, for the presence of weather information, weather data on the devices.

125
00:18:21,000 --> 00:18:29,000
Meaning this means that even that kind of data is not necessarily easy to circulate around the world, like one can assume.

126
00:18:29,000 --> 00:18:39,000
Public training data is data that others can download, like, you know, the archive of my blog posts, for example.

127
00:18:39,000 --> 00:18:46,000
And this is also this is subject to availability, right? Links go stale and everything like that.

128
00:18:46,000 --> 00:18:56,000
But we can work around that with tools like common crawl or Internet archive and software heritage and places like that where they keep, they can keep the archive.

129
00:18:56,000 --> 00:19:07,000
But that's the public training data. The obtainable training data is data that you can buy or data that you can, you know, they can be obtained if you ask.

130
00:19:07,000 --> 00:19:22,000
Things like the ImageNet data set, for example, and the unshareable non-public training data is private information, that kind of like medical, medical, medical data.

131
00:19:22,000 --> 00:19:27,000
This kind of data can never be asked for, can never be distributed.

132
00:19:27,000 --> 00:19:34,000
And so we need to be careful, but we want to have open source AI medical systems.

133
00:19:34,000 --> 00:19:38,000
So we need to find a way to work around this, this piece.

134
00:19:38,000 --> 00:19:46,000
So I can give a very quick overview of where we're thinking that the release candidate might go to explain things a little bit better.

135
00:19:46,000 --> 00:19:56,000
So in the data information piece, we're trying to make it clear that the requirements are a must.

136
00:19:56,000 --> 00:20:01,000
Like the, we noticed the confusion between the work that we create.

137
00:20:01,000 --> 00:20:06,000
So we're probably going to use, suggesting is to use the word build instead.

138
00:20:06,000 --> 00:20:16,000
Substantially equivalent systems stays together with, but we are adding the fact that the data information needs to work with the code below.

139
00:20:16,000 --> 00:20:19,000
You cannot just say data information alone.

140
00:20:19,000 --> 00:20:23,000
You cannot look at data information alone and think that you can be replicating.

141
00:20:23,000 --> 00:20:38,000
And this is, if you saw online, there was a comment from Professor Perciviere, who highlighted how much the knowing about the data and knowing how it's been processed is the crucial, crucial piece.

142
00:20:38,000 --> 00:20:59,000
And specifies here in particular, if used, so for example, if one uses training data that is publicly available, it needs to be made available.

143
00:20:59,000 --> 00:21:10,000
And for the code specification, there is a clarification that the code must be complete.

144
00:21:10,000 --> 00:21:14,000
You cannot just say, hey, here's the code. No, I need to be able to run it.

145
00:21:14,000 --> 00:21:18,000
I need to be able to run it together with the data information.

146
00:21:18,000 --> 00:21:22,000
I need to know I need to get a working data set in some way or form.

147
00:21:22,000 --> 00:21:29,000
And there is also one line in addition that we're working on.

148
00:21:29,000 --> 00:21:33,000
It's a way to cover all three of these pieces.

149
00:21:33,000 --> 00:21:44,000
So code, data and the issue here is that code weights and data.

150
00:21:44,000 --> 00:21:48,000
The issue here is that could be contractual terms.

151
00:21:48,000 --> 00:22:00,000
And actually, there are like the responsible AI license family has in some of the they show that they played the experiment.

152
00:22:00,000 --> 00:22:08,000
And they have a way of in one contract to say, take this model, take this data and use it under these conditions.

153
00:22:08,000 --> 00:22:20,000
So what we're trying to say here is that we need to have a we need to give away to people like the groups at the OSI who will be reviewing these contracts.

154
00:22:20,000 --> 00:22:28,000
To make sure that we can evaluate the these packages that cover different artifacts,

155
00:22:28,000 --> 00:22:34,000
because if you notice the code individually, the elements code parameters and data,

156
00:22:34,000 --> 00:22:40,000
they have this provision that says code shall be made available on their OSI approved license.

157
00:22:40,000 --> 00:22:49,000
This means that, yes, individually, some elements are covered, you know, they can be covered by this license.

158
00:22:49,000 --> 00:22:53,000
But collectively, we don't have a way in this definition. So it was a missing piece.

159
00:22:53,000 --> 00:22:58,000
And that's what we're working on on this final line.

160
00:22:58,000 --> 00:23:10,000
All right. So these are the groups that we're working with to get endorsements and get different representation and timeline.

161
00:23:10,000 --> 00:23:25,000
We're still aiming to get version one ready for all things open in North Carolina in October, the end of the month.

162
00:23:25,000 --> 00:23:33,000
So for now, you know, we want to hear your voice. So endorse the concepts embedded in the draft.

163
00:23:33,000 --> 00:23:40,000
It's unlikely that they're going to be changing going forward. But the adaptations and clarifications, absolutely.

164
00:23:40,000 --> 00:23:46,000
They're coming because the intention is there.

165
00:23:46,000 --> 00:23:57,000
The intention is to make sure that we have a clear definition that is white, has white support and preserves the principles of open source.

166
00:23:57,000 --> 00:24:08,000
All right. And with that, I think we can go to open the floor for questions.

167
00:24:08,000 --> 00:24:23,000
If you have the mic, you can probably open it yourself or you can type.

168
00:24:23,000 --> 00:24:32,000
Yeah, so I am here. I've obviously shared a lot of my thoughts on on where we're at.

169
00:24:32,000 --> 00:24:41,000
My main concern here is that this is a needs to be a litmus test like the open source definition.

170
00:24:41,000 --> 00:24:50,000
And the open source definition has the benefit of being able to approve a license and then have that license self applied to millions of projects.

171
00:24:50,000 --> 00:24:55,000
Whereas in this case, we really need to look at every single project.

172
00:24:55,000 --> 00:25:02,000
This is getting applied to or somebody needs to look at it, whether it's us or more likely it's going to be the project itself.

173
00:25:02,000 --> 00:25:21,000
And so my my suggestion or my my belief is that this doesn't function as a litmus test and that words like sufficiently detailed and if I can tell you, well, I've got a skilled person here and they're going to say, well, it's not skilled enough.

174
00:25:21,000 --> 00:25:30,000
We've given you enough information. Right. So there's not there's not going to be a way for us to really tell anybody that there is not a source.

175
00:25:30,000 --> 00:25:37,000
I sort of going and trying to implement the thing ourselves based on the information that they've given us.

176
00:25:37,000 --> 00:25:43,000
So let's say that they do achieve the certification.

177
00:25:43,000 --> 00:26:03,000
They get that they get the mark. Does it protect the four freedoms? And I think it's quite clearly I don't think Steven's done a really good job of saying, get these are the kind of proofs that you would need to show like you need to show me that you can make the same modifications to a machine learning system with fine tuning.

178
00:26:03,000 --> 00:26:13,000
Right, we know access to the source data as what you can with access to the source data. I think, you know, as a practitioner, it's clear that that's not the case.

179
00:26:13,000 --> 00:26:18,000
I mean, I feel like we need something that people can self apply, which is like the OSD.

180
00:26:18,000 --> 00:26:35,000
And so it really needs to be something where you say this is the ingredients. This is the manifest. And maybe it's even an electronic manifest, like a JSON file or something that says these are the scripts, the training script, the training data, maybe optionally.

181
00:26:35,000 --> 00:26:42,000
And then with those checkboxes checked, then you get access to the mark, right?

182
00:26:42,000 --> 00:26:59,000
So yeah, I mean, I don't think unless somebody sat down and actually like, tried to size the four freedoms on systems that have been through this, like I maintain that it doesn't function as a litmus test.

183
00:26:59,000 --> 00:27:07,000
Yeah. All right. So it doesn't function as a litmus test. And that's what we tried in the validation phase.

184
00:27:07,000 --> 00:27:18,000
So we said, let's take the, at the time, the definition also had a component called the checklist, which I haven't mentioned.

185
00:27:18,000 --> 00:27:29,000
Basically, the checklist is the list of components of AI systems as described in the model openness framework, which is a paper.

186
00:27:29,000 --> 00:27:45,000
I don't know if you're familiar with it. It's a paper from the Linux Foundation. That paper lists 16, 18 components and classifies the availability of them is used to classify where they stand in three classes.

187
00:27:45,000 --> 00:27:54,000
Open class one is open model class. That's the only way around. Class three is open model class two.

188
00:27:54,000 --> 00:28:03,000
Open models, basically just the weights and model cards, some metadata, model card and data card, technical report, then open tooling.

189
00:28:03,000 --> 00:28:12,000
If they make available also data pre-processing and some other and some other inference code and some other pieces of code.

190
00:28:12,000 --> 00:28:21,000
And then a top class is open science where they provide every everything, every required component, including open training.

191
00:28:21,000 --> 00:28:27,000
I mean, the training code and the training and testing data sets.

192
00:28:27,000 --> 00:28:40,000
So one way that the validation phase work was to say which of these components are required, then one could go and use the model openness framework and see the availability of these marks.

193
00:28:40,000 --> 00:28:47,000
You put zero at this bar, which is between class two and class three and class one, between open tooling and open science.

194
00:28:47,000 --> 00:28:52,000
And that could be one way of having that litmus test.

195
00:28:52,000 --> 00:28:59,000
What we discovered is what you were saying. It requires the finding.

196
00:28:59,000 --> 00:29:08,000
This doesn't work at a glance the same way that open source software and open source license approved open source licenses work.

197
00:29:08,000 --> 00:29:21,000
You cannot look at the repository and see if there is a check the license file could check the open source initiative website and say, hap, this is open source approved or not.

198
00:29:21,000 --> 00:29:25,000
It's not because there are so many different components with different nature.

199
00:29:25,000 --> 00:29:31,000
Some documentation is on a blog post, some code is in GitHub, some or other repositories.

200
00:29:31,000 --> 00:29:35,000
Some of the models are on Agnephase or Kaggle.

201
00:29:35,000 --> 00:29:44,000
Finding all these pieces for a third party of a viewer is almost impossible or very, very complicated without the collaboration of the original developers.

202
00:29:44,000 --> 00:29:54,000
So this is an issue that we are aware of, but it's also an issue that we don't know how to solve today and it could be solved with practice tomorrow.

203
00:29:54,000 --> 00:30:07,000
In other words, there might be a space, a place in time where Agnephase maintains a community of experts or I don't know.

204
00:30:07,000 --> 00:30:27,000
There is a collaboration effort with universities and in fact we started talking with a few of them to see if they want to have, if they want to help us expand in next year the validation phase and fine tune the key indicators of what an open source AI needs.

205
00:30:27,000 --> 00:30:31,000
Because I have a theory in my mind, but it's just a silly theory.

206
00:30:31,000 --> 00:30:37,000
If you're not releasing the data processing code and if you're not releasing the training code, you're not open source.

207
00:30:37,000 --> 00:30:49,000
It's a very, in my mind so far with the samples that I've seen, it's an easy test, but it's not an easy one to solve.

208
00:30:49,000 --> 00:30:56,000
The nature of this animal of AI machine learning is that they are not software, simply.

209
00:30:56,000 --> 00:31:05,000
And even when software, there are examples where understanding if it's open source or not is complicated.

210
00:31:05,000 --> 00:31:12,000
And there are different ways of interpreting what open source means.

