1
00:00:00,001 --> 00:00:00,840
Good.

2
00:00:00,840 --> 00:00:05,160
Let's get started.

3
00:00:05,160 --> 00:00:09,440
Thanks everyone for joining this public town hall.

4
00:00:09,440 --> 00:00:13,800
I don't even remember the fifth in the series,

5
00:00:13,800 --> 00:00:16,880
but there's been a while we've tried to be transparent

6
00:00:16,880 --> 00:00:18,960
about the process and what's happening

7
00:00:18,960 --> 00:00:22,000
and give a very quick overview

8
00:00:22,000 --> 00:00:25,000
of what's been happening in the past few months

9
00:00:25,000 --> 00:00:29,960
and look at the latest draft

10
00:00:29,960 --> 00:00:34,960
and have a quick glance at what's coming next

11
00:00:34,960 --> 00:00:38,160
because we're really approaching the last months,

12
00:00:38,160 --> 00:00:42,320
actually the last weeks of the drafting process.

13
00:00:42,320 --> 00:00:46,280
We really are getting close to something that is workable.

14
00:00:46,280 --> 00:00:48,600
Let's start with our community agreements

15
00:00:48,600 --> 00:00:52,880
to remind everyone that we are welcoming a place,

16
00:00:52,880 --> 00:00:56,360
we're also trying to balance with moving forward

17
00:00:56,360 --> 00:01:00,040
and finding converging towards a solution,

18
00:01:00,040 --> 00:01:04,040
leaving the largest building blocking paths,

19
00:01:04,040 --> 00:01:10,520
elements for something that can be fixed in the future.

20
00:01:10,520 --> 00:01:15,720
And we are aiming quickly towards having a definition

21
00:01:15,720 --> 00:01:17,920
for open source AI by the end of the year.

22
00:01:17,920 --> 00:01:22,320
So where do we stand now?

23
00:01:23,320 --> 00:01:28,320
Remember, this is a reminder of what we've been working on.

24
00:01:28,320 --> 00:01:33,800
We have a preamble that sets the objective

25
00:01:33,800 --> 00:01:36,120
of why we're doing this, what are the objectives,

26
00:01:36,120 --> 00:01:41,120
what are the reasons why we want to have a clear definition

27
00:01:41,120 --> 00:01:44,640
of open source AI, why that's necessary.

28
00:01:44,640 --> 00:01:48,600
And we want to also clarify what we think needs to be fixed

29
00:01:48,600 --> 00:01:52,720
and we need to be addressed the issues that AI creates

30
00:01:52,720 --> 00:01:55,960
are to be resolved at a different level,

31
00:01:55,960 --> 00:01:58,600
at the government level, governance level,

32
00:01:58,600 --> 00:02:02,400
deployment levels, and not inside the documents

33
00:02:02,400 --> 00:02:07,400
that are used to distribute and make AI systems available.

34
00:02:07,400 --> 00:02:12,800
Then we have worked quite heavily to define the freedoms,

35
00:02:12,800 --> 00:02:16,760
to define exactly what is open source AI in practice,

36
00:02:16,760 --> 00:02:21,760
what are the identifying elements of when we see it,

37
00:02:21,760 --> 00:02:27,000
we will know that that system is an open source AI.

38
00:02:27,000 --> 00:02:31,640
This top part, before what we call the legal checklist,

39
00:02:31,640 --> 00:02:35,400
this top part is the most important piece

40
00:02:35,400 --> 00:02:37,840
and the most urgent piece that we need to get right

41
00:02:37,840 --> 00:02:39,800
and we need to get it right quickly.

42
00:02:39,800 --> 00:02:46,320
What follows below is the space,

43
00:02:46,320 --> 00:02:50,960
is what we are reviewing and revising at this stage.

44
00:02:50,960 --> 00:02:53,360
The top part looks fairly stable

45
00:02:53,360 --> 00:02:56,720
and I really urge everyone to look at it,

46
00:02:56,720 --> 00:02:59,520
to give it a very solid, deep read,

47
00:02:59,520 --> 00:03:05,440
to see, to highlight possible mistakes,

48
00:03:05,440 --> 00:03:09,160
possible areas of improvement,

49
00:03:09,160 --> 00:03:11,840
because we really need to put this to bed

50
00:03:11,840 --> 00:03:13,040
as quickly as possible.

51
00:03:13,160 --> 00:03:17,160
And since no one has been giving a lot of comments to this

52
00:03:17,160 --> 00:03:19,400
in the past couple of iterations,

53
00:03:19,400 --> 00:03:24,400
I'd love to tell everyone, to remind everyone

54
00:03:24,400 --> 00:03:29,720
that this is very important that we complete the process

55
00:03:29,720 --> 00:03:33,120
and this top part is really, really, really the top,

56
00:03:33,120 --> 00:03:34,360
the most important one.

57
00:03:34,360 --> 00:03:39,080
What comes below is the,

58
00:03:39,080 --> 00:03:41,720
what comes below the legal checklist is,

59
00:03:43,000 --> 00:03:45,840
basically, an operation manual,

60
00:03:45,840 --> 00:03:48,800
or at least recommendations and the initial recommendations

61
00:03:48,800 --> 00:03:52,360
for operators of the,

62
00:03:52,360 --> 00:03:56,560
for the people who would be reviewing AI systems

63
00:03:56,560 --> 00:04:00,880
and evaluate whether they satisfy the definition

64
00:04:00,880 --> 00:04:03,080
about what is open source AI.

65
00:04:03,080 --> 00:04:08,000
And because these are strongly targeting,

66
00:04:08,000 --> 00:04:10,280
this checklist is strongly targeting

67
00:04:10,280 --> 00:04:11,480
machine learning systems,

68
00:04:11,480 --> 00:04:15,560
because those are the systems that introduce new elements

69
00:04:15,560 --> 00:04:19,280
and pose more challenges at the moment.

70
00:04:19,280 --> 00:04:21,160
In the future, it may be different,

71
00:04:21,160 --> 00:04:24,480
but at the moment, machine learning is what is creating

72
00:04:24,480 --> 00:04:29,480
this uncertainties around what is open source,

73
00:04:29,480 --> 00:04:34,600
rather than other systems.

74
00:04:34,600 --> 00:04:39,600
And the main reason is because the AI systems require data

75
00:04:40,600 --> 00:04:44,160
and they produce, machine learning systems require data

76
00:04:44,160 --> 00:04:46,080
and they produce model weights.

77
00:04:46,080 --> 00:04:51,080
And these two elements are fairly new in the software world.

78
00:04:51,080 --> 00:04:55,000
And we haven't really been thinking about how they interact

79
00:04:55,000 --> 00:04:56,560
with the open source definition.

80
00:04:56,560 --> 00:05:00,320
This whole exercise of finding the open source AI definition

81
00:05:00,320 --> 00:05:02,920
revolves around these disruption, if you want,

82
00:05:02,920 --> 00:05:07,400
this disturbance in the understanding

83
00:05:07,400 --> 00:05:08,960
of what open source means.

84
00:05:09,840 --> 00:05:12,160
And that's what we are analyzing.

85
00:05:12,160 --> 00:05:17,160
This checklist is the piece that we are using

86
00:05:17,160 --> 00:05:21,600
to evaluate whether a system is open source or not,

87
00:05:21,600 --> 00:05:25,960
the response to the open source definition for AI or not.

88
00:05:25,960 --> 00:05:29,920
So what we've been doing, we started in the fall

89
00:05:29,920 --> 00:05:34,920
and we found these principles with co-design workshops

90
00:05:34,920 --> 00:05:37,400
that we run in multiple parts of the world,

91
00:05:37,400 --> 00:05:40,640
like in October in Raleigh, North Carolina,

92
00:05:40,640 --> 00:05:41,960
then we went to Monterey.

93
00:05:41,960 --> 00:05:47,320
We went to Alisabeba with the United Nations

94
00:05:47,320 --> 00:05:50,200
Digital Public Goods Alliance to run another workshop.

95
00:05:50,200 --> 00:05:53,240
And the result of these conversations,

96
00:05:53,240 --> 00:05:55,600
also we had conversations online,

97
00:05:55,600 --> 00:05:59,520
came out, produced this Four Freedoms for AI,

98
00:05:59,520 --> 00:06:02,280
which is basically the four freedom for free software,

99
00:06:03,240 --> 00:06:08,240
adapted and reviewed and purposed for AI systems,

100
00:06:08,240 --> 00:06:12,240
thinking about the definition of AI system.

101
00:06:12,240 --> 00:06:18,320
Then we focused on understanding the required components

102
00:06:18,320 --> 00:06:24,360
that one needs to have for a machine learning system

103
00:06:24,360 --> 00:06:26,880
in order to exercise those freedoms.

104
00:06:30,520 --> 00:06:33,960
So what do you actually need in order to have access

105
00:06:33,960 --> 00:06:35,680
to these systems?

106
00:06:35,680 --> 00:06:40,680
So we had, again, more workshops in person in San Jose,

107
00:06:40,680 --> 00:06:46,920
and then we run virtual work groups online

108
00:06:46,920 --> 00:06:50,360
to analyze four specific systems,

109
00:06:50,360 --> 00:06:52,960
Lama2, Bloom, PTI, and OpenCV.

110
00:06:52,960 --> 00:06:56,480
And you will see a variety of elements in here.

111
00:06:56,480 --> 00:06:59,800
You see Lama2, it's a commercial project

112
00:06:59,800 --> 00:07:04,280
and distributed with legal terms

113
00:07:04,280 --> 00:07:05,920
that include restrictions,

114
00:07:05,920 --> 00:07:08,680
and the Open Source Initiative has already called Lama2

115
00:07:08,680 --> 00:07:10,120
not an open source system.

116
00:07:10,120 --> 00:07:15,760
But it's relevant to have the analysis.

117
00:07:15,760 --> 00:07:18,160
We looked at Bloom because it's an open science project.

118
00:07:18,160 --> 00:07:22,760
It's very open in terms of it distributes

119
00:07:22,760 --> 00:07:24,080
a lot of its components,

120
00:07:24,080 --> 00:07:27,440
and it's very complete from the science perspective.

121
00:07:27,440 --> 00:07:30,320
Also, we know that it's been shared with our license

122
00:07:30,320 --> 00:07:34,920
and with legal terms that prevent some uses.

123
00:07:34,920 --> 00:07:36,840
PTI is another open science project,

124
00:07:36,840 --> 00:07:38,520
and it's very permissive.

125
00:07:38,520 --> 00:07:42,920
And OpenCV is similarly an open source project

126
00:07:42,920 --> 00:07:47,840
with lots of openness and lots of open components

127
00:07:47,840 --> 00:07:50,520
made available, but it's non-generative.

128
00:07:50,520 --> 00:07:53,720
So it's a computer vision, machine learning,

129
00:07:53,720 --> 00:07:54,760
machine learning system.

130
00:07:54,760 --> 00:07:57,720
So adding a little bit of variety.

131
00:07:57,720 --> 00:08:01,760
We asked for volunteers and we reached out to volunteers

132
00:08:01,760 --> 00:08:06,760
to have the most, the widest possible diversity

133
00:08:06,760 --> 00:08:10,840
in different levels and different stages.

134
00:08:10,840 --> 00:08:14,480
We invited experts from like the developers

135
00:08:14,480 --> 00:08:18,240
and technical experts of each of these groups.

136
00:08:18,240 --> 00:08:22,360
And we included others.

137
00:08:22,360 --> 00:08:26,480
Like we invited Davide Testugine and Jonathan Torres

138
00:08:26,480 --> 00:08:29,960
from Meta into the LAMA working group explicitly

139
00:08:29,960 --> 00:08:32,080
because they are experts.

140
00:08:32,080 --> 00:08:34,920
And if someone couldn't find a component,

141
00:08:34,920 --> 00:08:37,520
we could ask the experts to go look for them

142
00:08:37,520 --> 00:08:41,000
and to tell us whether those were available or not.

143
00:08:41,000 --> 00:08:44,280
And that's the reason why they're here together with,

144
00:08:44,280 --> 00:08:46,720
for example, Stella Biderman from PTI.

145
00:08:48,040 --> 00:08:53,040
And we worked also, we aligned our work.

146
00:08:53,040 --> 00:09:01,280
We started with the model openness framework

147
00:09:01,280 --> 00:09:05,440
that is produced by Matt White, Ibrahim, Adad

148
00:09:05,440 --> 00:09:09,760
and others from the Linux Foundation and other researchers

149
00:09:09,760 --> 00:09:12,320
who they have analyzed machine learning

150
00:09:12,320 --> 00:09:15,640
and they grouped the required, the components

151
00:09:15,640 --> 00:09:20,160
that go into producing, distributing a system.

152
00:09:20,160 --> 00:09:25,200
And we based on that list of components,

153
00:09:25,200 --> 00:09:28,480
we asked the volunteers in the working groups

154
00:09:28,480 --> 00:09:31,240
to go look and see which of these components

155
00:09:31,240 --> 00:09:33,520
you think is required to study,

156
00:09:33,520 --> 00:09:35,960
which one is required to use the system,

157
00:09:35,960 --> 00:09:38,280
which one is required to share and modify.

158
00:09:38,280 --> 00:09:40,920
And they voted for each of these components.

159
00:09:42,080 --> 00:09:47,080
Then we composed and weighted all those results,

160
00:09:47,080 --> 00:09:49,240
all those votes.

161
00:09:49,240 --> 00:09:53,240
And we came up with a recommendation summary

162
00:09:53,240 --> 00:09:59,480
of what is required and what is less necessary

163
00:09:59,480 --> 00:10:06,080
or gathered less votes from the working groups.

164
00:10:06,080 --> 00:10:09,640
And that has produced the latest,

165
00:10:11,120 --> 00:10:15,600
no, that has produced a list of components

166
00:10:15,600 --> 00:10:19,120
for which we needed to go look at the legal documents.

167
00:10:19,120 --> 00:10:22,720
So we had the required components,

168
00:10:22,720 --> 00:10:27,080
we grouped them into three main areas for code,

169
00:10:27,080 --> 00:10:30,840
for what we call model, and then for data

170
00:10:30,840 --> 00:10:35,400
because the votes didn't reach a very high threshold

171
00:10:35,400 --> 00:10:37,280
for the votes of the working groups

172
00:10:37,280 --> 00:10:39,680
didn't reach a very high threshold

173
00:10:39,680 --> 00:10:43,400
to require the original training dataset.

174
00:10:43,400 --> 00:10:46,520
And because of other practical considerations,

175
00:10:46,520 --> 00:10:51,000
we substituted the information,

176
00:10:51,000 --> 00:10:54,560
we built a proxy for the access,

177
00:10:54,560 --> 00:10:57,200
having access to the original training dataset.

178
00:10:57,200 --> 00:11:01,880
We replaced it with requirements for data transparency.

179
00:11:01,880 --> 00:11:03,840
Like we wanna have the maximum amount

180
00:11:03,840 --> 00:11:06,320
of transparency on the data.

181
00:11:06,320 --> 00:11:09,040
And we wanna have also the tools

182
00:11:09,040 --> 00:11:12,960
instead of the actual dataset,

183
00:11:12,960 --> 00:11:17,960
the tools to build compatible data datasets.

184
00:11:17,960 --> 00:11:19,640
So we want to have the code

185
00:11:19,640 --> 00:11:23,160
that went into building that dataset as an alternative.

186
00:11:23,160 --> 00:11:27,240
And now with that list of the required components,

187
00:11:27,240 --> 00:11:30,000
we went and looked at the legal frameworks

188
00:11:30,000 --> 00:11:31,400
for each of these components.

189
00:11:31,400 --> 00:11:35,800
And we ended up with having basically everything

190
00:11:35,800 --> 00:11:37,760
we discovered basically that everything,

191
00:11:37,760 --> 00:11:38,960
all the required components,

192
00:11:38,960 --> 00:11:42,000
they fall under copyright quite clearly

193
00:11:42,000 --> 00:11:44,040
except one of the components.

194
00:11:44,040 --> 00:11:46,400
So everything that is code,

195
00:11:46,400 --> 00:11:49,960
so in this slide, you can see everything

196
00:11:49,960 --> 00:11:54,000
that is in the pre-training, I mean, the code section,

197
00:11:54,000 --> 00:11:55,640
in the data transparency section,

198
00:11:55,640 --> 00:11:58,120
that's documentation basically.

199
00:11:58,120 --> 00:12:00,360
And in model architecture,

200
00:12:00,360 --> 00:12:03,800
those are all distributed as code

201
00:12:03,800 --> 00:12:05,320
that is written by a human,

202
00:12:05,320 --> 00:12:08,560
falls squarely under copyright

203
00:12:08,560 --> 00:12:12,360
and maybe patent law eventually.

204
00:12:12,360 --> 00:12:15,400
But it's something that we are very familiar with.

205
00:12:15,400 --> 00:12:17,400
It's an environment that we understand very well.

206
00:12:17,400 --> 00:12:21,000
It's an environment where we have, it's a legal framework.

207
00:12:21,000 --> 00:12:24,320
These are legal frameworks for which we have licenses

208
00:12:24,320 --> 00:12:27,720
and clear understanding of what those means.

209
00:12:27,720 --> 00:12:30,280
But for the model parameters,

210
00:12:30,280 --> 00:12:31,960
and these include, for example,

211
00:12:31,960 --> 00:12:35,080
the weights and the biases.

212
00:12:35,080 --> 00:12:40,080
For model parameters, we don't have a universal understanding

213
00:12:40,080 --> 00:12:50,280
around the world of what these fall under,

214
00:12:50,280 --> 00:12:52,360
what kind of laws they fall under.

215
00:12:52,360 --> 00:12:55,600
So we have to be a bit more careful evaluating

216
00:12:55,600 --> 00:13:00,760
what are the terms that we want to apply here

217
00:13:00,760 --> 00:13:03,640
and how we want to have,

218
00:13:03,640 --> 00:13:05,920
evaluate the legal terms

219
00:13:05,920 --> 00:13:08,200
under which model parameters are distributed.

220
00:13:08,200 --> 00:13:12,280
So I mentioned that we worked a lot

221
00:13:12,280 --> 00:13:17,280
to get global representation in this process,

222
00:13:17,280 --> 00:13:21,840
because ultimately we want,

223
00:13:21,840 --> 00:13:23,640
we have engaged with,

224
00:13:23,640 --> 00:13:26,160
we have identified these groups of stakeholders,

225
00:13:26,160 --> 00:13:28,880
like system creator, license creators, regulators,

226
00:13:28,880 --> 00:13:32,080
licensees, end users, and subject.

227
00:13:32,080 --> 00:13:35,440
And we have tried to engage with as many as possible.

228
00:13:35,440 --> 00:13:39,000
With the most involved ones in the current phase

229
00:13:39,000 --> 00:13:41,480
have been the licensees and system creators

230
00:13:41,480 --> 00:13:43,400
and license creators,

231
00:13:43,400 --> 00:13:48,400
so lawyers and integrators and developers.

232
00:13:48,400 --> 00:13:54,600
And we are expanding our reach now at this stage

233
00:13:55,440 --> 00:13:59,040
to end users, subjects, and regulators.

234
00:13:59,040 --> 00:14:00,640
And as a proxy for regulators,

235
00:14:00,640 --> 00:14:02,480
we're gonna be engaging with civil society

236
00:14:02,480 --> 00:14:05,720
who talks to regulators, lawmakers around the world.

237
00:14:05,720 --> 00:14:10,960
And so we have closed that phase,

238
00:14:10,960 --> 00:14:14,640
so we have now the next steps,

239
00:14:14,640 --> 00:14:17,200
what's happening in the next steps.

240
00:14:17,200 --> 00:14:21,600
We're getting ready to release the next draft, draft 08.

241
00:14:21,600 --> 00:14:24,680
I'm actually at a conference this week

242
00:14:24,680 --> 00:14:26,400
where I gathered a lot of feedback

243
00:14:26,400 --> 00:14:30,360
and may be able to release at 08 before May,

244
00:14:30,360 --> 00:14:35,360
and maybe go into Pittsburgh at the PyCon

245
00:14:35,360 --> 00:14:38,680
with an even higher version number,

246
00:14:38,680 --> 00:14:43,680
even more clearly towards a release candidate with a 0.1.

247
00:14:43,680 --> 00:14:47,280
So skipping that level

248
00:14:47,280 --> 00:14:50,560
and going into minor release numbering.

249
00:14:53,440 --> 00:14:55,440
And like a feature complete,

250
00:14:55,440 --> 00:15:00,440
but with close to be a release candidate in June.

251
00:15:00,440 --> 00:15:06,400
Between in June, we'll hold in-person or online.

252
00:15:06,400 --> 00:15:12,560
This is getting into a territory

253
00:15:12,560 --> 00:15:14,400
where we'd love to have it in person,

254
00:15:14,400 --> 00:15:16,760
but we also would love to have different people

255
00:15:16,760 --> 00:15:18,800
from different parts of the world.

256
00:15:18,800 --> 00:15:23,800
And so it's probably most likely gonna be online

257
00:15:23,800 --> 00:15:27,040
instead of in-person, but we'll see what happens.

258
00:15:27,040 --> 00:15:30,280
In any case, in June, we wanna have a group

259
00:15:30,280 --> 00:15:33,720
of important relevant stakeholders

260
00:15:33,720 --> 00:15:36,440
who have been involved into the drafting process

261
00:15:36,440 --> 00:15:41,440
to meet and release the release candidate number one.

262
00:15:41,440 --> 00:15:45,360
And during the summer months between July

263
00:15:45,360 --> 00:15:48,240
and the end of October,

264
00:15:48,240 --> 00:15:53,240
we have a plan to go through a worldwide roadshow

265
00:15:53,240 --> 00:15:58,960
to demonstrate, to illustrate, to advocate for,

266
00:15:58,960 --> 00:16:02,600
and gather in different parts of the world

267
00:16:02,600 --> 00:16:07,000
and gather more sustained to support

268
00:16:07,000 --> 00:16:10,160
and endorsement from different groups and organizations

269
00:16:10,160 --> 00:16:14,400
so that at the end of the October in Raleigh

270
00:16:14,400 --> 00:16:16,360
at All Things Open,

271
00:16:16,360 --> 00:16:21,360
the OSI board can review the draft with the comments

272
00:16:21,360 --> 00:16:26,440
and release the stable version at the end of the month.

273
00:16:26,440 --> 00:16:29,240
And this is our plan.

274
00:16:29,240 --> 00:16:32,600
I think we're getting very close to the finish line

275
00:16:32,600 --> 00:16:34,440
and it's really exciting.

276
00:16:34,440 --> 00:16:37,280
If you haven't been involved until now,

277
00:16:37,280 --> 00:16:40,720
I really, really, really, really encourage you

278
00:16:40,720 --> 00:16:43,040
to go to the online forums

279
00:16:43,040 --> 00:16:47,920
where we have healthy, deep conversations.

280
00:16:47,920 --> 00:16:53,320
Keep coming to these down-home meetings,

281
00:16:53,320 --> 00:16:55,760
ask questions, and stay engaged

282
00:16:55,760 --> 00:16:59,120
because we are making history right now.

283
00:16:59,120 --> 00:17:04,120
We are writing a definition that we hope will remove,

284
00:17:04,120 --> 00:17:08,760
that we want people to use to remove uncertainties

285
00:17:08,760 --> 00:17:12,520
and doubts from the market.

286
00:17:12,520 --> 00:17:15,400
People are releasing more models.

287
00:17:15,400 --> 00:17:17,360
They're using the open source moniker.

288
00:17:17,360 --> 00:17:19,560
They are confusing regulators

289
00:17:19,560 --> 00:17:21,440
and legislators around the world.

290
00:17:21,440 --> 00:17:24,560
And we need to provide certainties.

291
00:17:24,560 --> 00:17:27,080
We need to provide a stable view

292
00:17:27,080 --> 00:17:29,640
of what open source AI means

293
00:17:29,640 --> 00:17:32,320
so that regulation can come in

294
00:17:32,320 --> 00:17:39,160
and encourage innovation without causing harm,

295
00:17:39,160 --> 00:17:42,720
without spreading more disinformation

296
00:17:42,720 --> 00:17:46,600
and providing a positive environment.

297
00:17:46,600 --> 00:17:49,240
So with that, I'm gonna take a pause.

298
00:17:49,240 --> 00:17:54,000
And if you have any questions, I'm happy to respond.

299
00:17:54,000 --> 00:17:59,000
Any curiosities?

300
00:17:59,000 --> 00:18:21,160
- Yeah, I just have a question.

301
00:18:21,160 --> 00:18:24,160
So again, you may have touched based on it,

302
00:18:24,160 --> 00:18:26,000
but I was not sure it was clear.

303
00:18:26,000 --> 00:18:29,600
You do not touch at all on the data used for training

304
00:18:29,600 --> 00:18:33,160
except getting some form of transparency.

305
00:18:33,160 --> 00:18:34,520
Do I get it right?

306
00:18:34,520 --> 00:18:37,760
- Yes, that is correct.

307
00:18:37,760 --> 00:18:40,680
This is a, it's really,

308
00:18:40,680 --> 00:18:43,960
we started to work on a frequently asked question document

309
00:18:43,960 --> 00:18:46,680
because it's becoming recurrent now

310
00:18:46,680 --> 00:18:50,560
as more people are becoming aware of the process.

311
00:18:50,560 --> 00:18:53,360
The data issue has been,

312
00:18:53,360 --> 00:18:55,840
we have debated it at the very beginning

313
00:18:55,840 --> 00:18:58,000
and it's been really confusing.

314
00:18:58,000 --> 00:19:01,880
It's been really, it's been going around in circles.

315
00:19:01,880 --> 00:19:04,120
Like the very big issue with data

316
00:19:04,120 --> 00:19:09,120
is that if we require the original dataset

317
00:19:09,120 --> 00:19:12,480
to be distributed together with the model weights

318
00:19:12,480 --> 00:19:15,240
and parameters and the rest of the code,

319
00:19:15,240 --> 00:19:19,880
we will automatically exclude from the pool

320
00:19:19,880 --> 00:19:22,000
of possible AI systems.

321
00:19:22,000 --> 00:19:26,640
We will exclude systems that don't have data

322
00:19:26,640 --> 00:19:30,320
where the data is not available, right?

323
00:19:30,320 --> 00:19:32,240
Like federated learning,

324
00:19:32,240 --> 00:19:39,240
federated learning or privacy preserving,

325
00:19:39,240 --> 00:19:44,720
training mechanisms, all of those, for example,

326
00:19:44,720 --> 00:19:48,320
will not be part of the open source AI ecosystem

327
00:19:48,320 --> 00:19:50,680
because there is no data.

328
00:19:50,680 --> 00:19:55,200
The other reason is that many times

329
00:19:55,200 --> 00:19:59,840
you have the right to download, for example,

330
00:19:59,840 --> 00:20:02,920
information from a website

331
00:20:02,920 --> 00:20:06,040
in order to do data mining on it,

332
00:20:06,040 --> 00:20:09,520
but you don't have the right to redistribute it further.

333
00:20:09,520 --> 00:20:12,720
So also in these cases,

334
00:20:12,720 --> 00:20:14,680
you got a model parameters

335
00:20:14,680 --> 00:20:17,800
that you can definitely use.

336
00:20:17,800 --> 00:20:20,840
You have instructions to rebuild that dataset,

337
00:20:20,840 --> 00:20:23,760
but you cannot really have the original dataset

338
00:20:23,760 --> 00:20:25,680
because it's illegal to distribute it.

339
00:20:25,680 --> 00:20:30,840
And so to obviate for these issues,

340
00:20:30,840 --> 00:20:35,040
it's much easier and probably also more relevant

341
00:20:35,040 --> 00:20:38,400
to have access instead as a proxy to the tooling,

342
00:20:38,400 --> 00:20:41,320
to the filtering and to the instructions

343
00:20:41,320 --> 00:20:45,800
on how the dataset was built as a minimum.

344
00:20:45,800 --> 00:20:48,760
Again, these are default requirement.

345
00:20:48,760 --> 00:20:52,560
Nothing really prevents from something like

346
00:20:52,560 --> 00:20:57,920
Starcoder or other systems

347
00:20:57,920 --> 00:21:00,160
that have been built with open science in mind

348
00:21:00,160 --> 00:21:02,880
and have been very careful.

349
00:21:02,880 --> 00:21:07,120
Like Luther AI is working on the pile

350
00:21:07,120 --> 00:21:10,200
to a new dataset that is more,

351
00:21:10,200 --> 00:21:14,600
since they became more concerned

352
00:21:14,600 --> 00:21:18,840
about the copyright status of the input,

353
00:21:18,840 --> 00:21:21,320
the training datasets,

354
00:21:21,320 --> 00:21:22,960
they're rebuilding the pile,

355
00:21:22,960 --> 00:21:27,640
excluding all the possible sources of lawsuits

356
00:21:27,640 --> 00:21:30,880
and DMCA takedowns.

357
00:21:30,880 --> 00:21:35,080
So nothing excludes from building datasets

358
00:21:35,080 --> 00:21:36,920
that are relevant and important

359
00:21:36,920 --> 00:21:40,120
and can be distributed further,

360
00:21:40,120 --> 00:21:45,120
but that would make the open source AI,

361
00:21:45,120 --> 00:21:48,960
would put the, having that as a harder requirement,

362
00:21:48,960 --> 00:21:52,120
would put the open source AI at a disadvantage

363
00:21:52,120 --> 00:21:57,000
compared to the commercial proprietary systems

364
00:21:57,000 --> 00:21:59,800
where they basically don't even disclose

365
00:21:59,800 --> 00:22:03,720
the list of the data.

366
00:22:03,720 --> 00:22:04,560
Yeah.

367
00:22:04,560 --> 00:22:07,240
- That makes sense.

368
00:22:07,240 --> 00:22:08,080
Thanks a lot.

369
00:22:08,080 --> 00:22:10,120
So that's indeed very clear.

370
00:22:10,120 --> 00:22:10,960
- Absolutely.

371
00:22:10,960 --> 00:22:11,800
- Thank you.

372
00:22:11,800 --> 00:22:12,640
- Absolutely.

373
00:22:12,640 --> 00:22:16,160
All right.

374
00:22:16,160 --> 00:22:17,480
If there are no more questions,

375
00:22:17,480 --> 00:22:21,640
then I will close it here.

376
00:22:21,640 --> 00:22:25,200
And again, please go to the forums

377
00:22:25,200 --> 00:22:27,840
and there is a long thread about data

378
00:22:27,840 --> 00:22:30,880
where you can see the past conversation

379
00:22:30,880 --> 00:22:34,560
on this very hot topic.

380
00:22:34,560 --> 00:22:38,160
And yeah, we can continue there,

381
00:22:38,160 --> 00:22:42,520
or we can also move on to the other big issue,

382
00:22:42,520 --> 00:22:45,520
which is defining and understanding a little bit better,

383
00:22:45,520 --> 00:22:49,200
gathering more comments on the legal status

384
00:22:49,200 --> 00:22:54,200
of the model parameters and understand,

385
00:22:54,200 --> 00:22:57,600
get more suggestions on how we should treat them.

386
00:22:57,600 --> 00:23:02,960
Thanks everyone for joining and talk to you soon.

387
00:23:02,960 --> 00:23:05,080
In two weeks, we'll redo this.

388
00:23:05,080 --> 00:23:05,920
Bye.

