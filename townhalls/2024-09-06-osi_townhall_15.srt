1
00:00:00,001 --> 00:00:06,040
Let's start recording our session and get started.

2
00:00:06,040 --> 00:00:07,280
Welcome, everyone.

3
00:00:07,280 --> 00:00:09,960
This is our new format--

4
00:00:09,960 --> 00:00:11,840
well, slightly new format in the sense

5
00:00:11,840 --> 00:00:15,880
that we're going to be doing these town halls now weekly,

6
00:00:15,880 --> 00:00:20,200
going into the session, going into the final stretch

7
00:00:20,200 --> 00:00:24,640
before we have the open source AI definition reviewed

8
00:00:24,640 --> 00:00:29,520
and approved by the board at the end of October.

9
00:00:29,520 --> 00:00:33,560
At the board meeting that we have scheduled then.

10
00:00:33,560 --> 00:00:37,240
So a quick reminder of our rules of engagement,

11
00:00:37,240 --> 00:00:39,800
our community agreements that we call them.

12
00:00:39,800 --> 00:00:42,160
This is our rules.

13
00:00:42,160 --> 00:00:44,440
We want to have--

14
00:00:44,440 --> 00:00:46,800
making sure that there are no overlaps

15
00:00:46,800 --> 00:00:50,520
and people keep space for speaking and listening.

16
00:00:50,520 --> 00:00:55,320
So take space if you are shy and tend not to speak.

17
00:00:55,320 --> 00:00:59,000
But also, if you tend to speak a lot, take a break.

18
00:00:59,000 --> 00:01:03,080
Consider others and ask questions.

19
00:01:03,080 --> 00:01:08,040
Give space for others to come in and give their own things.

20
00:01:08,040 --> 00:01:09,160
Be kind.

21
00:01:09,160 --> 00:01:15,840
And the ones that I look always with in mind

22
00:01:15,840 --> 00:01:19,160
is the think about the fact that we want to continue moving.

23
00:01:19,160 --> 00:01:24,680
We can't stop discussing for too long.

24
00:01:24,680 --> 00:01:26,960
We need to notice the places where

25
00:01:26,960 --> 00:01:30,440
we have disagreement or difficult, complicated issues.

26
00:01:30,440 --> 00:01:36,000
Maybe put a note in there and get back to this later.

27
00:01:36,000 --> 00:01:40,040
And always look for solutions.

28
00:01:40,040 --> 00:01:43,440
Listing the problems is OK to start the conversation,

29
00:01:43,440 --> 00:01:45,920
but moving on.

30
00:01:45,920 --> 00:01:50,240
And so let's review for-- this is the first time

31
00:01:50,240 --> 00:01:52,680
we do this at this time of the week.

32
00:01:52,680 --> 00:01:55,920
So you may have not seen the new version that

33
00:01:55,920 --> 00:02:00,440
was released a couple of weeks ago, version 0.9.

34
00:02:00,440 --> 00:02:03,360
We have presented it in--

35
00:02:03,360 --> 00:02:09,880
I mean, we were in China with Mayor Joyce when this came out.

36
00:02:09,880 --> 00:02:15,600
And basically, the principles haven't changed between 0.8

37
00:02:15,600 --> 00:02:16,640
and 0.9.

38
00:02:16,640 --> 00:02:19,880
The principles are that we want to have-- for an open source

39
00:02:19,880 --> 00:02:23,320
AI, we want to have three kind of components

40
00:02:23,320 --> 00:02:27,160
that can be grouped into three buckets.

41
00:02:27,160 --> 00:02:30,880
The weights, the parameters, the architecture,

42
00:02:30,880 --> 00:02:33,920
anything that is related to what we

43
00:02:33,920 --> 00:02:35,920
call the model in machine learning

44
00:02:35,920 --> 00:02:38,160
needs to be made available under--

45
00:02:38,160 --> 00:02:39,280
needs to be made available.

46
00:02:39,280 --> 00:02:43,760
We'll talk about the conditions for openness or availability.

47
00:02:43,760 --> 00:02:47,280
The open-- the code used to train the system

48
00:02:47,280 --> 00:02:49,480
needs to be made available also.

49
00:02:49,480 --> 00:02:56,160
And also, you need to have a way to run the model,

50
00:02:56,160 --> 00:02:58,760
run the AI system.

51
00:02:58,760 --> 00:03:01,400
And then you need the data.

52
00:03:01,400 --> 00:03:04,040
Everyone understands and agrees on the fact

53
00:03:04,040 --> 00:03:10,280
that the data is where the parameters, the weights come

54
00:03:10,280 --> 00:03:11,960
from.

55
00:03:11,960 --> 00:03:14,880
And you need the data.

56
00:03:14,880 --> 00:03:18,040
And the coalition, the group that

57
00:03:18,040 --> 00:03:22,520
has discussed for many months how to solve the--

58
00:03:22,520 --> 00:03:25,920
how to pass the big boulder of data

59
00:03:25,920 --> 00:03:31,680
being many things in different legislations of the world

60
00:03:31,680 --> 00:03:36,600
came with the description of the data, the requirement

61
00:03:36,600 --> 00:03:39,840
the data has sufficiently detailed information

62
00:03:39,840 --> 00:03:42,240
about the data used to train the system.

63
00:03:42,240 --> 00:03:45,240
And that includes either the data set itself,

64
00:03:45,240 --> 00:03:53,760
when it's possible and legally plausible to distribute safely,

65
00:03:53,760 --> 00:03:56,360
or in alternative--

66
00:03:56,360 --> 00:03:59,960
and also-- not in alternative, but also--

67
00:03:59,960 --> 00:04:03,440
also the code with the full instructions

68
00:04:03,440 --> 00:04:05,640
on how to replicate that data set.

69
00:04:05,640 --> 00:04:11,440
And that includes the scripts to download the original data

70
00:04:11,440 --> 00:04:18,160
and content, the code to run all the interesting--

71
00:04:18,160 --> 00:04:21,720
all the interesting manipulation to go from the raw data

72
00:04:21,720 --> 00:04:25,200
to the training data set.

73
00:04:25,200 --> 00:04:34,400
And so the definition itself has the text

74
00:04:34,400 --> 00:04:39,120
of what is required in a paragraph titled,

75
00:04:39,120 --> 00:04:41,720
what is the preferred form to make modifications

76
00:04:41,720 --> 00:04:44,720
for a machine learning system?

77
00:04:44,720 --> 00:04:47,920
And describes the weights and gives some high level examples.

78
00:04:47,920 --> 00:04:52,800
These are not-- this is not text that is strictly correct,

79
00:04:52,800 --> 00:04:53,520
always--

80
00:04:53,520 --> 00:04:58,440
always the-- always valid for every possible--

81
00:04:58,440 --> 00:05:02,720
very strictly prescriptive about every possible system,

82
00:05:02,720 --> 00:05:05,520
every possible kind of technology

83
00:05:05,520 --> 00:05:08,720
that we can see today or available in the future.

84
00:05:08,720 --> 00:05:12,800
These are examples that are useful to interpret

85
00:05:12,800 --> 00:05:17,160
the actual meaning of the words above.

86
00:05:17,160 --> 00:05:21,480
So for the model weights parameters,

87
00:05:21,480 --> 00:05:24,080
that might include the checkpoints, for example,

88
00:05:24,080 --> 00:05:28,160
if the training in the system is a large language model,

89
00:05:28,160 --> 00:05:29,880
for example.

90
00:05:29,880 --> 00:05:33,080
From the code perspective, it's the source code

91
00:05:33,080 --> 00:05:35,280
used to train the system.

92
00:05:35,280 --> 00:05:40,800
And this includes all the pre-processing code,

93
00:05:40,800 --> 00:05:43,440
the training, the validation, the testing,

94
00:05:43,440 --> 00:05:46,480
how it's been done, the supporting libraries,

95
00:05:46,480 --> 00:05:50,760
and all of that code software.

96
00:05:50,760 --> 00:05:56,720
And this, obviously, need to be made available

97
00:05:56,720 --> 00:06:01,840
with open source AI approved licenses.

98
00:06:01,840 --> 00:06:03,320
And for the data--

99
00:06:03,320 --> 00:06:05,880
for the database, again, there is a-- this

100
00:06:05,880 --> 00:06:08,240
is a little bit more worthy.

101
00:06:08,240 --> 00:06:12,120
The intention here is that the open source AI,

102
00:06:12,120 --> 00:06:13,840
the developers of open source AI,

103
00:06:13,840 --> 00:06:18,680
must be sharing with others all the instructions

104
00:06:18,680 --> 00:06:22,880
and all the knowledge that they have on how they built.

105
00:06:22,880 --> 00:06:24,000
That's the intention.

106
00:06:24,000 --> 00:06:28,320
The intention is to make sure that open source

107
00:06:28,320 --> 00:06:36,280
AI carries the same meaning and the same practical values

108
00:06:36,280 --> 00:06:38,760
that the open source software definition carries.

109
00:06:38,760 --> 00:06:41,360
You need to be able to understand--

110
00:06:41,360 --> 00:06:42,840
you need to be able to understand

111
00:06:42,840 --> 00:06:45,800
how the system's been built, its intention, et cetera.

112
00:06:45,800 --> 00:06:49,320
And you need to be able to learn from them,

113
00:06:49,320 --> 00:06:51,880
from the original developers, and build on top of them

114
00:06:51,880 --> 00:06:55,720
without having to reinvent the wheel or try to guess.

115
00:06:55,720 --> 00:06:57,840
Maybe this thing has been done this way,

116
00:06:57,840 --> 00:07:01,840
so I can improve it by doing this other thing.

117
00:07:01,840 --> 00:07:03,280
No, it's building on top.

118
00:07:03,280 --> 00:07:05,720
And this is where the--

119
00:07:05,720 --> 00:07:08,840
building on top of what others have built,

120
00:07:08,840 --> 00:07:10,840
one of the basic tenets of open source.

121
00:07:10,840 --> 00:07:21,440
But we do have spent--

122
00:07:21,440 --> 00:07:25,720
I mean, the community, and together with the--

123
00:07:25,720 --> 00:07:26,800
also with the board.

124
00:07:26,800 --> 00:07:28,600
And we have reviewed a lot of the comments

125
00:07:28,600 --> 00:07:31,600
that we have received in the past.

126
00:07:31,600 --> 00:07:36,320
And we have clarified also that we do have--

127
00:07:36,320 --> 00:07:39,120
there is a-- we do have-- we do care about data.

128
00:07:39,120 --> 00:07:41,440
We do care about the availability of the training

129
00:07:41,440 --> 00:07:41,920
data.

130
00:07:41,920 --> 00:07:44,640
We do know that the training data

131
00:07:44,640 --> 00:07:49,440
is valuable to understand and study the AI systems.

132
00:07:49,440 --> 00:07:54,720
And we also want to acknowledge that there

133
00:07:54,720 --> 00:07:58,360
is different kind of data.

134
00:07:58,360 --> 00:08:01,920
There is data that computers share,

135
00:08:01,920 --> 00:08:04,000
and that would be--

136
00:08:04,000 --> 00:08:08,840
make sense to always have open in terms of openly accessible

137
00:08:08,840 --> 00:08:13,080
and as open data, following the definition of-- maintained

138
00:08:13,080 --> 00:08:15,520
by the Open Knowledge Foundation.

139
00:08:15,520 --> 00:08:18,800
But there is also training data that is simply public,

140
00:08:18,800 --> 00:08:21,040
cannot be redistributed.

141
00:08:21,040 --> 00:08:26,120
And that is basically the whole of the whole internet.

142
00:08:26,120 --> 00:08:34,400
And where you have the right to crawl and build indexes on it.

143
00:08:34,400 --> 00:08:37,360
This is one of the basic rights that Google

144
00:08:37,360 --> 00:08:42,120
has had as a search engine for forever, since its existence.

145
00:08:42,120 --> 00:08:44,960
And we need to continue to acknowledge the fact

146
00:08:44,960 --> 00:08:49,960
that while search engines can crawl the internet,

147
00:08:49,960 --> 00:08:52,520
they don't have the right to redistribute

148
00:08:52,520 --> 00:08:53,680
what they have crawled.

149
00:08:53,680 --> 00:08:57,080
But they have the right to offer the public

150
00:08:57,080 --> 00:09:01,240
some new and transformative activity, actions,

151
00:09:01,240 --> 00:09:03,480
and services.

152
00:09:03,480 --> 00:09:05,960
So this is the same thing.

153
00:09:05,960 --> 00:09:08,840
This is the concept of public training data

154
00:09:08,840 --> 00:09:10,800
that is publicly available.

155
00:09:10,800 --> 00:09:13,360
And then there is private data, which is another category

156
00:09:13,360 --> 00:09:17,880
of data for which you may have the right to train on,

157
00:09:17,880 --> 00:09:22,120
but you don't have the right to redistribute.

158
00:09:22,120 --> 00:09:26,840
And so a reminder that we are working

159
00:09:26,840 --> 00:09:34,800
within the constraints of the policies

160
00:09:34,800 --> 00:09:41,120
that the board has set as criteria to have a definition.

161
00:09:41,120 --> 00:09:42,520
The board wanted to have--

162
00:09:42,520 --> 00:09:45,040
would ask us to work with the community

163
00:09:45,040 --> 00:09:47,320
to understand, to have a definition that

164
00:09:47,320 --> 00:09:51,680
is supported by a large coalition of individuals,

165
00:09:51,680 --> 00:09:55,800
organizations, and groups with globally representative--

166
00:09:55,800 --> 00:09:57,320
so a sample that is representative

167
00:09:57,320 --> 00:10:00,320
of global communities, but also represent

168
00:10:00,320 --> 00:10:01,840
various different interests.

169
00:10:01,840 --> 00:10:05,640
We're not just representing the interest of research

170
00:10:05,640 --> 00:10:07,720
and academia of individual developers,

171
00:10:07,720 --> 00:10:12,560
or large corporations, small corporations,

172
00:10:12,560 --> 00:10:15,840
European corporations, or governments

173
00:10:15,840 --> 00:10:17,480
from other parts of the country.

174
00:10:17,480 --> 00:10:19,320
But it's the whole--

175
00:10:19,320 --> 00:10:25,040
we try to be as global and diverse as possible.

176
00:10:25,040 --> 00:10:28,480
The other constraints that the board set for us

177
00:10:28,480 --> 00:10:30,640
is that we need to provide real-life examples.

178
00:10:30,640 --> 00:10:33,320
You can't really have a definition

179
00:10:33,320 --> 00:10:39,440
that defines theoretical models, theoretical systems that

180
00:10:39,440 --> 00:10:43,560
don't have any application in practice.

181
00:10:43,560 --> 00:10:47,640
It would be not acceptable by the board.

182
00:10:47,640 --> 00:10:49,920
And we also wanted to set a deadline,

183
00:10:49,920 --> 00:10:56,240
because it's a hard deadline, because otherwise--

184
00:10:56,240 --> 00:10:59,720
because the world needs this definition soon.

185
00:10:59,720 --> 00:11:01,200
And it's better to have something

186
00:11:01,200 --> 00:11:07,680
that is done rather than perfect.

187
00:11:07,680 --> 00:11:08,560
So what's next?

188
00:11:08,560 --> 00:11:11,640
In the next few months, we are working

189
00:11:11,640 --> 00:11:15,280
to really solve the comments as they come,

190
00:11:15,280 --> 00:11:19,800
and maybe release in the next weeks, couple of weeks,

191
00:11:19,800 --> 00:11:22,240
release candidate version.

192
00:11:22,240 --> 00:11:29,760
And then get a quick feedback with the top organizations,

193
00:11:29,760 --> 00:11:33,960
groups that have worked in the process

194
00:11:33,960 --> 00:11:37,120
to gain their endorsements, gather the last comments,

195
00:11:37,120 --> 00:11:41,880
and march towards a stable version for all things

196
00:11:41,880 --> 00:11:46,000
open on October 27.

197
00:11:46,000 --> 00:11:49,760
So we are seeking now--

198
00:11:49,760 --> 00:11:52,760
we're at the stage where we are seeking the comments

199
00:11:52,760 --> 00:11:55,640
from individuals and organizational endorsements

200
00:11:55,640 --> 00:11:57,120
for--

201
00:11:57,120 --> 00:12:00,040
comments and endorsements for the draft.

202
00:12:00,040 --> 00:12:03,760
So if you have--

203
00:12:03,760 --> 00:12:05,840
if you're ready to say, yeah, we're

204
00:12:05,840 --> 00:12:08,000
ready to endorse these principles,

205
00:12:08,000 --> 00:12:10,440
just send us an email.

206
00:12:10,440 --> 00:12:13,520
And you can email me or you can email Claire.

207
00:12:13,520 --> 00:12:16,440
I've been assisting our project.

208
00:12:16,440 --> 00:12:17,760
You can get in touch with Nick.

209
00:12:17,760 --> 00:12:23,160
You can even go public on the forum already as you prefer.

210
00:12:23,160 --> 00:12:26,760
We want to have--

211
00:12:26,760 --> 00:12:28,080
we need to start moving.

212
00:12:28,080 --> 00:12:31,200
We are already at the first week of September,

213
00:12:31,200 --> 00:12:34,760
and we have only six weeks of time

214
00:12:34,760 --> 00:12:40,120
basically left before we finish the process.

215
00:12:40,120 --> 00:12:45,080
And yeah, we've covered a lot of space, a lot of time

216
00:12:45,080 --> 00:12:45,720
over the months.

217
00:12:45,720 --> 00:12:47,240
We've been traveling quite a bit.

218
00:12:47,240 --> 00:12:54,040
We're trying to be presenting in many parts of the world,

219
00:12:54,040 --> 00:12:58,160
presenting and discussing with the community

220
00:12:58,160 --> 00:13:03,280
to make sure that there are the least amount of surprises

221
00:13:03,280 --> 00:13:07,480
by the time we issue the version, version 1,

222
00:13:07,480 --> 00:13:11,120
stable version in all things open.

223
00:13:11,120 --> 00:13:13,160
I can also give you a preview.

224
00:13:13,160 --> 00:13:15,280
You see here in October, there is--

225
00:13:15,280 --> 00:13:17,080
we have two events scheduled.

226
00:13:17,080 --> 00:13:20,120
One is the all things open launch,

227
00:13:20,120 --> 00:13:22,840
but also there is a workshop that we're

228
00:13:22,840 --> 00:13:27,080
organizing for around specifically about the issue

229
00:13:27,080 --> 00:13:32,200
to discuss and understand better the space of data governance

230
00:13:32,200 --> 00:13:34,160
distribution, et cetera.

231
00:13:34,160 --> 00:13:40,440
We'll have more details this coming week made available.

232
00:13:40,440 --> 00:13:43,560
And this is all thanks-- all the travel and all the--

233
00:13:43,560 --> 00:13:48,280
and the workshop that we're organizing in Paris

234
00:13:48,280 --> 00:13:54,120
is thanks to a grant, a large grant given to us

235
00:13:54,120 --> 00:13:58,880
by the Alfred P. Sloan Foundation.

236
00:13:58,880 --> 00:14:00,240
So how to participate?

237
00:14:00,240 --> 00:14:03,680
You can definitely still email me or Mer,

238
00:14:03,680 --> 00:14:07,400
the preferred for the endorsements.

239
00:14:07,400 --> 00:14:09,440
You can also send public comments

240
00:14:09,440 --> 00:14:12,080
on discuss.opensource.org.

241
00:14:12,080 --> 00:14:16,160
Can definitely signal that you appreciate

242
00:14:16,160 --> 00:14:19,000
the stewardship and the role of the Open Source

243
00:14:19,000 --> 00:14:20,960
Initiative in driving this process

244
00:14:20,960 --> 00:14:22,760
by becoming an OSI member.

245
00:14:22,760 --> 00:14:26,040
You can also donate as you become a member.

246
00:14:26,040 --> 00:14:28,400
And you can join these downloads that--

247
00:14:28,400 --> 00:14:30,360
realize this slide needs to be updated.

248
00:14:30,360 --> 00:14:33,800
Coming-- there's-- there being--

249
00:14:33,800 --> 00:14:38,600
there being-- they're opening now weekly at alternating time.

250
00:14:38,600 --> 00:14:40,240
One week is going to be at this time.

251
00:14:40,240 --> 00:14:46,400
One other week is going to be at 9 AM Central European time

252
00:14:46,400 --> 00:14:50,680
so that the Asian community can join.

253
00:14:50,680 --> 00:14:58,040
And now-- yeah, now we've got time for Q&A.

254
00:14:58,040 --> 00:14:59,960
So a comment from YouTube, an open AI

255
00:14:59,960 --> 00:15:03,840
should prove that its training data is all legally licensed

256
00:15:03,840 --> 00:15:05,000
open source data.

257
00:15:05,000 --> 00:15:07,120
Right, so this is a very interesting question

258
00:15:07,120 --> 00:15:10,520
because it's a frequently asked question, actually.

259
00:15:10,520 --> 00:15:14,080
And so it's been debated for many months.

260
00:15:14,080 --> 00:15:20,960
The short answer is that legally licensed open source data is--

261
00:15:20,960 --> 00:15:24,720
is a-- is a big--

262
00:15:24,720 --> 00:15:28,400
is a big-- is a different--

263
00:15:28,400 --> 00:15:34,360
is a different set of what you think it might be.

264
00:15:34,360 --> 00:15:37,440
And we may want to qualify.

265
00:15:37,440 --> 00:15:42,920
So think about the fact that Google Books, for example,

266
00:15:42,920 --> 00:15:47,680
has built a product built on legally--

267
00:15:47,680 --> 00:15:53,720
legally acquired books, scanned, and done object character

268
00:15:53,720 --> 00:16:00,480
recognition on it, and created a transformative work,

269
00:16:00,480 --> 00:16:02,400
and was sued.

270
00:16:02,400 --> 00:16:06,240
And then they won the lawsuit.

271
00:16:06,240 --> 00:16:10,320
Makes me-- makes us think that the concept of training data

272
00:16:10,320 --> 00:16:12,280
legally licensed open source data

273
00:16:12,280 --> 00:16:15,000
is something that needs to be--

274
00:16:15,000 --> 00:16:16,080
needs to be clarified.

275
00:16:16,080 --> 00:16:21,160
So it's a gut reaction that we all go towards.

276
00:16:21,160 --> 00:16:24,120
But we need to think about the consequences.

277
00:16:24,120 --> 00:16:27,040
It's a big-- it's a big topic.

278
00:16:27,040 --> 00:16:34,960
And that's why we're hosting that conference in Paris also.

279
00:16:34,960 --> 00:16:37,040
So Joshua.

280
00:16:37,040 --> 00:16:41,360
Hello.

281
00:16:41,360 --> 00:16:43,080
This is Joshua.

282
00:16:43,080 --> 00:16:49,160
So in the latest draft, I've been

283
00:16:49,160 --> 00:16:50,840
spending most of my time really trying

284
00:16:50,840 --> 00:16:54,480
to think through reading the definition

285
00:16:54,480 --> 00:17:00,640
from different viewpoints of users, especially

286
00:17:00,640 --> 00:17:02,360
a developer viewpoint.

287
00:17:02,360 --> 00:17:06,040
And so I've been really digging in and thinking about

288
00:17:06,040 --> 00:17:09,280
if I'm using Protobuf or TensorFlow

289
00:17:09,280 --> 00:17:19,320
or these other common frameworks and formats for AI models, what

290
00:17:19,320 --> 00:17:21,240
my world view looks like.

291
00:17:21,240 --> 00:17:25,400
And one thing that really stands out to me

292
00:17:25,400 --> 00:17:29,680
is that generally speaking, we're

293
00:17:29,680 --> 00:17:33,120
gravitating towards these binary formats as what

294
00:17:33,120 --> 00:17:36,880
we understand as the model.

295
00:17:36,880 --> 00:17:40,920
And this is different than the abstract kind of approach

296
00:17:40,920 --> 00:17:44,400
to thinking about what an AI model is in general.

297
00:17:44,400 --> 00:17:49,640
It's, in a sense, broader than the AI model,

298
00:17:49,640 --> 00:17:51,720
which is more narrow, which is counterintuitive.

299
00:17:51,720 --> 00:17:55,880
You would think it would be just the opposite.

300
00:17:55,880 --> 00:17:57,720
And the reason why it's somewhat broader

301
00:17:57,720 --> 00:18:01,280
is because these binary formats allow

302
00:18:01,280 --> 00:18:04,640
you to do a lot of things that go perhaps

303
00:18:04,640 --> 00:18:08,040
beyond what you would normally think of as part of the AI

304
00:18:08,040 --> 00:18:09,040
model.

305
00:18:09,040 --> 00:18:13,480
But when you need to identify in your code base what

306
00:18:13,480 --> 00:18:16,640
is the model, you point to that file.

307
00:18:16,640 --> 00:18:19,160
That's the model.

308
00:18:19,160 --> 00:18:20,920
And so I think that there's going

309
00:18:20,920 --> 00:18:24,880
to be some gap between a lot of people's

310
00:18:24,880 --> 00:18:27,680
intuitive understanding of what the model is

311
00:18:27,680 --> 00:18:33,920
and what the intent of the model is by OS in this definition.

312
00:18:33,920 --> 00:18:40,880
I think it would be helpful to put a little bit more

313
00:18:40,880 --> 00:18:42,560
in the definition.

314
00:18:42,560 --> 00:18:50,800
In the online feedback forum for the latest definition,

315
00:18:50,800 --> 00:18:55,920
I added some comments specifically around this idea

316
00:18:55,920 --> 00:19:00,760
that you should be encouraging you

317
00:19:00,760 --> 00:19:04,640
to be explicit about saying that you should not

318
00:19:04,640 --> 00:19:09,400
use the AI model as a way to pass through, say,

319
00:19:09,400 --> 00:19:12,760
binary blobs or object code.

320
00:19:12,760 --> 00:19:18,200
And the reason why I sort of came to that conclusion

321
00:19:18,200 --> 00:19:21,720
is because I was looking at real examples

322
00:19:21,720 --> 00:19:28,760
where people were using things like Protobuf and other--

323
00:19:28,760 --> 00:19:35,320
in TensorFlow and using variables that just

324
00:19:35,320 --> 00:19:40,320
store data effectively as part of their training.

325
00:19:40,320 --> 00:19:45,760
So they're just storing verbatim blobs of information

326
00:19:45,760 --> 00:19:48,520
that are then used as part of the program.

327
00:19:48,520 --> 00:19:52,800
But it's that interaction, but it's stored in the model

328
00:19:52,800 --> 00:19:55,040
from their perspective, from the user's perspective,

329
00:19:55,040 --> 00:19:56,760
and in the documentation.

330
00:19:56,760 --> 00:20:00,480
So I think it would be helpful to have a little bit more

331
00:20:00,480 --> 00:20:05,000
nuance in the definition to clarify that a trained model is

332
00:20:05,000 --> 00:20:10,000
not the same as merely storing files that can then be reused,

333
00:20:10,000 --> 00:20:16,240
that it should be about a little bit more detail around that,

334
00:20:16,240 --> 00:20:23,800
just to help abuse of the open source AI definition.

335
00:20:23,800 --> 00:20:26,520
I think-- yeah.

336
00:20:26,520 --> 00:20:29,800
So I'm not sure I understand exactly--

337
00:20:29,800 --> 00:20:32,760
in fact, I was emailing you to have a conversation

338
00:20:32,760 --> 00:20:34,520
to clarify what you actually meant.

339
00:20:34,520 --> 00:20:42,640
And I'm glad you came for this and explained it via voice.

340
00:20:42,640 --> 00:20:44,440
So I'm still a little bit puzzled

341
00:20:44,440 --> 00:20:49,320
by the technical details here.

342
00:20:49,320 --> 00:20:55,080
But the general principle of the definition

343
00:20:55,080 --> 00:21:02,160
is that it must resist as much as possible the test of time.

344
00:21:02,160 --> 00:21:06,520
And it needs to set high-level principles at this stage.

345
00:21:06,520 --> 00:21:13,800
If you want to have a mental map to frame the intention here,

346
00:21:13,800 --> 00:21:16,240
the open source AI definition file,

347
00:21:16,240 --> 00:21:19,960
the way you see it linked in the chat,

348
00:21:19,960 --> 00:21:23,960
is the page that the FSF, the Free Software Foundation,

349
00:21:23,960 --> 00:21:26,480
hosts and calls what is free software.

350
00:21:26,480 --> 00:21:31,120
It's the basic principles that we

351
00:21:31,120 --> 00:21:37,720
want to have represented in a view of the world, what

352
00:21:37,720 --> 00:21:39,880
needs to be achieved.

353
00:21:39,880 --> 00:21:42,520
That page has gone through multiple iterations.

354
00:21:42,520 --> 00:21:45,360
Like, if you go below and you read it,

355
00:21:45,360 --> 00:21:49,080
the initial freedoms were three, and then a fourth was added.

356
00:21:49,080 --> 00:21:52,760
And wording has been changed on that page to clarify.

357
00:21:52,760 --> 00:21:55,960
But the principles, what is free software, those three,

358
00:21:55,960 --> 00:21:58,120
and then letter four freedoms have pretty much

359
00:21:58,120 --> 00:21:59,480
remained the same.

360
00:21:59,480 --> 00:22:05,520
What has changed and has evolved and derived from that

361
00:22:05,520 --> 00:22:07,520
is the open source definition, which

362
00:22:07,520 --> 00:22:11,400
is a sort of a checklist to evaluate,

363
00:22:11,400 --> 00:22:15,880
in practical forms, software packages used by--

364
00:22:15,880 --> 00:22:20,200
in order to be included in the FTP servers at the Debian

365
00:22:20,200 --> 00:22:24,880
project first, and then became the open source definition

366
00:22:24,880 --> 00:22:27,400
that are currently used, those 10 principles,

367
00:22:27,400 --> 00:22:33,120
to evaluate the licenses and legal documents.

368
00:22:33,120 --> 00:22:36,280
We want-- we're trying to do the same thing here.

369
00:22:36,280 --> 00:22:40,600
The open source AI definition is what is free software page.

370
00:22:40,600 --> 00:22:46,680
And then the new split document, the checklist,

371
00:22:46,680 --> 00:22:50,800
is more of those 10 points practical interpretation

372
00:22:50,800 --> 00:22:53,560
of, in practice, of what will have

373
00:22:53,560 --> 00:22:58,680
to happen for an AI system to be judged, evaluated,

374
00:22:58,680 --> 00:23:02,520
to be respecting the original intention written

375
00:23:02,520 --> 00:23:04,480
in the definition.

376
00:23:04,480 --> 00:23:06,880
So what I would recommend is that we

377
00:23:06,880 --> 00:23:10,400
spend a little bit more time, collectively,

378
00:23:10,400 --> 00:23:13,400
to think about the interpretation.

379
00:23:13,400 --> 00:23:17,080
Instead of making the first document longer,

380
00:23:17,080 --> 00:23:19,120
we could be spending a little bit more time

381
00:23:19,120 --> 00:23:21,440
to refine and review the-- and we

382
00:23:21,440 --> 00:23:23,320
have more time with another set deadline

383
00:23:23,320 --> 00:23:26,240
to finish the checklist.

384
00:23:26,240 --> 00:23:31,480
Make that document a little bit more rich.

385
00:23:31,480 --> 00:23:35,500
Bentley?

386
00:23:38,800 --> 00:23:41,960
Yeah, I noticed this definition doesn't

387
00:23:41,960 --> 00:23:44,800
mention content generated by models

388
00:23:44,800 --> 00:23:47,840
under an open source license.

389
00:23:47,840 --> 00:23:50,560
This might be much a bigger question,

390
00:23:50,560 --> 00:23:55,480
but is there a plan to address the content generated

391
00:23:55,480 --> 00:23:58,600
by an open source model, how that will be licensed,

392
00:23:58,600 --> 00:24:03,360
or is that part of a much bigger discussion?

393
00:24:03,360 --> 00:24:06,200
That's a good question.

394
00:24:06,200 --> 00:24:11,040
So the jury's still out of what are the legal ramifications

395
00:24:11,040 --> 00:24:11,880
for that part.

396
00:24:11,880 --> 00:24:14,280
But definitely, the definition does not

397
00:24:14,280 --> 00:24:18,400
touch that the same way that, more or less,

398
00:24:18,400 --> 00:24:20,200
the open source software definition

399
00:24:20,200 --> 00:24:24,400
doesn't say whether you can use a C compiler to build

400
00:24:24,400 --> 00:24:29,680
malware or other things like that.

401
00:24:29,680 --> 00:24:33,440
It's a separate-- it would be a separate conversation.

402
00:24:33,440 --> 00:24:38,360
There might be legal documents that

403
00:24:38,360 --> 00:24:41,840
say you cannot use this model, this AI system,

404
00:24:41,840 --> 00:24:45,200
to create--

405
00:24:45,200 --> 00:24:47,600
to infringe on someone else's copyright

406
00:24:47,600 --> 00:24:51,280
or to invent things that have already been invented

407
00:24:51,280 --> 00:24:53,000
or do other things.

408
00:24:53,000 --> 00:24:55,280
But that's something that will have to be evaluated

409
00:24:55,280 --> 00:24:56,800
by the legal community.

410
00:24:56,800 --> 00:25:08,200
[AUDIO OUT]

411
00:25:08,200 --> 00:25:09,600
- I have a question.

412
00:25:09,600 --> 00:25:11,120
Can you hear me, Stefano?

413
00:25:11,120 --> 00:25:11,620
- Yes.

414
00:25:11,620 --> 00:25:17,240
- By way of introduction, I'm a tech attorney.

415
00:25:17,240 --> 00:25:18,840
I've been practicing for 15 years.

416
00:25:18,840 --> 00:25:21,360
And for better or for worse, I've

417
00:25:21,360 --> 00:25:24,400
done a lot of work in the open source space.

418
00:25:24,400 --> 00:25:28,560
The one question I had is, at the top of the call,

419
00:25:28,560 --> 00:25:30,400
you went through the definition.

420
00:25:30,400 --> 00:25:33,960
And you explained that there are three parts to the definition.

421
00:25:33,960 --> 00:25:36,960
Is it fair to state that the first two

422
00:25:36,960 --> 00:25:42,200
parts of that definition have not generated

423
00:25:42,200 --> 00:25:45,280
any controversy versus the third?

424
00:25:45,280 --> 00:25:48,960
- Yeah.

425
00:25:48,960 --> 00:25:51,240
Well, not completely true.

426
00:25:51,240 --> 00:25:54,120
So the first two parts, meaning the model weights

427
00:25:54,120 --> 00:25:57,240
and parameters, and the second part--

428
00:25:57,240 --> 00:25:58,320
- And the source code.

429
00:25:58,320 --> 00:26:00,240
- --the source code of--

430
00:26:00,240 --> 00:26:02,560
so there has been debate.

431
00:26:02,560 --> 00:26:04,800
And there will continue to be a little bit.

432
00:26:04,800 --> 00:26:06,720
In the legal community, I believe

433
00:26:06,720 --> 00:26:13,440
that the legal nature of the weights parameters

434
00:26:13,440 --> 00:26:16,760
is still being debated.

435
00:26:16,760 --> 00:26:20,840
Some legislations may not consider those subject

436
00:26:20,840 --> 00:26:23,560
to any exclusive right.

437
00:26:23,560 --> 00:26:26,400
And whether they should be considered

438
00:26:26,400 --> 00:26:32,480
under any IP law or other exclusive rights

439
00:26:32,480 --> 00:26:36,440
is to be debated.

440
00:26:36,440 --> 00:26:39,360
It's not ferocious debate, but it's an intellectually

441
00:26:39,360 --> 00:26:40,800
stimulating one.

442
00:26:40,800 --> 00:26:44,840
And on the code front, on the source code front,

443
00:26:44,840 --> 00:26:56,040
there is debate over whether the training code should

444
00:26:56,040 --> 00:26:58,160
be strictly required or not.

445
00:26:58,160 --> 00:27:07,040
And there is a little bit of a push and pull.

446
00:27:07,040 --> 00:27:11,000
I've heard rumors about that requirement,

447
00:27:11,000 --> 00:27:13,920
strict requirement, as being a little bit too limiting.

448
00:27:13,920 --> 00:27:14,560
Too limiting.

449
00:27:14,560 --> 00:27:19,400
- Got it.

450
00:27:19,400 --> 00:27:23,520
But just to clarify, the third component of the definition,

451
00:27:23,520 --> 00:27:25,400
with respect to the training data,

452
00:27:25,400 --> 00:27:26,760
is the most controversial.

453
00:27:26,760 --> 00:27:27,560
Is that correct?

454
00:27:27,560 --> 00:27:28,600
- Yeah.

455
00:27:28,600 --> 00:27:29,440
Correct, yes.

456
00:27:29,440 --> 00:27:34,280
Because, yes, instinctively, all of us,

457
00:27:34,280 --> 00:27:40,520
at the beginning of the process, everyone had the same thought.

458
00:27:40,520 --> 00:27:44,440
Data is where the weights come from.

459
00:27:44,440 --> 00:27:48,440
Some people use the term source, which is confusing.

460
00:27:48,440 --> 00:27:51,120
It's not source code in the same way.

461
00:27:51,120 --> 00:27:53,320
But it's where it comes from.

462
00:27:53,320 --> 00:27:55,840
If without the data, you don't have the models.

463
00:27:55,840 --> 00:27:57,840
You don't have the parameters.

464
00:27:57,840 --> 00:28:02,720
And therefore, given that requirement,

465
00:28:02,720 --> 00:28:04,320
all of the pipe, the whole pipeline

466
00:28:04,320 --> 00:28:07,560
needs to be open and open source.

467
00:28:07,560 --> 00:28:10,800
But data is not source code.

468
00:28:10,800 --> 00:28:19,160
It doesn't fall under the same easy, air quotes,

469
00:28:19,160 --> 00:28:20,960
legal framework.

470
00:28:20,960 --> 00:28:22,480
It's a whole different beast.

471
00:28:22,480 --> 00:28:25,760
And realizing that became the big boulder

472
00:28:25,760 --> 00:28:31,200
that we had to navigate around, find a way to navigate around.

473
00:28:31,200 --> 00:28:31,680
- Got it.

474
00:28:31,680 --> 00:28:32,160
Thank you.

475
00:28:32,160 --> 00:28:35,280
That's very helpful, Stefano.

476
00:28:35,280 --> 00:28:36,600
And I apologize.

477
00:28:36,600 --> 00:28:40,360
I have read a bunch of the comments on the various versions.

478
00:28:40,360 --> 00:28:42,640
But obviously, I don't have the full history here.

479
00:28:42,640 --> 00:28:46,040
And nobody knows this stuff better than yourself.

480
00:28:46,040 --> 00:28:52,800
Did OSI, at any point, consider using different words

481
00:28:52,800 --> 00:28:57,120
to describe models that would fit within this definition

482
00:28:57,120 --> 00:29:01,320
rather than open source as a way to resolve

483
00:29:01,320 --> 00:29:05,080
the complaints from various members of the community?

484
00:29:05,080 --> 00:29:06,920
Was that considered as an option?

485
00:29:06,920 --> 00:29:08,920
- That is a very awesome question.

486
00:29:08,920 --> 00:29:10,240
That is a very awesome question.

487
00:29:10,240 --> 00:29:12,200
It was one of the very first questions,

488
00:29:12,200 --> 00:29:15,280
is what we're going to call this thing.

489
00:29:15,280 --> 00:29:18,080
And yeah, I'm on the left.

490
00:29:18,080 --> 00:29:23,240
But yes, unfortunately, though, our hand

491
00:29:23,240 --> 00:29:28,280
was forced by the fact that open source AI, as a term,

492
00:29:28,280 --> 00:29:29,520
was already being used.

493
00:29:29,520 --> 00:29:35,840
And even abused by some players.

494
00:29:35,840 --> 00:29:37,600
And I can be public about it, because I've

495
00:29:37,600 --> 00:29:38,680
been public about it.

496
00:29:38,680 --> 00:29:40,960
Meta is one of the abusers of the term.

497
00:29:40,960 --> 00:29:43,560
They keep on using, referring to open source AI.

498
00:29:43,560 --> 00:29:48,760
So in order to safeguard open source, the term itself,

499
00:29:48,760 --> 00:29:54,280
we don't have another choice but to call it open source AI

500
00:29:54,280 --> 00:29:57,120
and work around it.

501
00:29:57,120 --> 00:29:57,640
- Thank you.

502
00:29:57,640 --> 00:29:59,600
That's very helpful context.

503
00:29:59,600 --> 00:30:01,480
That's all I have.

504
00:30:01,480 --> 00:30:01,980
- Yeah.

505
00:30:01,980 --> 00:30:09,680
I see Joshua typing.

506
00:30:09,680 --> 00:30:19,840
Are there any other questions from the Mozilla group or YouTube?

507
00:30:20,840 --> 00:30:21,880
Josh, I see you typing.

508
00:30:21,880 --> 00:30:22,920
Feel free to grab the mic.

509
00:30:22,920 --> 00:30:27,240
- Thanks.

510
00:30:27,240 --> 00:30:31,320
Yeah, it was just sort of a follow up.

511
00:30:31,320 --> 00:30:35,560
My point is that, in principle, we

512
00:30:35,560 --> 00:30:42,640
know that when training a model, that we

513
00:30:42,640 --> 00:30:52,880
know that when training a model, that it gets trained on code

514
00:30:52,880 --> 00:30:53,760
at times.

515
00:30:53,760 --> 00:30:56,440
Some of the data is code.

516
00:30:56,440 --> 00:30:59,160
Some of the data is object code.

517
00:30:59,160 --> 00:31:02,080
We know this because GitHub and other things

518
00:31:02,080 --> 00:31:07,240
are often used as the training data.

519
00:31:07,240 --> 00:31:13,560
And we know that models sometimes make use of just

520
00:31:13,560 --> 00:31:14,720
verbatim storage.

521
00:31:14,720 --> 00:31:21,320
Instead of taking the data and trying to learn something

522
00:31:21,320 --> 00:31:25,800
from it, sometimes you just save blobs of data in your model.

523
00:31:25,800 --> 00:31:31,120
All major models, model formats, TensorFlow, and so forth,

524
00:31:31,120 --> 00:31:33,920
have facilities for doing exactly that function.

525
00:31:36,600 --> 00:31:40,680
TensorFlow Save Models has a bunch of things.

526
00:31:40,680 --> 00:31:43,240
Protobuf has options.

527
00:31:43,240 --> 00:31:47,680
And I don't think, in principle, that is not what you're saying.

528
00:31:47,680 --> 00:31:53,560
You're not saying that training can be taking data,

529
00:31:53,560 --> 00:31:57,960
like an object code blob, storing it in the model.

530
00:31:57,960 --> 00:32:02,840
And then when you build your AI system and you use the model,

531
00:32:02,840 --> 00:32:07,480
copy that data, load it into an executable area of memory,

532
00:32:07,480 --> 00:32:09,440
and run it.

533
00:32:09,440 --> 00:32:13,600
Now, if I'm building consumer electronics, though,

534
00:32:13,600 --> 00:32:16,560
and I'm sending updates using my model, which

535
00:32:16,560 --> 00:32:21,000
is going to be an increasingly common paradigm, given

536
00:32:21,000 --> 00:32:26,240
the fact that we have just huge amounts of AI hardware being

537
00:32:26,240 --> 00:32:29,920
put out there, then it would be an attractive thing for me

538
00:32:29,920 --> 00:32:34,200
to update my platform via that AI model.

539
00:32:34,200 --> 00:32:36,880
And some of the things I'll put in there are, yes,

540
00:32:36,880 --> 00:32:38,160
they're technically trained.

541
00:32:38,160 --> 00:32:44,200
They're trained to say, here's a lookup table of what

542
00:32:44,200 --> 00:32:48,280
should be executed when the system initializes,

543
00:32:48,280 --> 00:32:49,360
the AI system.

544
00:32:49,360 --> 00:32:52,360
And I think it would be good to just draw a bright line

545
00:32:52,360 --> 00:32:58,440
and say, no, you can't just say that--

546
00:32:58,440 --> 00:33:00,680
you can't just move something technically

547
00:33:00,680 --> 00:33:05,480
into what is the model and get around these principles

548
00:33:05,480 --> 00:33:06,480
that we're making clear.

549
00:33:06,480 --> 00:33:08,000
And that's the same thing as if you

550
00:33:08,000 --> 00:33:13,800
were to move that code elsewhere in your AI model.

551
00:33:13,800 --> 00:33:15,000
I see your point.

552
00:33:15,000 --> 00:33:16,000
I see your point.

553
00:33:16,000 --> 00:33:18,480
Now, it's more clear.

554
00:33:18,480 --> 00:33:21,040
But I would recommend that we go back

555
00:33:21,040 --> 00:33:25,040
to the reason why we put, at the beginning of the document,

556
00:33:25,040 --> 00:33:28,320
the definition of what is an AI system.

557
00:33:28,320 --> 00:33:29,440
Maybe that helps.

558
00:33:29,440 --> 00:33:31,880
Because if we go back to the definition, what is the AI

559
00:33:31,880 --> 00:33:35,720
system that we are defining in this work, in this document?

560
00:33:35,720 --> 00:33:42,640
The AI system is anything that, for implicit objectives,

561
00:33:42,640 --> 00:33:46,320
given an input spits out an output.

562
00:33:46,320 --> 00:33:50,880
So whatever you call that, whether you package it

563
00:33:50,880 --> 00:33:53,880
in a binary that loads a blob and executes

564
00:33:53,880 --> 00:33:57,680
a virtual machine that executes something else,

565
00:33:57,680 --> 00:34:04,040
if what we're calling here is what we're defining

566
00:34:04,040 --> 00:34:07,560
and what we need the corresponding--

567
00:34:07,560 --> 00:34:10,000
the preferred form to make modification of

568
00:34:10,000 --> 00:34:12,480
is whatever that system is, however it's packaged,

569
00:34:12,480 --> 00:34:14,280
however it's shipped, whatever it ships.

570
00:34:14,280 --> 00:34:22,680
So when we go into the implementation,

571
00:34:22,680 --> 00:34:24,880
some reviewer who wants to--

572
00:34:24,880 --> 00:34:27,320
someone like, I don't know, the Linux Foundation

573
00:34:27,320 --> 00:34:28,640
wants to--

574
00:34:28,640 --> 00:34:31,680
may want to have a requirement some day,

575
00:34:31,680 --> 00:34:33,800
maybe will have a requirement that says,

576
00:34:33,800 --> 00:34:39,880
we only accept AI systems in our projects that are open source

577
00:34:39,880 --> 00:34:43,120
AI following the definition of DOSI.

578
00:34:43,120 --> 00:34:47,280
Then they will have a standard set for specifically

579
00:34:47,280 --> 00:34:49,840
that technology, the same way that they have the model

580
00:34:49,840 --> 00:34:52,880
openness framework now for the generative AI models.

581
00:34:52,880 --> 00:34:57,240
And in that place, they can put all of the requirements

582
00:34:57,240 --> 00:35:00,200
specifically for that technology to say,

583
00:35:00,200 --> 00:35:02,160
nope, that loading of that binary

584
00:35:02,160 --> 00:35:04,280
here is not a good thing.

585
00:35:04,280 --> 00:35:06,520
It's basically a workaround.

586
00:35:06,520 --> 00:35:08,200
It's not acceptable.

587
00:35:08,200 --> 00:35:12,960
Or we'll have to wait and see.

588
00:35:12,960 --> 00:35:15,560
I mean, another way, another approach that we could have

589
00:35:15,560 --> 00:35:16,760
is to wait and see what--

590
00:35:16,760 --> 00:35:19,680
if this threat that you have in mind

591
00:35:19,680 --> 00:35:21,080
is actually going to show up.

592
00:35:21,080 --> 00:35:24,600
It's not just a theoretical threat.

593
00:35:24,600 --> 00:35:31,120
If it will be showing up, we're thinking one of the tasks

594
00:35:31,120 --> 00:35:37,160
that we have to do by the launch is to really come up

595
00:35:37,160 --> 00:35:40,920
with suggest recommendations on how we're going to be

596
00:35:40,920 --> 00:35:44,800
monitoring the efficacy, the adoption of the open source AI

597
00:35:44,800 --> 00:35:48,000
definition, the availability of other--

598
00:35:48,000 --> 00:35:52,320
availability and reviewing of the existence of open source

599
00:35:52,320 --> 00:35:56,520
AI systems besides the initial ones that we have identified,

600
00:35:56,520 --> 00:36:03,240
which, by the way, are the ones released by the Eleuther AI,

601
00:36:03,240 --> 00:36:07,720
Alen AI Institute, LLM 360, and most likely,

602
00:36:07,720 --> 00:36:11,080
DII, we're working to understand those better.

603
00:36:11,080 --> 00:36:14,480
So those are the ones that pass the open source AI definition

604
00:36:14,480 --> 00:36:14,960
right now.

605
00:36:14,960 --> 00:36:18,600
Thank you.

606
00:36:18,600 --> 00:36:19,640
I see.

607
00:36:19,640 --> 00:36:21,200
Yeah.

608
00:36:22,040 --> 00:36:25,040
Ben?

609
00:36:25,040 --> 00:36:32,440
So when a company modifies an open source AI model

610
00:36:32,440 --> 00:36:35,480
for commercial use, at what point

611
00:36:35,480 --> 00:36:39,480
does the modification become a derivative work subject

612
00:36:39,480 --> 00:36:40,560
to the original license?

613
00:36:40,560 --> 00:36:45,760
And would existing legal tests around open source licensing

614
00:36:45,760 --> 00:36:50,560
be sufficient to determine the AI model modifications

615
00:36:50,560 --> 00:36:52,120
and the derivative work aspect of it?

616
00:36:52,120 --> 00:36:57,040
Or do you foresee a whole new level of legal guidance

617
00:36:57,040 --> 00:37:00,280
needed with regards to these type of things?

618
00:37:00,280 --> 00:37:02,640
Yeah, that's a very good question.

619
00:37:02,640 --> 00:37:09,120
Frankly, lots of it depends on what the models themselves

620
00:37:09,120 --> 00:37:12,560
are considered under laws.

621
00:37:12,560 --> 00:37:16,040
Because right now, yeah, it all depends

622
00:37:16,040 --> 00:37:23,320
on what contracts get built around them

623
00:37:23,320 --> 00:37:27,040
and how those propagate between--

624
00:37:27,040 --> 00:37:29,600
all the rights propagate between this new artifact,

625
00:37:29,600 --> 00:37:35,800
the trained weights, trained parameters, to derivatives.

626
00:37:35,800 --> 00:37:37,360
Yeah.

627
00:37:37,360 --> 00:37:37,880
Not easy.

628
00:37:45,960 --> 00:37:50,200
So there is a conversation now happening on the license

629
00:37:50,200 --> 00:37:55,600
review mailing list, which is the community of volunteers

630
00:37:55,600 --> 00:38:00,400
who've been reviewing licenses, software licenses.

631
00:38:00,400 --> 00:38:04,040
And they've been asked to consider

632
00:38:04,040 --> 00:38:08,600
how to handle licenses and other legal documents that

633
00:38:08,600 --> 00:38:13,000
are not covering software, so the traditional space.

634
00:38:13,000 --> 00:38:17,760
So if you're interested in joining that conversation,

635
00:38:17,760 --> 00:38:22,320
I would highly recommend to join the license-review mailing

636
00:38:22,320 --> 00:38:22,820
list.

637
00:38:22,820 --> 00:38:30,360
I see a question from Peter.

638
00:38:30,360 --> 00:38:33,240
Other non-profits in OSI ecosystems,

639
00:38:33,240 --> 00:38:37,120
like GNOME, Alliance Foundation, Physotek Foundation, et cetera,

640
00:38:37,120 --> 00:38:40,200
have any of them attempted to independently define

641
00:38:40,200 --> 00:38:46,160
open-source AI or provided input to OSI about OSI's definition?

642
00:38:46,160 --> 00:38:47,880
Oh, Peter, yeah, this is a great question

643
00:38:47,880 --> 00:38:50,280
because it allows us to explain a little bit more

644
00:38:50,280 --> 00:38:52,720
what the process has been.

645
00:38:52,720 --> 00:38:55,160
So this is not OSI's definition, the same way

646
00:38:55,160 --> 00:38:58,320
that the open-source definition is not OSI's definition.

647
00:38:58,320 --> 00:39:01,040
We merely maintain it for the community.

648
00:39:01,040 --> 00:39:04,840
So we have worked from the very beginning with Linux Foundation

649
00:39:04,840 --> 00:39:09,400
and FSF and others, reached out to them

650
00:39:09,400 --> 00:39:13,920
and asked them to contribute to the definition.

651
00:39:13,920 --> 00:39:15,760
This definition is not written by us.

652
00:39:15,760 --> 00:39:21,200
It's written by a process held by a co-design process.

653
00:39:21,200 --> 00:39:26,880
It's a process designed to work with the people affected

654
00:39:26,880 --> 00:39:29,000
by the decision, not for them.

655
00:39:29,000 --> 00:39:32,880
So yes, all of these organizations

656
00:39:32,880 --> 00:39:37,080
mentioned, plus many others-- Creative Commons, Eleuthera

657
00:39:37,080 --> 00:39:41,520
AI, Mozilla Foundation, many other groups

658
00:39:41,520 --> 00:39:44,360
that have participated to the list.

659
00:39:44,360 --> 00:39:46,000
And we have somewhere on the website,

660
00:39:46,000 --> 00:39:51,000
opensource.org/deepdive, we have a list of the groups

661
00:39:51,000 --> 00:39:53,360
that have been-- and the volunteers that

662
00:39:53,360 --> 00:39:56,200
have been included.

663
00:39:56,200 --> 00:40:11,920
, All right.

664
00:40:11,920 --> 00:40:23,760
Are there any more questions?

665
00:40:23,760 --> 00:40:43,680
[AUDIO OUT]

666
00:40:43,680 --> 00:40:45,160
OK, then.

667
00:40:45,160 --> 00:40:48,880
I'm going to call it a day, call it a week.

668
00:40:48,880 --> 00:40:49,600
Thanks, everyone.

669
00:40:49,600 --> 00:40:52,280
This has been very useful, very interesting for me.

670
00:40:52,280 --> 00:40:54,760
I hope it's been informative for you, too.

671
00:40:54,760 --> 00:41:00,520
And I would like Nick is saying on the chat,

672
00:41:00,520 --> 00:41:05,040
our discussions continue on our forum,

673
00:41:05,040 --> 00:41:07,640
discuss.opensource.org.

674
00:41:07,640 --> 00:41:14,400
And the license review working group is available.

675
00:41:14,400 --> 00:41:18,560
Peter, stay on the chat here.

676
00:41:18,560 --> 00:41:21,040
I will send you the link where to join it.

677
00:41:21,040 --> 00:41:22,960
It's old school, meaningless.

678
00:41:22,960 --> 00:41:29,320
Thank you, everyone.

