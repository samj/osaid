1
00:00:00,001 --> 00:00:10,600
All right, everyone, and welcome to the final town hall before the release of version 1.0.

2
00:00:10,600 --> 00:00:17,760
The last week of October, October 28th, will be at all things open and there will be a

3
00:00:17,760 --> 00:00:24,120
public announcement of a sort.

4
00:00:24,120 --> 00:00:31,360
And so let's recap just a little bit for everyone who may have not followed.

5
00:00:31,360 --> 00:00:37,840
We have received a grant from the Alfred Sloan Foundation to conduct part of this work.

6
00:00:37,840 --> 00:00:42,240
So we're grateful for that and a sponsorship donation from Mercado Libre.

7
00:00:42,240 --> 00:00:46,280
We're also very grateful for that contribution.

8
00:00:46,280 --> 00:00:51,360
Let's go through the reason why we're doing this because people are asking us, why don't

9
00:00:51,360 --> 00:00:52,360
you take a break?

10
00:00:52,360 --> 00:00:53,360
Why don't you wait a little bit?

11
00:00:53,360 --> 00:00:55,640
Are you ready for it?

12
00:00:55,640 --> 00:01:01,440
The thing is that we need to respond to what's happening already outside.

13
00:01:01,440 --> 00:01:08,680
So we know that this is a challenging effort and it's been a hard conversation to drive.

14
00:01:08,680 --> 00:01:17,440
But we also are in a space where the term open source AI is being used and referred

15
00:01:17,440 --> 00:01:29,440
in many different areas from the market, from companies, organizations, and regulators are

16
00:01:29,440 --> 00:01:39,320
all referring to the term open source with the AI space and with the AI word right after

17
00:01:39,320 --> 00:01:42,000
it.

18
00:01:42,000 --> 00:01:49,640
And our stakeholders in general, the open source initiatives, supporters and communities

19
00:01:49,640 --> 00:01:52,120
keep asking us the same thing.

20
00:01:52,120 --> 00:01:54,200
Like, this is not open source.

21
00:01:54,200 --> 00:02:02,440
Why are you not correcting these statements?

22
00:02:02,440 --> 00:02:05,480
And this was visible a long time ago.

23
00:02:05,480 --> 00:02:13,120
We started noticing that this issue was of AI not having a clear understanding was very

24
00:02:13,120 --> 00:02:17,320
visible when I started at the open source initiative in 2022.

25
00:02:17,320 --> 00:02:25,360
So we started that year to interview experts in various areas because this is a new space.

26
00:02:25,360 --> 00:02:29,920
It's a new domain that we don't have familiarity with.

27
00:02:29,920 --> 00:02:34,840
And so we started educating ourselves with a series of interviews that we published as

28
00:02:34,840 --> 00:02:35,840
a podcast.

29
00:02:35,840 --> 00:02:43,640
We also held discussions in four different areas in the business, society impact, legal,

30
00:02:43,640 --> 00:02:46,960
academic research, and with experts.

31
00:02:46,960 --> 00:02:54,040
And we published the results of all of that in a nice easy to read report.

32
00:02:54,040 --> 00:03:04,400
I recommend you all to go find it and read it to get an initial overview of why AI and

33
00:03:04,400 --> 00:03:09,080
machine learning, this machine learning space is so radically different from the software

34
00:03:09,080 --> 00:03:12,280
space that we are very familiar with.

35
00:03:12,280 --> 00:03:20,280
And at the end of this research that continued in 2023 with a series of webinars, we decided

36
00:03:20,280 --> 00:03:26,960
that we needed to have a process that was wide.

37
00:03:26,960 --> 00:03:36,080
It had to be global and it had to be involving not the typical people who you find at the

38
00:03:36,080 --> 00:03:40,120
open source conferences or in the open source software circles.

39
00:03:40,120 --> 00:03:47,440
It needed to be something that was involving researchers and from other parts of the world,

40
00:03:47,440 --> 00:03:51,360
but also experts in different specific domains.

41
00:03:51,360 --> 00:03:52,600
Not just the software space.

42
00:03:52,600 --> 00:03:59,960
We needed to have experts in civil, civic society and environmental impact.

43
00:03:59,960 --> 00:04:06,120
Legal experts in copyright, but also in privacy rights and indigenous knowledge.

44
00:04:06,120 --> 00:04:12,120
All of those, all of the areas that we were not traditionally familiar with in the open

45
00:04:12,120 --> 00:04:15,320
source software space.

46
00:04:15,320 --> 00:04:22,040
And we held this, this is how we started with this process called co-design, which is a

47
00:04:22,040 --> 00:04:34,120
process that is a technique basically to design new things with the people who are going to

48
00:04:34,120 --> 00:04:37,240
be impacted by it, not for them.

49
00:04:37,240 --> 00:04:42,680
And the process was structured in a series of questions.

50
00:04:42,680 --> 00:04:47,080
The first question that we asked groups of volunteers that we have made a concerted effort

51
00:04:47,080 --> 00:04:58,200
to be different, diverse with various dimensions of diversity from geographic, gender expertise.

52
00:04:58,200 --> 00:05:02,760
And we tried to assemble working groups that had all of these characteristics represented.

53
00:05:02,760 --> 00:05:07,840
We asked the first question, what should the open, the meaning of open source and the free

54
00:05:07,840 --> 00:05:10,600
software principles, how do we translate them?

55
00:05:10,600 --> 00:05:15,320
How do we apply them to artificial intelligence space?

56
00:05:15,320 --> 00:05:21,400
And it was fairly easy to transform it slightly wordsmith, if you want the four freedoms,

57
00:05:21,400 --> 00:05:27,560
the freedom to use, study, modify and share and apply them with changing some of the words

58
00:05:27,560 --> 00:05:32,160
so that they will fit the machine learning, the artificial intelligence space.

59
00:05:32,160 --> 00:05:38,160
So then the next question we asked them is what components of AI systems you need in

60
00:05:38,160 --> 00:05:41,080
order to use, study, modify and share.

61
00:05:41,080 --> 00:05:44,400
Again, this is because it's a new space.

62
00:05:44,400 --> 00:05:50,400
We know that there are multiple artifacts of different nature and we needed to ask them

63
00:05:50,400 --> 00:05:55,480
the working groups to help us understand what components are required.

64
00:05:55,480 --> 00:06:02,680
And we have these, the working group set up to analyze for example, systems, some that

65
00:06:02,680 --> 00:06:08,040
we knew were not open source and some that we knew are more open in different ways, like

66
00:06:08,040 --> 00:06:14,400
the difference between Bloom and Pythia and some of like OpenCV that we know is works

67
00:06:14,400 --> 00:06:15,840
in the open source space.

68
00:06:15,840 --> 00:06:22,560
So, but it's not a generative AI, it's more, it is about computer vision, but still using

69
00:06:22,560 --> 00:06:24,420
machine learning techniques.

70
00:06:24,420 --> 00:06:32,700
So we asked them also to rank the importance of each of the components and they give us

71
00:06:32,700 --> 00:06:39,740
indications of how to proceed, how to move forward.

72
00:06:39,740 --> 00:06:42,620
And we released a new draft of the definition.

73
00:06:42,620 --> 00:06:49,580
Now with that draft definition version 006, we asked another question to other people,

74
00:06:49,580 --> 00:06:57,900
which of these, if you use these criteria listed in 006, which of these systems actually

75
00:06:57,900 --> 00:07:00,420
comply with those criteria?

76
00:07:00,420 --> 00:07:10,940
And we asked, we have these reviewers, many more involved, and they give us, they give

77
00:07:10,940 --> 00:07:15,660
us the, they validated the theory.

78
00:07:15,660 --> 00:07:20,260
And that theory confirmed what we thought was going to be happening.

79
00:07:20,260 --> 00:07:26,580
In other words, that the systems like LLM 360, Olmo and Pythia, which had been developed

80
00:07:26,580 --> 00:07:33,300
by nonprofit organizations in the open, supplying all of the components that are necessary to

81
00:07:33,300 --> 00:07:40,020
really study, share, modify, use, including the data sets, including the training code,

82
00:07:40,020 --> 00:07:48,300
and including crucially also the data processing code and filtering, all of these would pass.

83
00:07:48,300 --> 00:07:52,740
The other one, the ones that would pass if they changed their license, were other, the

84
00:07:52,740 --> 00:08:00,300
other ones from Bloom, the big research group, it would pass if they changed the license.

85
00:08:00,300 --> 00:08:06,660
And then there were other groups, other that were easily confirmed as no, like Mistral

86
00:08:06,660 --> 00:08:14,020
and Lama, not only because they don't have sufficient permissions granted through their

87
00:08:14,020 --> 00:08:20,820
terms of use and distribution, but also because they lack critical components in specifically

88
00:08:20,820 --> 00:08:27,620
the training code and the training, the data filtering, the data filtering code.

89
00:08:27,620 --> 00:08:33,900
And so we ended up with a definition that you can see on release candidate one, that

90
00:08:33,900 --> 00:08:39,540
is structured in a way that you recognize that it's a top part, it is more of a manifesto,

91
00:08:39,540 --> 00:08:44,700
basic concept of why we're doing this, what is an open source AI, those four freedoms,

92
00:08:44,700 --> 00:08:49,220
which are referred to a preferred form of making modifications to machine learning system.

93
00:08:49,220 --> 00:08:56,380
And that part is, the top part is simple and probably likely to change in the future, but

94
00:08:56,380 --> 00:09:05,220
what will, is more likely to be adapted to the evolution in the future is this preferred

95
00:09:05,220 --> 00:09:07,660
form of making modifications to machine learning.

96
00:09:07,660 --> 00:09:10,620
And that's, we're going to look at it more quickly.

97
00:09:10,620 --> 00:09:17,940
But the very short version of it is that an open source AI is an AI that gives you the

98
00:09:17,940 --> 00:09:26,780
basic components of weights, code, and data in a way that you can use it, modify, share

99
00:09:26,780 --> 00:09:29,100
it as you want.

100
00:09:29,100 --> 00:09:37,340
Now under, yeah, under open source terms and conditions approved by the Open Source Initiative

101
00:09:37,340 --> 00:09:44,100
using the open source definition as the classic open source definition as a thermal reference.

102
00:09:44,100 --> 00:09:52,380
Now the thing is that we want also open source AI in fields like medical space.

103
00:09:52,380 --> 00:09:58,500
And some of you may have noticed that there is some controversy.

104
00:09:58,500 --> 00:10:10,420
You may have read this post by Red Monk saying that there is a conundrum, that the definition

105
00:10:10,420 --> 00:10:19,380
contains the possibility around data and people ask for all the data to be made available.

106
00:10:19,380 --> 00:10:24,140
Transform probably, well, all right, ask for all the data.

107
00:10:24,140 --> 00:10:30,540
But we do ask for all the data also in this definition, and it's all about all the data

108
00:10:30,540 --> 00:10:33,460
that you can distribute.

109
00:10:33,460 --> 00:10:41,740
And so that allows us to have medical AI without compromising any of our assumptions.

110
00:10:41,740 --> 00:10:52,060
We do because we do require all the information and all the code to replicate the data set.

111
00:10:52,060 --> 00:10:59,140
And so this is also rooted in a practice historically accepted by the open source movement, which

112
00:10:59,140 --> 00:11:07,700
is to make sure that there is the possibility to actually create data software that can

113
00:11:07,700 --> 00:11:10,740
compete with the proprietary ones.

114
00:11:10,740 --> 00:11:17,020
And the reason why we have, I mean, we follow the same pattern in this case to make sure

115
00:11:17,020 --> 00:11:22,380
that we do not compromise on our principles.

116
00:11:22,380 --> 00:11:30,420
And we don't leave open AI or meta to use all of the, to have all of the freedoms and

117
00:11:30,420 --> 00:11:38,980
why we prevent ourselves from ever being able to have open source competing platforms and

118
00:11:38,980 --> 00:11:42,540
projects.

119
00:11:42,540 --> 00:11:50,300
The board of AOSI gave us guidance that I think we have pretty met because we have a

120
00:11:50,300 --> 00:11:57,260
diversity, a variety of stakeholders that are supporting already and endorsing this

121
00:11:57,260 --> 00:11:58,260
definition.

122
00:11:58,260 --> 00:12:07,260
We have a set of real life examples of systems that comply, which are PTR, Olmo, and LLM

123
00:12:07,260 --> 00:12:08,260
360.

124
00:12:08,260 --> 00:12:13,140
And there are a few more that if we spent more time or if they changed the license,

125
00:12:13,140 --> 00:12:18,820
they would fit like Bloom, Starcoder, and others.

126
00:12:18,820 --> 00:12:26,620
And we have a release candidate that is in pretty decent shape, can be proposed to the

127
00:12:26,620 --> 00:12:30,580
meeting in Raleigh.

128
00:12:30,580 --> 00:12:37,740
To clarify all of this, also accompanying the definition, we have worked on an FAQ.

129
00:12:37,740 --> 00:12:39,680
It's pretty extensive.

130
00:12:39,680 --> 00:12:46,500
We have responded to the most frequently asked questions during this debates in the past

131
00:12:46,500 --> 00:12:48,300
months.

132
00:12:48,300 --> 00:12:56,220
And we have clarified in here the type of data and the reason for why data is completely

133
00:12:56,220 --> 00:12:57,500
different from software.

134
00:12:57,500 --> 00:13:04,420
So it does require people who come from the software space to spend some time training,

135
00:13:04,420 --> 00:13:10,420
updating, and we will keep on doing this work to educate the public in general that this

136
00:13:10,420 --> 00:13:12,440
is a different space.

137
00:13:12,440 --> 00:13:14,540
It's not software.

138
00:13:14,540 --> 00:13:22,500
And we clarified in release candidate one of the intention here is to have data, details

139
00:13:22,500 --> 00:13:29,640
about either the data itself or if it's not available because of legal constraints or

140
00:13:29,640 --> 00:13:38,420
other constraints, like other constraints, it needs to be available together with the

141
00:13:38,420 --> 00:13:44,540
code, one needs to be able to rebuild a data set that can be fed into the machine.

142
00:13:44,540 --> 00:13:52,660
And together with the code, that gives you as a downstream user all the necessary tools,

143
00:13:52,660 --> 00:14:00,500
abilities, licenses to create a system that is a fork of the original one.

144
00:14:00,500 --> 00:14:04,380
And you can build and innovate on top with your own data or with the same data that has

145
00:14:04,380 --> 00:14:10,100
been provided by the original developer.

146
00:14:10,100 --> 00:14:15,060
So we've been doing this quite intensively for many months.

147
00:14:15,060 --> 00:14:25,660
In 2024, it was a very intense list of meetings, travel, and we're to the end.

148
00:14:25,660 --> 00:14:27,180
We're going to the end.

149
00:14:27,180 --> 00:14:32,220
So at this time, it's about time for you to endorse it.

150
00:14:32,220 --> 00:14:38,500
We have a public process, a public website where you can go and you can endorse it as

151
00:14:38,500 --> 00:14:42,380
an individual or as an organization.

152
00:14:42,380 --> 00:14:45,140
And you can also criticize it.

153
00:14:45,140 --> 00:14:48,060
You can write long form.

154
00:14:48,060 --> 00:14:54,180
We encourage you to write long form critiques for it.

155
00:14:54,180 --> 00:14:57,740
And this is not going to be the end of it.

156
00:14:57,740 --> 00:15:04,780
Because we're going to be releasing a definition that we know is only a 1.0.

157
00:15:04,780 --> 00:15:05,900
It's a humble release.

158
00:15:05,900 --> 00:15:10,780
It's not going to be something that we know it's going to last for 26 years pretty much

159
00:15:10,780 --> 00:15:16,140
untouched like the original open source definition, even though we know that the open source definition

160
00:15:16,140 --> 00:15:22,580
also changed quite-- yeah, it was nine points when it was first released.

161
00:15:22,580 --> 00:15:26,020
And then a 10th one was added.

162
00:15:26,020 --> 00:15:28,540
So we're going to do something similar.

163
00:15:28,540 --> 00:15:36,900
The board is ready to establish a committee and call for working groups to monitor with

164
00:15:36,900 --> 00:15:44,300
the rest of the communities involved, monitor the space, and get ready to adapt the definition

165
00:15:44,300 --> 00:15:50,380
to response that we get from the public after it's launched.

166
00:15:50,380 --> 00:15:55,020
And so with that, I think I can take some questions.

167
00:15:55,020 --> 00:15:59,980
And I want to remind everyone to follow our community agreements.

168
00:15:59,980 --> 00:16:00,980
Be nice.

169
00:16:00,980 --> 00:16:02,580
We're not here to get bashed.

170
00:16:02,580 --> 00:16:06,040
And we're not here to listen to long rants.

171
00:16:06,040 --> 00:16:14,460
So you can raise your hand or ask written down on the public chat, however you feel

172
00:16:14,460 --> 00:16:15,460
more comfortable.

173
00:16:15,460 --> 00:16:16,460
Sure.

174
00:16:16,460 --> 00:16:17,460
Go ahead.

175
00:16:17,460 --> 00:16:18,460
Let's see.

176
00:16:18,460 --> 00:16:25,460
All right.

177
00:16:25,460 --> 00:16:30,460
Lunduk.

178
00:16:30,460 --> 00:16:43,260
You should be-- yeah, OK.

179
00:16:43,260 --> 00:16:50,540
I can see you but not hear you.

180
00:16:50,540 --> 00:17:01,220
Oh, from your icon, I think when you joined, you were listen only.

181
00:17:01,220 --> 00:17:06,660
You need to rejoin, I think.

182
00:17:06,660 --> 00:17:09,660
Where is that?

183
00:17:09,660 --> 00:17:14,420
Let me see.

184
00:17:14,420 --> 00:17:21,700
Let's try this way.

185
00:17:21,700 --> 00:17:22,700
Let's try this way.

186
00:17:22,700 --> 00:17:23,700
There we go.

187
00:17:23,700 --> 00:17:24,700
There you are.

188
00:17:24,700 --> 00:17:25,700
There.

189
00:17:25,700 --> 00:17:26,700
Thank you so much.

190
00:17:26,700 --> 00:17:29,020
I just want to say thank you for holding this.

191
00:17:29,020 --> 00:17:32,980
I appreciate you guys doing this very out in the open.

192
00:17:32,980 --> 00:17:34,940
That's most outstanding.

193
00:17:34,940 --> 00:17:39,660
I do have a couple of questions and I know some of this has been kind of things you've

194
00:17:39,660 --> 00:17:47,380
reiterated over the past several months as you guys have been pulling the OSAID together.

195
00:17:47,380 --> 00:17:49,480
But two key questions.

196
00:17:49,480 --> 00:17:56,380
So being as it is technically impossible to accurately replicate the exact functionality

197
00:17:56,380 --> 00:18:03,420
of any large language model system without an exact replication of the entire data set,

198
00:18:03,420 --> 00:18:11,220
how can this license be open source and meet the share definition, the share requirement

199
00:18:11,220 --> 00:18:18,460
without 100% data availability as a requirement of the license?

200
00:18:18,460 --> 00:18:24,980
I mean, I know this comes up regularly, but it really hasn't been addressed how that can

201
00:18:24,980 --> 00:18:25,980
actually work.

202
00:18:25,980 --> 00:18:28,660
I would love to hear.

203
00:18:28,660 --> 00:18:35,860
So are you sure that you can actually replicate an LLM given the data?

204
00:18:35,860 --> 00:18:39,540
No, that's a fair argument.

205
00:18:39,540 --> 00:18:43,140
I mean, LLMs have a wide variety of issues with it.

206
00:18:43,140 --> 00:18:48,780
But in theory, you could at least if you have the entirety of the data set and the data

207
00:18:48,780 --> 00:18:51,100
model and all the details.

208
00:18:51,100 --> 00:18:52,940
Are you sure?

209
00:18:52,940 --> 00:18:56,340
You definitely can't if you don't have the data.

210
00:18:56,340 --> 00:18:57,340
Who told you?

211
00:18:57,540 --> 00:19:01,620
Okay, procure the science that says so.

212
00:19:01,620 --> 00:19:02,620
And then we can discuss.

213
00:19:02,620 --> 00:19:03,620
Really?

214
00:19:03,620 --> 00:19:04,620
Because that's not...

215
00:19:04,620 --> 00:19:05,620
Yep.

216
00:19:05,620 --> 00:19:09,620
Well, that's pretty well accepted.

217
00:19:09,620 --> 00:19:16,820
I mean, the way large language models work, data is a core part of the functionality,

218
00:19:16,820 --> 00:19:17,820
right?

219
00:19:17,820 --> 00:19:18,820
I mean, it's...

220
00:19:18,820 --> 00:19:21,820
It's definitely a part of the functionality.

221
00:19:21,820 --> 00:19:23,020
Yeah.

222
00:19:23,020 --> 00:19:24,900
Let me use this as an example.

223
00:19:24,900 --> 00:19:29,100
To move out of the AI space, if you were to make, say, an open source...

224
00:19:29,100 --> 00:19:30,100
Stay in the AI space.

225
00:19:30,100 --> 00:19:34,500
Look, Lunduk, if you go by Lunduk, stay in the AI space.

226
00:19:34,500 --> 00:19:39,100
Do not move out of the AI space because it's only confusing.

227
00:19:39,100 --> 00:19:41,940
And I want to give also time to others if they have questions.

228
00:19:41,940 --> 00:19:42,940
Okay.

229
00:19:42,940 --> 00:19:43,940
All right.

230
00:19:43,940 --> 00:19:45,620
Then a second question then.

231
00:19:45,620 --> 00:19:52,380
Considering that AI systems can be considered open source under the open source AI definition,

232
00:19:52,380 --> 00:19:59,220
under this definition, without providing the dataset, by merely providing the source code

233
00:19:59,220 --> 00:20:07,700
and a description of what data might be, what is the point of the OS AI definition over

234
00:20:07,700 --> 00:20:11,020
simply using the existing open source definition?

235
00:20:11,020 --> 00:20:17,460
Because if you release the source code of the license for your AI system, you are already

236
00:20:17,460 --> 00:20:18,820
OSD.

237
00:20:18,820 --> 00:20:21,620
You are already under the open source definition.

238
00:20:21,620 --> 00:20:29,100
And if you're not releasing any additional content under any sort of open source or related

239
00:20:29,100 --> 00:20:36,380
license, what's the difference between OSD and OSAID in that particular case?

240
00:20:36,380 --> 00:20:38,000
Okay.

241
00:20:38,000 --> 00:20:43,300
So if I understand correctly, if I can rephrase your question to see if I got it right.

242
00:20:43,300 --> 00:20:49,420
You're asking basically, given that if the data is not required or considering the data

243
00:20:49,420 --> 00:20:56,740
as a separate issue, you're basically saying you need to give code and parameters of the

244
00:20:56,740 --> 00:20:58,780
weights, right?

245
00:20:58,780 --> 00:21:05,540
And therefore you can use the OSD only because you can use the OSD to say, well, ignore also

246
00:21:05,540 --> 00:21:06,540
the weights.

247
00:21:06,540 --> 00:21:11,460
You can say, just give me the code under these conditions, the OSAID proof license, right?

248
00:21:11,460 --> 00:21:12,460
Okay.

249
00:21:12,460 --> 00:21:19,420
So the real, really important answer that we have found during this conversation is

250
00:21:19,420 --> 00:21:25,700
that a way to interpret OSD section two, point number two.

251
00:21:25,700 --> 00:21:32,940
The point number two in the open source definition defines the fact that forces says that an

252
00:21:32,940 --> 00:21:39,580
open source software program needs to provide the source code and describes the source code

253
00:21:39,580 --> 00:21:49,380
as the form in which a programmer would make modifications to the program, a software developer.

254
00:21:49,380 --> 00:21:50,460
Okay.

255
00:21:50,460 --> 00:22:00,820
So for AI machine learning LLMs, if I asked you the question three years ago, what is

256
00:22:00,820 --> 00:22:10,540
that you need in order, what code do you actually need to make modifications to an AI machine?

257
00:22:10,540 --> 00:22:15,060
Think LLAMA or think Bloom or Pythia or what have you, right?

258
00:22:15,060 --> 00:22:16,500
What actually do you need?

259
00:22:16,500 --> 00:22:17,860
What kind of code do you need?

260
00:22:17,860 --> 00:22:19,700
There's so much code that goes in there.

261
00:22:19,700 --> 00:22:25,060
There's training code, there is data filtering and processing, there is inference code for

262
00:22:25,060 --> 00:22:26,420
running it.

263
00:22:26,420 --> 00:22:32,740
There are applications going around the fact that you need, there is code for architecture

264
00:22:32,740 --> 00:22:35,140
itself of the model, right?

265
00:22:35,140 --> 00:22:37,900
Which of these do you need?

266
00:22:37,900 --> 00:22:38,900
That's the answer.

267
00:22:38,900 --> 00:22:43,460
The answer is in the open source AI definition, you will find that you need code for filtering

268
00:22:43,460 --> 00:22:45,940
the end processing the data.

269
00:22:45,940 --> 00:22:50,660
You need the code for the inference and you need the code for the architecture of the

270
00:22:50,660 --> 00:22:52,260
model.

271
00:22:52,260 --> 00:22:54,140
That's the answer in there.

272
00:22:54,140 --> 00:23:00,740
And then these pieces are part of the rest of it, like the data and the data details,

273
00:23:00,740 --> 00:23:06,420
the data information and the parameters themselves.

274
00:23:06,420 --> 00:23:17,620
So in the current OSD, that applies to not just say C source code files, but also applies

275
00:23:17,620 --> 00:23:25,300
to configuration files, make files, anything that you can release under that license that

276
00:23:25,300 --> 00:23:29,900
goes into the generation of the software.

277
00:23:29,900 --> 00:23:35,900
So wouldn't that also apply in exactly the same way to AI systems?

278
00:23:35,900 --> 00:23:42,180
I don't see how anything is gained here, I guess is what I'm getting to.

279
00:23:42,180 --> 00:23:43,620
And I don't mean that as a criticism.

280
00:23:43,620 --> 00:23:48,660
I mean that as a, I've been legitimately racking my brain trying to figure out what is gained

281
00:23:48,660 --> 00:23:51,300
here under the OSAID.

282
00:23:51,300 --> 00:23:58,020
If I were to release a large language model system, but not provide the data, there is

283
00:23:58,020 --> 00:24:00,780
functionally no difference.

284
00:24:00,780 --> 00:24:05,700
I could use the OSD or the OSAID and it's really no different.

285
00:24:05,700 --> 00:24:11,020
Now I appreciate that, you know, there's at least discussion around data being provided

286
00:24:11,020 --> 00:24:12,220
in the OSAID.

287
00:24:12,220 --> 00:24:14,100
So that's a good thing.

288
00:24:14,100 --> 00:24:19,980
However, I don't see how without the data, there's any functional difference between

289
00:24:19,980 --> 00:24:20,980
the two.

290
00:24:20,980 --> 00:24:23,820
And I've heard this criticism used repeatedly.

291
00:24:23,820 --> 00:24:28,700
Yeah, I think you're coming at it from the software angle.

292
00:24:28,700 --> 00:24:33,500
From the software angle, you're thinking there is a pipeline, very simple.

293
00:24:33,500 --> 00:24:37,980
There is the source code, there are the building scripts, the files, what have you.

294
00:24:37,980 --> 00:24:42,220
And then there is the final, after the compilation phase, you get binary.

295
00:24:42,220 --> 00:24:46,900
And you're thinking of it into this framework.

296
00:24:46,900 --> 00:24:50,100
For AI machine learning, that's not the framework.

297
00:24:50,100 --> 00:24:56,340
The code, source code, after the compilation becomes a binary and the two are basically

298
00:24:56,340 --> 00:25:01,200
the same artifact seen from different angles.

299
00:25:01,200 --> 00:25:03,580
It's not what's happening here, right?

300
00:25:03,580 --> 00:25:08,860
So if you keep on applying the same framework that works in software into the AI machine

301
00:25:08,860 --> 00:25:13,060
learning space, you will get all of it confused.

302
00:25:13,060 --> 00:25:21,020
With the people who say that the software, the original open source developers, OSD,

303
00:25:21,020 --> 00:25:25,860
the open source definition can be applied, they're actually correct.

304
00:25:25,860 --> 00:25:30,300
We are applying the OSD to evaluate data licenses.

305
00:25:30,300 --> 00:25:35,580
We're going to be evaluating licenses or legal terms, legal agreements.

306
00:25:35,580 --> 00:25:42,300
We're going to be evaluating the OSD also for the weights, the legal terms of services

307
00:25:42,300 --> 00:25:43,620
of the weights.

308
00:25:43,620 --> 00:25:50,420
These pieces, and of course the OSD for the code, these pieces individually have a completely

309
00:25:50,420 --> 00:25:53,860
different meaning in the space of machine learning.

310
00:25:53,860 --> 00:25:59,660
They have an interdependency that the open source definition alone does not explain.

311
00:25:59,660 --> 00:26:06,460
We needed to have this step, this learning package, these learning steps, right?

312
00:26:06,460 --> 00:26:11,340
To understand what is the preferred form of making modifications to a machine learning

313
00:26:11,340 --> 00:26:12,460
system.

314
00:26:12,460 --> 00:26:19,380
Now we have an answer and now we can tell others to ask, what is an open source AI?

315
00:26:19,380 --> 00:26:27,300
We say it's data with training code, with data processing code, with model weights available

316
00:26:27,300 --> 00:26:33,500
under an open source approved license or terms or service or agreements.

317
00:26:33,500 --> 00:26:35,620
I think I get what you're saying on that.

318
00:26:35,620 --> 00:26:37,100
And I don't want to beat a dead horse.

319
00:26:37,100 --> 00:26:42,100
I have one completely different question on the creation of the license.

320
00:26:42,100 --> 00:26:47,380
You've taken a very interesting approach to the creation of the OSA ID.

321
00:26:47,380 --> 00:26:53,340
And that is a dramatic departure from how the original OSD was developed.

322
00:26:53,340 --> 00:26:58,900
The original OSD was primarily written by Bruce Perens with a little bit of input from

323
00:26:58,900 --> 00:27:01,660
a few other additional developers.

324
00:27:01,660 --> 00:27:07,460
And it stayed mostly static since that time with a few caveats.

325
00:27:07,460 --> 00:27:13,140
Whereas the OSA ID you have developed in a kind of a large group by committee sort of

326
00:27:13,140 --> 00:27:15,140
setting.

327
00:27:15,140 --> 00:27:22,940
And you've put a very heavy emphasis on diversity and inclusion and those sorts of things in

328
00:27:22,940 --> 00:27:25,780
the development process.

329
00:27:25,780 --> 00:27:33,420
Considering how successful the original OSD was, that was primarily developed by a singular

330
00:27:33,420 --> 00:27:43,660
individual, why change so dramatically the process to use an entirely different process

331
00:27:43,660 --> 00:27:49,100
that almost has no similarities to the development of the original OSD?

332
00:27:49,100 --> 00:27:51,380
Considering how successful the OSD was.

333
00:27:51,380 --> 00:27:53,860
Yeah, why change it up?

334
00:27:53,860 --> 00:28:01,300
Would you have taken something that I developed by myself in my room and coming out with a

335
00:28:01,300 --> 00:28:02,300
secret text?

336
00:28:02,300 --> 00:28:06,500
Or would you have passed it to the open source world?

337
00:28:06,500 --> 00:28:07,500
That's just it.

338
00:28:07,500 --> 00:28:12,940
The whole open source world really did accept that document that came out of Bruce Perens

339
00:28:12,940 --> 00:28:14,500
and the Debian community and whatnot.

340
00:28:14,500 --> 00:28:15,500
Of course.

341
00:28:15,500 --> 00:28:17,260
And it was a great document.

342
00:28:17,260 --> 00:28:22,540
And I'm not saying that you can't make something decent by committee and whatnot.

343
00:28:22,540 --> 00:28:30,100
But typically, when something works so well the first time, why change it so radically

344
00:28:30,100 --> 00:28:31,100
the second time?

345
00:28:31,100 --> 00:28:32,900
What was the motivation?

346
00:28:32,900 --> 00:28:36,100
Did you feel that the current OSD wasn't developed properly?

347
00:28:36,100 --> 00:28:39,740
What was the motivation of that?

348
00:28:39,740 --> 00:28:41,860
The motivation was very simple.

349
00:28:41,860 --> 00:28:44,460
Those days are not replicable.

350
00:28:44,460 --> 00:28:52,900
Those days were times when there were a few dozens developers, hundreds of developers.

351
00:28:52,900 --> 00:29:01,060
They were coming from a history of sharing and understanding the space.

352
00:29:01,060 --> 00:29:06,500
Regulators and legal requirements, they were not even paying attention.

353
00:29:06,500 --> 00:29:10,460
The lawyers were not even paying attention to what was happening.

354
00:29:10,460 --> 00:29:14,340
It was a whole different time.

355
00:29:14,340 --> 00:29:21,820
And if today, with millions of developers around the world, we would have been accepting

356
00:29:21,820 --> 00:29:28,900
the work of one lone genius coming out at the right time with a definition, all of a

357
00:29:28,900 --> 00:29:32,380
sudden, what acceptance do you think it would have had?

358
00:29:32,380 --> 00:29:35,420
We knew that no one would have said, "Oh, yeah.

359
00:29:35,420 --> 00:29:36,940
Follow that guy."

360
00:29:36,940 --> 00:29:40,740
Whether it's me or anyone else.

361
00:29:40,740 --> 00:29:42,060
It would have been impossible.

362
00:29:42,060 --> 00:29:46,820
So with that in mind, we needed to come up with something else.

363
00:29:46,820 --> 00:29:53,180
Involve the communities, plural, and definitely work with them to understand what do you think

364
00:29:53,180 --> 00:29:56,020
that open source is?

365
00:29:56,020 --> 00:30:00,420
What do you think that open source means for what you are doing today and what you will

366
00:30:00,420 --> 00:30:03,860
be doing in the next few months and years?

367
00:30:03,860 --> 00:30:05,660
But I want to give space to others.

368
00:30:05,660 --> 00:30:06,660
I think we should be -- >> Fair enough.

369
00:30:06,660 --> 00:30:08,660
I appreciate the time.

370
00:30:08,660 --> 00:30:10,060
>> Yeah.

371
00:30:10,060 --> 00:30:11,500
Absolutely.

372
00:30:11,500 --> 00:30:12,500
Anyone else?

373
00:30:12,500 --> 00:30:19,620
I see there is quite some chatter in here.

374
00:30:19,620 --> 00:30:27,820
Tom, go ahead.

375
00:30:27,820 --> 00:30:36,420
Let's see.

376
00:30:36,420 --> 00:30:50,940
I think you should be able now to --

377
00:30:50,940 --> 00:31:08,060
>> Okay.

378
00:31:08,060 --> 00:31:12,940
I just wanted to hear more about the co-design process itself and how it was selected.

379
00:31:12,940 --> 00:31:15,940
But specifically, I think we have heard a bit on that already.

380
00:31:15,940 --> 00:31:23,940
But specifically, how does it work in terms of what is the kind of ceremonies, the artifacts,

381
00:31:23,940 --> 00:31:28,820
how agile you have got, regular sprints and daily stand-ups and things like that.

382
00:31:28,820 --> 00:31:34,900
If you can walk us through how co-design actually works, that would be great.

383
00:31:34,900 --> 00:31:40,060
>> Co-design worked in a way that we asked people to join it.

384
00:31:40,060 --> 00:31:46,340
We made a concerted outreach to get diversity.

385
00:31:46,340 --> 00:31:52,100
And we asked them questions.

386
00:31:52,100 --> 00:31:53,700
It's published on the website.

387
00:31:53,700 --> 00:31:54,700
You can go look at it.

388
00:31:54,700 --> 00:31:55,700
Orbitz source.org/deep dive.

389
00:31:55,700 --> 00:31:56,700
You will find the list.

390
00:31:56,700 --> 00:31:57,700
You will find -- >> I have been through all of that.

391
00:31:57,700 --> 00:31:58,700
I have been through all of the references.

392
00:31:58,700 --> 00:32:07,140
But I can't find anything that actually describes the process or its suitability for creating

393
00:32:07,140 --> 00:32:09,420
a technical standard, which would typically be by --

394
00:32:09,420 --> 00:32:10,420
>> Not a technical standard.

395
00:32:10,420 --> 00:32:11,420
This is not a technical standard.

396
00:32:11,420 --> 00:32:12,420
It's not a technical standard.

397
00:32:12,420 --> 00:32:13,420
It's never supposed to be a technical standard.

398
00:32:13,420 --> 00:32:14,420
>> Let's focus on co-design.

399
00:32:14,420 --> 00:32:23,940
Can you help us to learn and understand co-design?

400
00:32:23,940 --> 00:32:26,260
>> You ask questions.

401
00:32:26,260 --> 00:32:29,100
You get people in a room, virtual or physical.

402
00:32:29,100 --> 00:32:31,460
You ask them questions.

403
00:32:31,460 --> 00:32:36,900
And work with them through different techniques.

404
00:32:36,900 --> 00:32:42,060
Sometimes it's been butcher papers and Post-it notes.

405
00:32:42,060 --> 00:32:45,860
Sometimes it's been more assignments of tasks.

406
00:32:45,860 --> 00:32:47,980
And we ask them to give feedback.

407
00:32:47,980 --> 00:32:52,460
And we reconcile that feedback at the end of the session.

408
00:32:52,460 --> 00:32:53,460
That's it.

409
00:32:53,460 --> 00:32:58,260
>> So the closest thing I found is a list here in the design justice essay that talks

410
00:32:58,260 --> 00:33:01,940
about things like we believe everyone is an expert based on their own lived experience.

411
00:33:01,940 --> 00:33:08,820
But it also says that we see the role of the designer as a facilitator rather than an expert.

412
00:33:08,820 --> 00:33:14,700
And I guess that you're kind of playing the role of facilitating most of the discussions

413
00:33:14,700 --> 00:33:18,100
I realize Mare has in some circumstances.

414
00:33:18,100 --> 00:33:21,820
You're making quite opinionated decisions throughout the process.

415
00:33:21,820 --> 00:33:27,460
Like for example, closing Carsten's proposal for a consensus for 1.0.

416
00:33:27,460 --> 00:33:29,020
How does that reconcile with co-design?

417
00:33:29,020 --> 00:33:35,220
I know you've chosen co-design but it looks like you're not doing what I understand co-design.

418
00:33:35,220 --> 00:33:38,060
>> Because those are happening outside of the co-design sessions.

419
00:33:38,060 --> 00:33:41,380
Those are just feedback sessions in the forum.

420
00:33:41,380 --> 00:33:44,100
It's a different story.

421
00:33:44,100 --> 00:33:49,780
So you're referring now to specific conversations happening on the forum.

422
00:33:49,780 --> 00:33:52,500
That conversation has been closed.

423
00:33:52,500 --> 00:33:55,340
And it cannot be heard anymore.

424
00:33:55,340 --> 00:34:00,940
Because it's reopening a question that has already been achieved, that has already achieved

425
00:34:00,940 --> 00:34:01,940
consensus.

426
00:34:01,940 --> 00:34:05,340
There has already been conversations for months.

427
00:34:05,340 --> 00:34:08,660
>> Not by any kind of reasonable definition of consensus.

428
00:34:08,660 --> 00:34:09,660
>> Right.

429
00:34:09,660 --> 00:34:13,740
There is a minority of vocal opposition.

430
00:34:13,740 --> 00:34:14,740
It's not consensus.

431
00:34:14,740 --> 00:34:19,940
There is a vocal minority opposing the results.

432
00:34:19,940 --> 00:34:28,460
The arguments that have been brought up from people have been -- from this minority are

433
00:34:28,460 --> 00:34:33,860
arguments that have been known for more than -- for years, I would say.

434
00:34:33,860 --> 00:34:38,220
And they all lead to unacceptable conditions.

435
00:34:38,220 --> 00:34:44,300
Either no open source AI or open source AI that is limited in scope.

436
00:34:44,300 --> 00:34:48,860
In specific words, it cannot be applied to medical fields.

437
00:34:48,860 --> 00:34:50,220
For example.

438
00:34:50,220 --> 00:35:00,120
Or they provide only M50 sets of approvable open source AI.

439
00:35:00,120 --> 00:35:08,300
So those conditions, those conversations cannot continue to happen when there is a formed

440
00:35:08,300 --> 00:35:14,900
majority visible in the endorsements page on our website.

441
00:35:14,900 --> 00:35:18,440
And it would be even more visible as we release 1.0.

442
00:35:18,440 --> 00:35:24,380
There is no point in continuing to argue for all data to be open.

443
00:35:24,380 --> 00:35:29,740
That excludes -- reduces the scope of open source.

444
00:35:29,740 --> 00:35:31,740
So without -- >> It's not my intention to have the data

445
00:35:31,740 --> 00:35:32,740
discussion.

446
00:35:32,740 --> 00:35:37,820
It's my intention to have the co-design discussion and really just understand that process.

447
00:35:37,820 --> 00:35:42,380
Because software is increasingly being written by an incorporating AI.

448
00:35:42,380 --> 00:35:45,700
So this is effectively going to be OSD 2.0.

449
00:35:45,700 --> 00:35:47,300
And it's such an important decision.

450
00:35:47,300 --> 00:35:49,580
It's very important for us to understand.

451
00:35:49,580 --> 00:35:52,580
>> Why OSD 2.0?

452
00:35:52,580 --> 00:35:57,700
>> Because increasingly AI and software are merging.

453
00:35:57,700 --> 00:35:59,940
It's why I dropped electrical engineering and went to computing.

454
00:35:59,940 --> 00:36:05,060
I realized electrical engineers were not creating systems that provide any functionality.

455
00:36:05,060 --> 00:36:08,300
They were just life support systems for computers.

456
00:36:08,300 --> 00:36:11,860
And now inference code is just life support systems for a model.

457
00:36:11,860 --> 00:36:16,180
So the model might be working out whether or not you have access to a system.

458
00:36:16,180 --> 00:36:19,060
Everything around it is just kind of the wiring, like DevOps type stuff.

459
00:36:19,060 --> 00:36:22,860
So it is increasingly the case that AI is software.

460
00:36:22,860 --> 00:36:25,740
So the OSA ID is going to be OSD 2.0.

461
00:36:25,740 --> 00:36:33,140
So you are wholesale redefining open source as freeware.

462
00:36:33,140 --> 00:36:35,700
>> I can fully disagree with that assessment.

463
00:36:35,700 --> 00:36:36,780
And not me.

464
00:36:36,780 --> 00:36:42,660
It's just the people who are endorsing this definition don't agree with that assessment.

465
00:36:42,660 --> 00:36:51,420
The definition is providing you all the equivalent that allows you to meaningfully fork, take

466
00:36:51,420 --> 00:36:58,740
apart, build on top, reshape, however you want to call it, an existing system.

467
00:36:58,740 --> 00:37:04,380
If it is an open source AI, you have all the means and all the ability and all the rights

468
00:37:04,380 --> 00:37:12,860
given by the legal agreements to rebuild, replicate, reshape.

469
00:37:12,860 --> 00:37:21,540
I'm not -- with lacking words here to say that you can, given what the open source AI

470
00:37:21,540 --> 00:37:27,180
definition, release candidate one, gives you, you can do what you could do with only source

471
00:37:27,180 --> 00:37:29,980
code and the build scripts.

472
00:37:29,980 --> 00:37:30,980
It's the same thing.

473
00:37:30,980 --> 00:37:37,980
>> I would be opposed to that question.

474
00:37:37,980 --> 00:37:43,260
It's a bit like scratching your name in the back of a toilet door.

475
00:37:43,260 --> 00:37:45,260
It's not particularly useful.

476
00:37:45,260 --> 00:37:46,260
>> Yeah.

477
00:37:46,260 --> 00:37:47,260
Okay.

478
00:37:47,260 --> 00:37:55,860
We'll have more details about the process itself and we'll share more information about

479
00:37:55,860 --> 00:37:56,860
that.

480
00:37:56,860 --> 00:37:57,860
Hopefully it's satisfactory.

481
00:37:57,860 --> 00:37:58,860
Gerardo, you want to --

482
00:37:58,860 --> 00:38:01,860
>> Hello.

483
00:38:01,860 --> 00:38:04,860
>> Hey.

484
00:38:04,860 --> 00:38:07,860
>> Hi.

485
00:38:07,860 --> 00:38:31,860
I would -- sorry for arriving late.

486
00:38:31,860 --> 00:38:38,420
I mixed up the time zones again.

487
00:38:38,420 --> 00:38:46,060
I wanted just to make sure that everybody is on more or less the same page.

488
00:38:46,060 --> 00:38:54,220
That is my position, that the OSI board has all the legitimacy and power to deliver the

489
00:38:54,220 --> 00:39:00,380
document in the form it believes it has to have.

490
00:39:00,380 --> 00:39:06,580
These discussions that we are having and the arguments that we are having are meant to

491
00:39:06,580 --> 00:39:16,100
help OSI and its board to better sustain all the decisions it makes.

492
00:39:16,100 --> 00:39:27,740
In my case, which I endorsed the document a long time ago, even with all its defects,

493
00:39:27,740 --> 00:39:38,060
it is important that the community sees an effort to define what is open source in the

494
00:39:38,060 --> 00:39:40,340
AI context.

495
00:39:40,340 --> 00:39:48,500
That doesn't mean that we endorse all the decisions, but the principle of OSI presenting

496
00:39:48,500 --> 00:39:49,500
something.

497
00:39:49,500 --> 00:39:58,660
Just to make sure that it is also understood that just because you have endorsements,

498
00:39:58,660 --> 00:40:05,180
it doesn't mean that all the closed questions, as you say, are valid.

499
00:40:05,180 --> 00:40:11,560
The thing that I have been asking myself and others is that, okay, for every issue and

500
00:40:11,560 --> 00:40:17,340
argument that has posed, we have to -- and the forum is probably not the best way to

501
00:40:17,340 --> 00:40:18,340
do it.

502
00:40:18,340 --> 00:40:24,300
We have to have the reasoning behind the decisions.

503
00:40:24,300 --> 00:40:29,860
That's why I propose that we should be using something like Git repo with issues for each

504
00:40:29,860 --> 00:40:33,780
question, which would be much easier to follow.

505
00:40:33,780 --> 00:40:43,460
But just for one thing that you just said, and I think it's important to clear, which

506
00:40:43,460 --> 00:40:52,820
is whatever the decision is on the document, we'll never have an empty set.

507
00:40:52,820 --> 00:41:03,340
It's already established that there are enough open source AI projects that would fit even

508
00:41:03,340 --> 00:41:07,460
the strictest definition.

509
00:41:07,460 --> 00:41:16,260
And as we saw with the open source adoption, is that even projects that were very closed

510
00:41:16,260 --> 00:41:25,820
in the past have become more open and then truly open as time has come by because of

511
00:41:25,820 --> 00:41:34,740
the adoption of the open source and the strength that other users, and in this case I'm clearly

512
00:41:34,740 --> 00:41:41,940
defining the governments, have decided that the full open source option is the best way

513
00:41:41,940 --> 00:41:42,940
for them.

514
00:41:42,940 --> 00:41:50,100
That has brought the vendors, the commercial vendors, to say maybe we should change our

515
00:41:50,100 --> 00:41:51,100
code.

516
00:41:51,100 --> 00:41:56,420
And here I'm talking about change our code to be fully open source.

517
00:41:56,420 --> 00:41:59,220
We've seen this happening time by time.

518
00:41:59,220 --> 00:42:02,940
But I'm digressing.

519
00:42:02,940 --> 00:42:09,500
It's important and the AI Act has forced us into this.

520
00:42:09,500 --> 00:42:16,420
The Act and the decisions on the Japanese and Singaporean governments has forced us

521
00:42:16,420 --> 00:42:22,460
into go forward with a new thing.

522
00:42:22,460 --> 00:42:27,380
Anyway, and just to lighten up this.

523
00:42:27,380 --> 00:42:32,180
You have -- >> Just to close this.

524
00:42:32,180 --> 00:42:39,580
Even on the case of things that we think the data is closed and we might not have, I just

525
00:42:39,580 --> 00:42:46,700
shared on the forum that there are data sets of medical data that are actually cleared

526
00:42:46,700 --> 00:42:48,500
for public use.

527
00:42:48,500 --> 00:42:54,100
They have been sufficiently anonymized and vetted and tested and peer reviewed, more

528
00:42:54,100 --> 00:43:06,860
than that, which allows to say maybe the universe of fully open, open source AI systems is not

529
00:43:06,860 --> 00:43:09,300
as small as we think it is.

530
00:43:09,300 --> 00:43:12,780
And so -- >> No, it could change.

531
00:43:12,780 --> 00:43:16,460
>> In a positive mood, okay?

532
00:43:16,460 --> 00:43:22,700
So because there is a strength in going on to the fully open thing.

533
00:43:22,700 --> 00:43:24,420
But okay.

534
00:43:24,420 --> 00:43:34,500
But let's say that even with the current definition and the fact it has too many explanations

535
00:43:34,500 --> 00:43:40,540
and clarifications and redefinitions inside makes it a fragile document.

536
00:43:40,540 --> 00:43:41,660
But okay.

537
00:43:41,660 --> 00:43:51,020
As long as we can sell to the politicians, decisions makers, this is still a work in

538
00:43:51,020 --> 00:43:53,740
progress even if it gets a 1.0.

539
00:43:53,740 --> 00:43:55,420
It's still a work in progress.

540
00:43:55,420 --> 00:43:58,980
It's not a mutable document.

541
00:43:58,980 --> 00:44:04,460
Then we are free to go and to go around.

542
00:44:04,460 --> 00:44:11,620
And just to finish one thing, and that is really the crucial part.

543
00:44:11,620 --> 00:44:14,620
We have definition.

544
00:44:14,620 --> 00:44:18,900
We need process of validation.

545
00:44:18,900 --> 00:44:20,900
And we need a validation tool.

546
00:44:20,900 --> 00:44:25,540
We already have two validation tools that we can base our work with.

547
00:44:25,540 --> 00:44:33,980
So the MOT from the LinkedIn Foundation and the other one from Stanford about the transparency

548
00:44:33,980 --> 00:44:35,060
index.

549
00:44:35,060 --> 00:44:36,700
And we should create them.

550
00:44:36,700 --> 00:44:45,500
And I'm willing to help in creating a process of validation that anyone, especially on the

551
00:44:45,500 --> 00:44:50,300
public administration side, can look into and see how can we make sure that this has

552
00:44:50,300 --> 00:44:51,300
been validated.

553
00:44:51,300 --> 00:44:52,300
That's all.

554
00:44:52,300 --> 00:44:53,780
>> That's very good.

555
00:44:53,780 --> 00:44:58,300
And I take that as a very, very useful comment.

556
00:44:58,300 --> 00:45:01,740
We heard not just from you but others.

557
00:45:01,740 --> 00:45:02,740
And we knew from the beginning.

558
00:45:02,740 --> 00:45:05,540
First, this is version 1.0.

559
00:45:05,540 --> 00:45:07,780
It's very early.

560
00:45:07,780 --> 00:45:10,260
It's stable but needs to be tested.

561
00:45:10,260 --> 00:45:15,380
It needs to be put into production in order to find the issues and bugs.

562
00:45:15,380 --> 00:45:19,740
And to put it into production, we need exactly what you were talking about.

563
00:45:19,740 --> 00:45:25,220
We need a standard operating manual of some sort.

564
00:45:25,220 --> 00:45:27,060
And we know it's a fragile document.

565
00:45:27,060 --> 00:45:32,060
It needs way too many explanations in order to be understood.

566
00:45:32,060 --> 00:45:34,540
And we will need to get there.

567
00:45:34,540 --> 00:45:38,060
I remember the day when the open source definition was introduced.

568
00:45:38,060 --> 00:45:40,340
It was not that simple.

569
00:45:40,340 --> 00:45:44,340
And I challenge anyone to recite the ten points of it.

570
00:45:44,340 --> 00:45:47,300
While the free software definition was so simple.

571
00:45:47,300 --> 00:45:51,540
And four points and everyone can recite them by heart.

572
00:45:51,540 --> 00:45:54,060
So I think we're gonna get there.

573
00:45:54,060 --> 00:46:01,780
And the board, the OSI, but also wider array of the supporters of the endorsers of the

574
00:46:01,780 --> 00:46:04,020
definition are asking for the same thing.

575
00:46:04,020 --> 00:46:05,620
This is first step.

576
00:46:05,620 --> 00:46:07,500
And then we need to continue.

577
00:46:07,500 --> 00:46:14,060
So we'll have some news after the end of October.

578
00:46:14,060 --> 00:46:18,940
After 30s.

579
00:46:18,940 --> 00:46:19,940
Any other questions?

580
00:46:19,940 --> 00:46:21,940
We have ten minutes left.

581
00:46:21,940 --> 00:46:26,420
Yes, you're right, Nick.

582
00:46:26,420 --> 00:46:34,300
25 years and we still have a lot to explain to do about open source itself.

583
00:46:34,300 --> 00:46:38,700
So it's not a done deal.

584
00:46:38,700 --> 00:46:43,300
Especially in many, many areas.

585
00:46:43,300 --> 00:46:47,420
Let's see.

586
00:46:47,420 --> 00:46:58,420
Karan, were you asking a question?

587
00:46:58,420 --> 00:47:00,980
Oh, okay.

588
00:47:00,980 --> 00:47:02,580
All right.

589
00:47:02,580 --> 00:47:08,340
If there are no more questions, I think we can let you all go and gain another ten minutes

590
00:47:08,340 --> 00:47:10,140
of your life.

591
00:47:10,140 --> 00:47:12,460
And we're gonna see you soon.

592
00:47:12,460 --> 00:47:16,060
Hope to see many of your faces also at all things open.

593
00:47:16,060 --> 00:47:18,660
We're gonna have a celebration.

594
00:47:18,660 --> 00:47:22,940
And we're gonna have to put on our working boots.

595
00:47:22,940 --> 00:47:24,820
We're gonna have to keep our working boots.

596
00:47:24,820 --> 00:47:30,140
Because there's still gonna be a lot of work to be done after 1.0 has been released.

597
00:47:30,140 --> 00:47:34,740
Thanks, everyone, for all of the comments and participation.

598
00:47:34,740 --> 00:47:35,900
Bye-bye.

599
00:47:35,900 --> 00:47:36,900
Bye.

600
00:47:36,900 --> 00:47:36,900
Bye.

601
00:47:36,900 --> 00:47:37,900
Bye.

602
00:47:37,900 --> 00:47:37,900
Bye.

603
00:47:37,900 --> 00:47:42,900
Bye.

