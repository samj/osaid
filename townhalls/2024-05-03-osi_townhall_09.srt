1
00:00:00,001 --> 00:00:07,000
>> Hello, everyone.

2
00:00:07,000 --> 00:00:10,000
>> Hi, welcome.

3
00:00:10,000 --> 00:00:15,000
>> Do you want to take it now?

4
00:00:15,000 --> 00:00:18,000
>> I will.

5
00:00:18,000 --> 00:00:21,000
>> It's becoming a tradition that we tag team on this one.

6
00:00:21,000 --> 00:00:22,000
>> Yeah.

7
00:00:22,000 --> 00:00:24,000
Teamwork.

8
00:00:24,000 --> 00:00:30,000
>> So, yes, welcome to the online public town hall for the

9
00:00:30,000 --> 00:00:33,000
open source AI definition for May 3rd.

10
00:00:33,000 --> 00:00:36,000
We always go through our community agreements.

11
00:00:36,000 --> 00:00:40,000
Those who have been here before are familiar, but just to

12
00:00:40,000 --> 00:00:44,000
remember one mic, one speaker, take space, make space.

13
00:00:44,000 --> 00:00:48,000
So if you ask a question, wait for others to ask before you

14
00:00:48,000 --> 00:00:50,000
ask another one.

15
00:00:50,000 --> 00:00:54,000
If you don't want to ask a question, pause and let others

16
00:00:54,000 --> 00:00:56,000
take a chance to speak.

17
00:00:56,000 --> 00:01:00,000
And if you don't usually speak in a public venue, we invite

18
00:01:00,000 --> 00:01:03,000
you to say what's on your mind.

19
00:01:03,000 --> 00:01:08,000
Kindness, just that we -- the work is hard, but that we be

20
00:01:08,000 --> 00:01:10,000
gentle with each other.

21
00:01:10,000 --> 00:01:15,000
And obviously hate speech is not permitted in this space.

22
00:01:15,000 --> 00:01:17,000
Forward motion.

23
00:01:17,000 --> 00:01:20,000
We start by focusing on what's possible.

24
00:01:20,000 --> 00:01:24,000
And we note obstacles and come back to them, but we reroute

25
00:01:24,000 --> 00:01:27,000
around them and do what is possible in the moment.

26
00:01:27,000 --> 00:01:29,000
We seek solutions.

27
00:01:29,000 --> 00:01:33,000
And we know that that is vulnerable, but it is crucial and

28
00:01:33,000 --> 00:01:36,000
that we are all needed in this work.

29
00:01:36,000 --> 00:01:41,000
And are there any other -- any other community agreements that

30
00:01:41,000 --> 00:01:46,000
we have for this meeting today?

31
00:01:46,000 --> 00:01:48,000
Okay.

32
00:01:48,000 --> 00:01:53,000
So -- playing with my windows one moment.

33
00:01:53,000 --> 00:01:54,000
Okay.

34
00:01:54,000 --> 00:01:56,000
There we go.

35
00:01:56,000 --> 00:01:57,000
Okay.

36
00:01:57,000 --> 00:02:01,000
So, yes, as you all know, our objective for 2024 is to have a

37
00:02:01,000 --> 00:02:05,000
stable version of the open source AI definition and that is

38
00:02:05,000 --> 00:02:07,000
still scheduled for October.

39
00:02:07,000 --> 00:02:09,000
Where are we now?

40
00:02:09,000 --> 00:02:12,000
We have a couple slides that are similar to our last meeting,

41
00:02:12,000 --> 00:02:15,000
but the last meeting was in a different time zone, so I'm

42
00:02:15,000 --> 00:02:18,000
hoping this won't be redundant for anyone.

43
00:02:18,000 --> 00:02:22,000
We are on version 0.0.8, which is feature complete.

44
00:02:22,000 --> 00:02:24,000
It's our feature complete version.

45
00:02:24,000 --> 00:02:28,000
So we have the preamble, the four freedoms, which have been

46
00:02:28,000 --> 00:02:30,000
here for a while.

47
00:02:30,000 --> 00:02:34,000
And then we now have a checklist, legal checklist for

48
00:02:34,000 --> 00:02:38,000
required and optional components that has the required legal

49
00:02:38,000 --> 00:02:40,000
checklist for every single component.

50
00:02:40,000 --> 00:02:44,000
So that has been the new content for this version.

51
00:02:44,000 --> 00:02:47,000
And very briefly, how did we get here?

52
00:02:47,000 --> 00:02:52,000
I see there's chat, and I will trust Stefano and Nick to pause

53
00:02:52,000 --> 00:02:56,000
me if I need to attend to that.

54
00:02:56,000 --> 00:03:00,000
So we started with the four freedoms in the fall of 2023.

55
00:03:00,000 --> 00:03:03,000
I know some of you are actually part of this process.

56
00:03:03,000 --> 00:03:06,000
And this was study, use, modify, and share.

57
00:03:06,000 --> 00:03:10,000
What should these open source principles mean for AI?

58
00:03:10,000 --> 00:03:15,000
And so we had in-person co-design workshops.

59
00:03:15,000 --> 00:03:18,000
We had one at All Things Open.

60
00:03:18,000 --> 00:03:22,000
We had one at Linux Foundation Member Summit.

61
00:03:22,000 --> 00:03:26,000
We had one at the Digital Public Goods Alliance member meeting

62
00:03:26,000 --> 00:03:28,000
in Addis Ababa.

63
00:03:28,000 --> 00:03:32,000
And from that, we came up with the definitions of the four

64
00:03:32,000 --> 00:03:34,000
freedoms for AI.

65
00:03:34,000 --> 00:03:38,000
So these are in 0.0.8.

66
00:03:38,000 --> 00:03:43,000
And they should not surprise anyone, but we've now formalized

67
00:03:43,000 --> 00:03:45,000
them for this technological context.

68
00:03:45,000 --> 00:03:50,000
So we have use the system for any purpose without asking permission,

69
00:03:50,000 --> 00:03:54,000
study the system and inspect its components, modify for any purpose,

70
00:03:54,000 --> 00:03:59,000
and then also to use with or without modification, again,

71
00:03:59,000 --> 00:04:02,000
for any purpose.

72
00:04:02,000 --> 00:04:05,000
I actually will pause for comments at this point.

73
00:04:05,000 --> 00:04:07,000
Let's see.

74
00:04:07,000 --> 00:04:08,000
Yep.

75
00:04:08,000 --> 00:04:09,000
Oh, I see.

76
00:04:09,000 --> 00:04:10,000
Great.

77
00:04:10,000 --> 00:04:15,000
So in the winter, we asked this very big question.

78
00:04:15,000 --> 00:04:19,000
What components must be open in order for an AI system to be used,

79
00:04:19,000 --> 00:04:22,000
studied, modified, and shared, of which there are many,

80
00:04:22,000 --> 00:04:27,000
many different opinions and many, many valid opinions?

81
00:04:27,000 --> 00:04:31,000
And we started doing this, again, by having in-person workshops.

82
00:04:31,000 --> 00:04:35,000
We were at AI Dev in December in San Jose.

83
00:04:35,000 --> 00:04:38,000
And then we got some really good feedback,

84
00:04:38,000 --> 00:04:44,000
which is that working in person only is exclusionary because not

85
00:04:44,000 --> 00:04:46,000
everyone can be in the room.

86
00:04:46,000 --> 00:04:52,000
And so we shifted into virtual work groups in January of this year.

87
00:04:52,000 --> 00:04:57,000
And we also decided to have our work groups focus on specific AI

88
00:04:57,000 --> 00:05:02,000
systems self-described as open so that we could get specific in all

89
00:05:02,000 --> 00:05:06,000
these different questions and debates around open-source AI.

90
00:05:06,000 --> 00:05:10,000
And we're basically looking for systems that represent a diversity of

91
00:05:10,000 --> 00:05:12,000
approaches to AI openness.

92
00:05:12,000 --> 00:05:18,000
And we chose Wama2, Bloom, Pythia, and OpenCV.

93
00:05:18,000 --> 00:05:24,000
And then we recruited members for these groups.

94
00:05:24,000 --> 00:05:28,000
And we were very concerned about having global representation.

95
00:05:28,000 --> 00:05:31,000
So we conducted specific outreach to Black, Indigenous,

96
00:05:31,000 --> 00:05:34,000
and other people of color, particularly women and individuals

97
00:05:34,000 --> 00:05:36,000
from the global South.

98
00:05:36,000 --> 00:05:40,000
And part of being a member of the work group is to have your name and

99
00:05:40,000 --> 00:05:44,000
affiliation shared publicly for the sake of transparency.

100
00:05:44,000 --> 00:05:48,000
And over 50% of work group participants are people of color.

101
00:05:48,000 --> 00:05:50,000
And you can see them here.

102
00:05:50,000 --> 00:05:53,000
And, again, the deck will be available on the website.

103
00:05:53,000 --> 00:05:58,000
So the first thing that the work groups did is they selected the required

104
00:05:58,000 --> 00:05:59,000
components.

105
00:05:59,000 --> 00:06:02,000
So we started with the model openness framework,

106
00:06:02,000 --> 00:06:07,000
which was created by Matt White and colleagues at Linux Foundation.

107
00:06:07,000 --> 00:06:14,000
And you can see those components from that paper on the left,

108
00:06:14,000 --> 00:06:17,000
things like data pre-processing code, training code, evaluation code.

109
00:06:17,000 --> 00:06:22,000
And then there's also model and data components as well that aren't shown.

110
00:06:22,000 --> 00:06:27,000
And then we had work group members vote, initialed vote.

111
00:06:27,000 --> 00:06:33,000
So that's a public vote of is this component required to use or study or

112
00:06:33,000 --> 00:06:36,000
modify or share the system as a whole.

113
00:06:36,000 --> 00:06:40,000
And then I, and this is also public,

114
00:06:40,000 --> 00:06:47,000
created a Likert scale based on the number of votes per component.

115
00:06:47,000 --> 00:06:51,000
And we came up with certain components that were, yes,

116
00:06:51,000 --> 00:06:53,000
this definitely should be required down to no,

117
00:06:53,000 --> 00:06:56,000
there really aren't many votes saying that this should be required.

118
00:06:56,000 --> 00:07:02,000
And then that, that results of that Likert scale, the required, likely,

119
00:07:02,000 --> 00:07:04,000
maybe, probably not, not required,

120
00:07:04,000 --> 00:07:09,000
but into a public forum post for further feedback.

121
00:07:09,000 --> 00:07:14,000
And that became version 0.0.6 of the definition.

122
00:07:14,000 --> 00:07:18,000
So that is the first version of the definition where we actually said,

123
00:07:18,000 --> 00:07:21,000
these are the components that we think should be required for something to be

124
00:07:21,000 --> 00:07:23,000
open source.

125
00:07:23,000 --> 00:07:28,000
And what we just finished early spring was a second activity of the work

126
00:07:28,000 --> 00:07:29,000
groups,

127
00:07:29,000 --> 00:07:33,000
which is to look at the legal documents for these required components and see

128
00:07:33,000 --> 00:07:35,000
are the,

129
00:07:35,000 --> 00:07:39,000
are there legal documents in these systems that are associated with these

130
00:07:39,000 --> 00:07:41,000
components as described?

131
00:07:41,000 --> 00:07:46,000
So we had the required components, which are, there's code components.

132
00:07:46,000 --> 00:07:49,000
As you can see a couple of model components and then a number of data

133
00:07:49,000 --> 00:07:52,000
information or in this earlier version,

134
00:07:52,000 --> 00:07:56,000
data documentation components.

135
00:07:56,000 --> 00:08:01,000
And then we asked volunteers to find links to the documents or licenses that

136
00:08:01,000 --> 00:08:05,000
reference the rights to access and use those components.

137
00:08:05,000 --> 00:08:08,000
And then to evaluate, to look through the document and say,

138
00:08:08,000 --> 00:08:14,000
is use restricted or allowed is modification restricted or allowed is sharing

139
00:08:14,000 --> 00:08:20,000
restricted or allowed for all these different components.

140
00:08:20,000 --> 00:08:23,000
And these are the, so, and this is just a zoom in on the result,

141
00:08:23,000 --> 00:08:27,000
which is 0.0.8. As you saw in the beginning, we have,

142
00:08:27,000 --> 00:08:31,000
we are calling it data information now.

143
00:08:31,000 --> 00:08:34,000
And so data sets are not required,

144
00:08:34,000 --> 00:08:38,000
but information about training and methodologies, scope,

145
00:08:38,000 --> 00:08:42,000
and characteristics provenance labeling procedures,

146
00:08:42,000 --> 00:08:47,000
if used and cleaning methodology is required.

147
00:08:47,000 --> 00:08:51,000
And then we have code components, data, pre-processing code,

148
00:08:51,000 --> 00:08:55,000
training, validation, and testing inference and supporting libraries and

149
00:08:55,000 --> 00:09:00,000
tools. And then two model components, architecture and parameters.

150
00:09:00,000 --> 00:09:01,000
And you can see on the right,

151
00:09:01,000 --> 00:09:07,000
the legal frameworks that we're using for each and then a longer list of

152
00:09:07,000 --> 00:09:08,000
optional components.

153
00:09:08,000 --> 00:09:13,000
This is basically the remainder of the model openness framework components

154
00:09:13,000 --> 00:09:19,000
because obviously we want as many components as possible to be open,

155
00:09:19,000 --> 00:09:24,000
but these are not required. So then, so basically all these data sets,

156
00:09:24,000 --> 00:09:27,000
we say we would love if they're available,

157
00:09:27,000 --> 00:09:31,000
but they are not required to for the system to be called open according to

158
00:09:31,000 --> 00:09:34,000
our definition, additional code elements model.

159
00:09:34,000 --> 00:09:36,000
I see for some reason, I didn't put a highlight on model,

160
00:09:36,000 --> 00:09:39,000
but you can see it in there and other, which is for example,

161
00:09:39,000 --> 00:09:43,000
a research paper or a technical report.

162
00:09:43,000 --> 00:09:45,000
And then just again,

163
00:09:45,000 --> 00:09:49,000
that it's very important that we have a representative process.

164
00:09:49,000 --> 00:09:54,000
It's a global definition. And so this requires global consultation and we

165
00:09:54,000 --> 00:10:00,000
have various stakeholder groups that we're,

166
00:10:00,000 --> 00:10:02,000
that as part of our outreach,

167
00:10:02,000 --> 00:10:06,000
particularly as we move into the next phases of the project and I can go back

168
00:10:06,000 --> 00:10:11,000
to this if people are interested in it,

169
00:10:11,000 --> 00:10:15,000
most involved in the current phase are system creators and licensed creators

170
00:10:15,000 --> 00:10:22,000
and licensees. So people seeking to study, use, modify, and share the system.

171
00:10:22,000 --> 00:10:29,000
And then to increase, I guess, identity-based diversity.

172
00:10:29,000 --> 00:10:35,000
We, I'm using phrases like this one that are specifically inviting,

173
00:10:35,000 --> 00:10:38,000
for example, Black, Indigenous, Latina, other people of color, women,

174
00:10:38,000 --> 00:10:41,000
you can read that paragraph.

175
00:10:41,000 --> 00:10:43,000
And then we also do outreach,

176
00:10:43,000 --> 00:10:49,000
specific outreach to bring underrepresented groups into the process.

177
00:10:49,000 --> 00:10:52,000
So next steps, spring through fall.

178
00:10:52,000 --> 00:10:57,000
So now through October, basically. Right now we're doing definition validation.

179
00:10:57,000 --> 00:11:02,000
So we are seeking volunteers to review. It says one to three.

180
00:11:02,000 --> 00:11:06,000
This is not accurate anymore because now we've decided that we want to do about

181
00:11:06,000 --> 00:11:10,000
10 systems total. So it's actually about six additional systems.

182
00:11:10,000 --> 00:11:17,000
Using that same procedure as of the spreadsheet.

183
00:11:17,000 --> 00:11:22,000
So finding the legal document and doing the analysis of each component,

184
00:11:22,000 --> 00:11:25,000
according to these definitions of study, use, modify, and share.

185
00:11:25,000 --> 00:11:29,000
And we're hoping to have that complete by the 20th.

186
00:11:29,000 --> 00:11:32,000
So in about two and a half weeks.

187
00:11:32,000 --> 00:11:37,000
And these are the systems that we're currently looking for to review.

188
00:11:37,000 --> 00:11:42,000
And we have at least one volunteer for everything except Arctic,

189
00:11:42,000 --> 00:11:45,000
Snowflake Arctic, which is a new addition.

190
00:11:45,000 --> 00:11:50,000
But additional volunteers are welcome on all systems. It's a big task.

191
00:11:50,000 --> 00:11:56,000
So I'm sure that Casey and Mark and Victor and Jan and Racine would love to

192
00:11:56,000 --> 00:12:02,000
have a pal to share that review task with.

193
00:12:02,000 --> 00:12:05,000
And you can email me if you're interested in that,

194
00:12:05,000 --> 00:12:09,000
being a volunteer on any of those systems.

195
00:12:09,000 --> 00:12:16,000
Again, to test the definition against the documentation available for these

196
00:12:16,000 --> 00:12:20,000
systems, the legal documents and licenses.

197
00:12:20,000 --> 00:12:26,000
And our timeline for the rest of the year, we did just release 0.0.8.

198
00:12:26,000 --> 00:12:31,000
We may do a 0.0.9, depending on if there are changes that come back,

199
00:12:31,000 --> 00:12:36,000
major changes that come back from this definition validation activity we're

200
00:12:36,000 --> 00:12:38,000
currently engaged in.

201
00:12:38,000 --> 00:12:43,000
We will have a virtual launch event associated with the release of RC1 in

202
00:12:43,000 --> 00:12:46,000
June, and that date will be TBD.

203
00:12:46,000 --> 00:12:50,000
We had been thinking of doing something in person, and then we thought

204
00:12:50,000 --> 00:12:53,000
inclusion, this is a very important event.

205
00:12:53,000 --> 00:12:56,000
We want as many people to be able to participate as possible.

206
00:12:56,000 --> 00:12:58,000
So we decided to do virtual.

207
00:12:58,000 --> 00:13:02,000
And then we will have a stable version released in October,

208
00:13:02,000 --> 00:13:03,000
and that will be in person.

209
00:13:03,000 --> 00:13:08,000
That will be at All Things Open in North Carolina.

210
00:13:08,000 --> 00:13:12,000
And we have additional virtual and in-person meetings where we'll be

211
00:13:12,000 --> 00:13:18,000
sharing and seeking feedback on, I guess, RC1, release candidate 1.

212
00:13:18,000 --> 00:13:22,000
You can see where we'll be.

213
00:13:22,000 --> 00:13:25,000
And some events, you can see we have the month but not the date.

214
00:13:25,000 --> 00:13:32,000
So we're still working on certain details of this road show.

215
00:13:32,000 --> 00:13:38,000
So, yes, we would love for you to be on the public forum discussed at

216
00:13:38,000 --> 00:13:41,000
opensource.org, and I think that's the QR code.

217
00:13:41,000 --> 00:13:45,000
We'll take you to that site, which you can join for free or become a

218
00:13:45,000 --> 00:13:46,000
paid member.

219
00:13:46,000 --> 00:13:50,000
And we have these biweekly town halls, which you're at right now, so you

220
00:13:50,000 --> 00:13:51,000
know about those.

221
00:13:51,000 --> 00:13:55,000
And then if you're interested in volunteering for definition validation,

222
00:13:55,000 --> 00:14:00,000
you can email me or you can direct message me on the -- on that discussed

223
00:14:00,000 --> 00:14:01,000
platform.

224
00:14:01,000 --> 00:14:03,000
So thank you.

225
00:14:03,000 --> 00:14:10,000
And, yes, Stefano, do you have anything to say before we do the Q&A?

226
00:14:10,000 --> 00:14:12,000
>> I don't know.

227
00:14:12,000 --> 00:14:14,000
Maybe we can share the news.

228
00:14:14,000 --> 00:14:16,000
Shall we?

229
00:14:16,000 --> 00:14:18,000
>> Oh, yes, Stefano.

230
00:14:18,000 --> 00:14:23,000
>> Well, you know, we got -- we have received a grant from the Sloan

231
00:14:23,000 --> 00:14:25,000
Foundation.

232
00:14:25,000 --> 00:14:30,000
So we'll be doing a lot of the -- we're well positioned to have a lot of

233
00:14:30,000 --> 00:14:35,000
participation, a lot of those travel, a lot of those trips to those events

234
00:14:35,000 --> 00:14:37,000
will be supported by this program.

235
00:14:37,000 --> 00:14:40,000
Maybe you can go back.

236
00:14:40,000 --> 00:14:43,000
I had something also I wanted to mention.

237
00:14:43,000 --> 00:14:46,000
Go to slide 28.

238
00:14:46,000 --> 00:14:49,000
I don't know why I remembered this in my head.

239
00:14:49,000 --> 00:14:54,000
I said -- mental note that I wanted to say something here.

240
00:14:54,000 --> 00:14:56,000
Oh, yes.

241
00:14:56,000 --> 00:15:01,000
So here you can see how the legal frameworks in the column legal

242
00:15:01,000 --> 00:15:06,000
frameworks, we have the legal frameworks, the legal frameworks, the

243
00:15:06,000 --> 00:15:12,000
legal frameworks, we have -- we talk about data information available

244
00:15:12,000 --> 00:15:15,000
under OSD compliant license.

245
00:15:15,000 --> 00:15:23,000
And when you look at the code, like inference or data preprocessing, you

246
00:15:23,000 --> 00:15:28,000
see that the legal framework is available under OSI approved license.

247
00:15:28,000 --> 00:15:33,000
And then if you look at the model -- oh, no, yes.

248
00:15:33,000 --> 00:15:37,000
Parameters -- oh, it's cut.

249
00:15:37,000 --> 00:15:48,000
Available under OSD -- parameter says OSD conformant terms.

250
00:15:48,000 --> 00:15:57,000
I think it's interesting here because code, we know that is licensed -- I

251
00:15:57,000 --> 00:16:00,000
mean, we know the licenses, we know the legal frameworks, we know it's

252
00:16:00,000 --> 00:16:04,000
copyright mostly, there is some patent issue, but we've been using OSI

253
00:16:04,000 --> 00:16:06,000
approved license for a long time.

254
00:16:06,000 --> 00:16:10,000
So straightforward, we don't have any problem.

255
00:16:10,000 --> 00:16:18,000
Judging whether the code component are suitable, are available under the

256
00:16:18,000 --> 00:16:20,000
open source principles.

257
00:16:20,000 --> 00:16:26,000
For data information, which is mostly documentation, it's a little bit

258
00:16:26,000 --> 00:16:34,000
fuzzy, but still we can debate -- we know we can identify -- there is no

259
00:16:34,000 --> 00:16:36,000
definition of open documentation.

260
00:16:36,000 --> 00:16:43,000
But I think we can easily identify licenses that give us the possibility

261
00:16:43,000 --> 00:16:49,000
to read the documentation, modify, and redistribute to others.

262
00:16:49,000 --> 00:16:56,000
So that's why we're using the term OSD compliant license.

263
00:16:56,000 --> 00:17:01,000
Although there is a question whether we should use OSD compliant or OSD

264
00:17:01,000 --> 00:17:03,000
compatible.

265
00:17:03,000 --> 00:17:08,000
So compatible with the open source definition or compliant with the open

266
00:17:08,000 --> 00:17:10,000
source definition.

267
00:17:10,000 --> 00:17:13,000
So it's a little bit of a debate and it's ongoing on the forum.

268
00:17:13,000 --> 00:17:18,000
I encourage you to go find the thread and vote.

269
00:17:18,000 --> 00:17:21,000
There is actually a little poll in there.

270
00:17:21,000 --> 00:17:27,000
But the model parameters is interesting because model parameters don't seem

271
00:17:27,000 --> 00:17:30,000
to fall under copyright law.

272
00:17:30,000 --> 00:17:38,000
And there is discussion whether any other exclusive rights apply in

273
00:17:38,000 --> 00:17:40,000
different jurisdictions.

274
00:17:40,000 --> 00:17:47,000
So it's different between like Europe, United States, China, UK.

275
00:17:47,000 --> 00:17:50,000
There is still a little bit of a debate.

276
00:17:50,000 --> 00:17:55,000
So that's why we're using a more generic term that says OSD conformant.

277
00:17:55,000 --> 00:18:00,000
So we want to have -- leave the flexibility to interpret how the

278
00:18:00,000 --> 00:18:04,000
principles or how these parameters are distributed.

279
00:18:04,000 --> 00:18:09,000
Until the dust settles on the legal point of view.

280
00:18:09,000 --> 00:18:14,000
These are the questions -- these are the points that I think will have

281
00:18:14,000 --> 00:18:17,000
to be clarified in the next few weeks during the validation process

282
00:18:17,000 --> 00:18:20,000
among other things.

283
00:18:20,000 --> 00:18:24,000
So if you have opinions, if you know someone who might have opinions,

284
00:18:24,000 --> 00:18:28,000
now is the time to go and check the document.

285
00:18:28,000 --> 00:18:33,000
Because the definition, the draft 08, 008 is feature complete.

286
00:18:33,000 --> 00:18:35,000
It has all the elements that we want.

287
00:18:35,000 --> 00:18:40,000
And if no one really objects or if there are no strong pushback, I think

288
00:18:40,000 --> 00:18:45,000
it's going to be -- the release candidate is going to look very similar

289
00:18:45,000 --> 00:18:48,000
to what we have now.

290
00:18:48,000 --> 00:18:56,000
So any questions, I'm happy to -- me and Mer here are happy to answer.

291
00:18:56,000 --> 00:18:58,000
>> A question on the GDPR.

292
00:18:58,000 --> 00:19:00,000
I can read it.

293
00:19:00,000 --> 00:19:03,000
Shall I read it for the recording or Jan, do you want to come off mute?

294
00:19:03,000 --> 00:19:06,000
>> Yeah, go ahead.

295
00:19:06,000 --> 00:19:08,000
>> I can unmute.

296
00:19:08,000 --> 00:19:12,000
So there's been questions about like when it's -- a model is generating

297
00:19:12,000 --> 00:19:18,000
results that someone is saying, oh, this isn't -- I want to remove this

298
00:19:18,000 --> 00:19:20,000
from this.

299
00:19:20,000 --> 00:19:23,000
From your results according to GDPR.

300
00:19:23,000 --> 00:19:28,000
Wouldn't it be very nice to also have a possibility to do it?

301
00:19:28,000 --> 00:19:32,000
Because if you just have the model, you cannot really remove it.

302
00:19:32,000 --> 00:19:36,000
Because you have to remove it from the training data and then retrain to

303
00:19:36,000 --> 00:19:38,000
get a new thing.

304
00:19:38,000 --> 00:19:40,000
>> Yeah.

305
00:19:40,000 --> 00:19:42,000
I -- yes.

306
00:19:42,000 --> 00:19:46,000
But that is such a big question in fact.

307
00:19:46,000 --> 00:19:55,000
And I'm really curious to see what the results of the lawsuit that just

308
00:19:55,000 --> 00:19:58,000
happened against open AI.

309
00:19:58,000 --> 00:20:02,000
Granted that is a closed, opaque, we don't know what's happening inside

310
00:20:02,000 --> 00:20:04,000
there.

311
00:20:04,000 --> 00:20:09,000
The fact that the source data is not required is because it's exactly for

312
00:20:09,000 --> 00:20:11,000
this reason.

313
00:20:11,000 --> 00:20:15,000
So if there is private information in the source data sets, those -- that

314
00:20:15,000 --> 00:20:17,000
data set cannot be distributed legally.

315
00:20:17,000 --> 00:20:21,000
So that's why we want to know what's going in there.

316
00:20:21,000 --> 00:20:26,000
You know, we went into the training data set.

317
00:20:26,000 --> 00:20:28,000
We want to know exactly how it was filtered.

318
00:20:28,000 --> 00:20:31,000
You know, the duplication, all that thing.

319
00:20:31,000 --> 00:20:35,000
That's why there is a requirement on data preprocessing in the code

320
00:20:35,000 --> 00:20:37,000
section.

321
00:20:37,000 --> 00:20:39,000
Because that's what it means.

322
00:20:39,000 --> 00:20:43,000
You should -- you know, sometimes it's very valuable also to know exactly

323
00:20:43,000 --> 00:20:50,000
how to have the exact same instruments that went into the training.

324
00:20:50,000 --> 00:20:57,000
Building the data set for the training so that one can retrain the model

325
00:20:57,000 --> 00:21:01,000
and/or readjust the parameters with their own data.

326
00:21:01,000 --> 00:21:03,000
Or similar data.

327
00:21:03,000 --> 00:21:10,000
In fact, the exact spelling of the data information requirement is -- says

328
00:21:10,000 --> 00:21:16,000
it needs to be sufficiently detailed information about the data used to

329
00:21:16,000 --> 00:21:22,000
train the system so that a skilled person can recreate a substantially

330
00:21:22,000 --> 00:21:27,000
equivalent system using the same or similar data.

331
00:21:27,000 --> 00:21:30,000
We hope that this solves the problem.

332
00:21:30,000 --> 00:21:35,000
And if -- honestly, yes, if there is GDPR data inside the data set and the

333
00:21:35,000 --> 00:21:42,000
model itself has it, then you should be able to rebuild the -- with your own

334
00:21:42,000 --> 00:21:47,000
data without it.

335
00:21:47,000 --> 00:21:51,000
And maybe that model itself is illegal, right?

336
00:21:51,000 --> 00:21:53,000
In some legislation.

337
00:21:53,000 --> 00:22:01,000
So there is a higher standard that applies here.

338
00:22:01,000 --> 00:22:03,000
>> Thanks.

339
00:22:03,000 --> 00:22:05,000
Any other questions?

340
00:22:05,000 --> 00:22:10,000
You can chat or take yourself off mute.

341
00:22:10,000 --> 00:22:37,000
If anyone has any other questions or thoughts.

342
00:22:37,000 --> 00:22:41,000
>> I can come back on and just ask a little bit of a background question

343
00:22:41,000 --> 00:22:44,000
here because I missed a couple of town halls.

344
00:22:44,000 --> 00:22:56,000
So what is the idea of not requiring the training data in full and just have

345
00:22:56,000 --> 00:22:58,000
the opaque model?

346
00:22:58,000 --> 00:23:04,000
Because that sort of seems to me to be counterintuitive to the spirit of the

347
00:23:04,000 --> 00:23:07,000
open source definition.

348
00:23:07,000 --> 00:23:12,000
And I understand for the legality, but one can also say, well, all open

349
00:23:12,000 --> 00:23:16,000
source models should be legal.

350
00:23:16,000 --> 00:23:20,000
Is that the only thing or is there something more as well?

351
00:23:20,000 --> 00:23:24,000
>> No, I think there is something more.

352
00:23:24,000 --> 00:23:33,000
So many -- the fact that data set did not make it into the draft 06, was it?

353
00:23:33,000 --> 00:23:41,000
Is because the working group as they were evaluating the various systems,

354
00:23:41,000 --> 00:23:47,000
not enough of them put the requirement of the original data set as strictly

355
00:23:47,000 --> 00:23:52,000
required to modify, study, use, and share.

356
00:23:52,000 --> 00:23:59,000
So they ranked higher other -- like the training information.

357
00:23:59,000 --> 00:24:09,000
Like the training code, the documentation on the data used were ranked

358
00:24:09,000 --> 00:24:10,000
higher.

359
00:24:10,000 --> 00:24:16,000
And so because there are so many machine learning systems that don't have

360
00:24:16,000 --> 00:24:24,000
the original data set, like it just doesn't exist because it's not created.

361
00:24:24,000 --> 00:24:33,000
Like for anything that is used with -- learning and other ways that are

362
00:24:33,000 --> 00:24:35,000
privacy preserving.

363
00:24:35,000 --> 00:24:39,000
Then there is that question of private data.

364
00:24:39,000 --> 00:24:47,000
Where it might go -- I mean, it may be part -- I mean, technically part of

365
00:24:47,000 --> 00:24:53,000
the -- good part of the originating data set.

366
00:24:53,000 --> 00:25:03,000
But can be obfuscated or hidden from the model itself.

367
00:25:03,000 --> 00:25:06,000
Like we were concerned -- we are concerned.

368
00:25:06,000 --> 00:25:12,000
I mean, all of us are concerned about generating -- putting the bar so high

369
00:25:12,000 --> 00:25:14,000
that there is no incentive.

370
00:25:14,000 --> 00:25:18,000
Not only there is no incentive into releasing open source, but there is

371
00:25:18,000 --> 00:25:23,000
actually a very strong disincentive into the open source AI.

372
00:25:23,000 --> 00:25:32,000
And in fact, if you look at the amount of lawsuits that any of the more

373
00:25:32,000 --> 00:25:37,000
open foundation models that have been released, they're receiving just

374
00:25:37,000 --> 00:25:43,000
because they've been transparently saying, hey, we have scour the web and

375
00:25:43,000 --> 00:25:47,000
incorporated pretty much anything that we could find.

376
00:25:47,000 --> 00:25:52,000
That, you know, give us -- while on the other side of the spectrum, we

377
00:25:52,000 --> 00:26:00,000
have things like Lama 3 or open AI, they don't say what's inside the

378
00:26:00,000 --> 00:26:04,000
data sets, what went into their training sets, because they don't want to

379
00:26:04,000 --> 00:26:06,000
be sued.

380
00:26:06,000 --> 00:26:10,000
So we're trying to see if there is a balance to be found.

381
00:26:10,000 --> 00:26:14,000
And let's -- the validation is also for this.

382
00:26:14,000 --> 00:26:23,000
Like do we have a set of models out there that already exist that can

383
00:26:23,000 --> 00:26:27,000
fit into the open source AI definition and we can actually work with

384
00:26:27,000 --> 00:26:29,000
them?

385
00:26:29,000 --> 00:26:36,000
Yeah, we want to be pragmatic, but we also want to maintain the

386
00:26:36,000 --> 00:26:38,000
principles right.

387
00:26:38,000 --> 00:26:44,000
The principles that we want to give users freedom, agency, control

388
00:26:44,000 --> 00:26:48,000
over the technical choices doesn't necessarily mean that we want to

389
00:26:48,000 --> 00:26:52,000
have the full spectrum of everything always open all the time.

390
00:26:52,000 --> 00:26:59,000
Because it may be that there is enough elements even without having

391
00:26:59,000 --> 00:27:01,000
the complete spectrum.

392
00:27:01,000 --> 00:27:06,000
If you have looked at the model openness framework paper, it's an

393
00:27:06,000 --> 00:27:11,000
interesting read because it has -- lists all of those components and

394
00:27:11,000 --> 00:27:21,000
then gives a sliding scale of what's required for the -- to be included

395
00:27:21,000 --> 00:27:23,000
in the Linux foundation projects.

396
00:27:23,000 --> 00:27:28,000
And it scales from basically just give us the model parameters all the

397
00:27:28,000 --> 00:27:30,000
way to give us everything.

398
00:27:30,000 --> 00:27:35,000
And they call it open models, open tooling, when some more extra

399
00:27:35,000 --> 00:27:37,000
pieces are available.

400
00:27:37,000 --> 00:27:39,000
In open science where there is everything.

401
00:27:39,000 --> 00:27:43,000
So we're trying to find a balance in there where there is a bar where

402
00:27:43,000 --> 00:27:47,000
we can say, okay, this is open source AI, but -- and then everything

403
00:27:47,000 --> 00:27:49,000
else goes on top.

404
00:27:49,000 --> 00:27:51,000
Yeah.

405
00:27:51,000 --> 00:27:53,000
All right.

406
00:27:53,000 --> 00:27:56,000
So the transparency is a feature of open source.

407
00:27:56,000 --> 00:27:58,000
>> I just want to pause.

408
00:27:58,000 --> 00:28:00,000
I just want to pause.

409
00:28:00,000 --> 00:28:03,000
Because we can continue this conversation for a long time.

410
00:28:03,000 --> 00:28:05,000
I just want to pause.

411
00:28:05,000 --> 00:28:09,000
Is there anyone else on the call who has a different kind of question?

412
00:28:09,000 --> 00:28:14,000
And if not, we can continue this conversation for a bit of time.

413
00:28:14,000 --> 00:28:16,000
Okay.

414
00:28:16,000 --> 00:28:18,000
No.

415
00:28:18,000 --> 00:28:20,000
All right.

416
00:28:20,000 --> 00:28:22,000
So, yeah.

417
00:28:22,000 --> 00:28:24,000
So we can continue, I don't know, to the half hour maybe talking about

418
00:28:24,000 --> 00:28:26,000
this topic?

419
00:28:26,000 --> 00:28:28,000
>> Yeah.

420
00:28:28,000 --> 00:28:32,000
I just want to also announce the fact that we're going to be talking

421
00:28:32,000 --> 00:28:34,000
about data in a separate talk.

422
00:28:34,000 --> 00:28:39,000
Like there is absolutely a need to better understand the space also

423
00:28:39,000 --> 00:28:41,000
as data becomes functional.

424
00:28:41,000 --> 00:28:43,000
Which is a new thing.

425
00:28:43,000 --> 00:28:45,000
Fairly new thing.

426
00:28:45,000 --> 00:28:47,000
And we want to understand it a little bit better.

427
00:28:47,000 --> 00:28:51,000
One of the stops in September is in France.

428
00:28:51,000 --> 00:28:55,000
And we're working on a workshop specifically to start the

429
00:28:55,000 --> 00:28:59,000
conversation that I'm quite sure is going to take a little bit longer.

430
00:28:59,000 --> 00:29:01,000
Because transparency is the feature.

431
00:29:01,000 --> 00:29:04,000
I want to highlight that.

432
00:29:04,000 --> 00:29:08,000
That's why we're insisting on having data information, provenance

433
00:29:08,000 --> 00:29:10,000
and all of that.

434
00:29:10,000 --> 00:29:13,000
And the code for the training.

435
00:29:13,000 --> 00:29:16,000
The code used for the creation of that data set.

436
00:29:16,000 --> 00:29:20,000
I think it's going to be -- it's a good compromise.

437
00:29:20,000 --> 00:29:28,000
But let's see what else exists in there.

438
00:29:28,000 --> 00:29:30,000
>> I see Claire, you have a comment.

439
00:29:30,000 --> 00:29:33,000
Do you want to come off mute, Claire?

440
00:29:33,000 --> 00:29:35,000
>> Sure.

441
00:29:35,000 --> 00:29:37,000
Thank you.

442
00:29:37,000 --> 00:29:42,000
So this is just a question to see if there is any current effort

443
00:29:42,000 --> 00:29:46,000
going into keeping track of any other definitions of open source

444
00:29:46,000 --> 00:29:48,000
AI.

445
00:29:48,000 --> 00:29:51,000
And specifically thinking about anything that might be referenced

446
00:29:51,000 --> 00:29:55,000
in any emerging regulation or policies that might be coming at a

447
00:29:55,000 --> 00:29:57,000
government level.

448
00:29:57,000 --> 00:30:01,000
Knowing that I think it was referenced in the AI act, but I'm

449
00:30:01,000 --> 00:30:05,000
not sure how it was referenced or what they defined it as in that

450
00:30:05,000 --> 00:30:07,000
act.

451
00:30:07,000 --> 00:30:09,000
>> Yeah.

452
00:30:09,000 --> 00:30:11,000
So thanks for the question.

453
00:30:11,000 --> 00:30:15,000
Because the AI act mentions multiple times, a couple of times

454
00:30:15,000 --> 00:30:19,000
that it's a free and open source AI system without providing any

455
00:30:19,000 --> 00:30:21,000
explanation of what that means.

456
00:30:21,000 --> 00:30:26,000
So it was one of the triggers to push for this project to start

457
00:30:26,000 --> 00:30:31,000
two years ago when we saw -- when I saw the first draft of the

458
00:30:31,000 --> 00:30:36,000
AI act, I was like, oh, we need to have -- we need to help

459
00:30:36,000 --> 00:30:38,000
regulators understand this space.

460
00:30:38,000 --> 00:30:42,000
And we're having fairly good, intense conversations also with

461
00:30:42,000 --> 00:30:48,000
the American agencies which are now under pressure to come up

462
00:30:48,000 --> 00:30:52,000
with regulations inside to -- right.

463
00:30:52,000 --> 00:30:55,000
To control a little bit this market.

464
00:30:55,000 --> 00:30:59,000
As it comes out, there are so many foundation models that it

465
00:30:59,000 --> 00:31:03,000
talks about risks and they all want to know what open source

466
00:31:03,000 --> 00:31:07,000
means, what open means in this space and what the implications

467
00:31:07,000 --> 00:31:11,000
are for public in general, for public interest.

468
00:31:11,000 --> 00:31:13,000
>> Thank you.

469
00:31:13,000 --> 00:31:19,000
>> Maybe one final thought or comment and then we'll close the

470
00:31:19,000 --> 00:31:21,000
session.

471
00:31:21,000 --> 00:31:23,000
I saw you typing.

472
00:31:23,000 --> 00:31:30,000
If you want to have a last thought, you're more than welcome

473
00:31:30,000 --> 00:31:32,000
to share it.

474
00:31:32,000 --> 00:31:46,000
>> Maybe I see types.

475
00:31:46,000 --> 00:31:51,000
I can say watch the space, watch our blog, too, because we're

476
00:31:51,000 --> 00:31:55,000
going to be continuing the conversations around data.

477
00:31:55,000 --> 00:32:00,000
I think it's a really important space where we don't have a lot

478
00:32:00,000 --> 00:32:02,000
of practice.

479
00:32:02,000 --> 00:32:07,000
There are new legal questions that are being raised like what

480
00:32:07,000 --> 00:32:11,000
is exactly right, acceptable when it comes to text and data

481
00:32:11,000 --> 00:32:13,000
mining.

482
00:32:13,000 --> 00:32:16,000
New regulation around text and data mining specifically

483
00:32:16,000 --> 00:32:22,000
appearing around the world like Japan has an approach that is

484
00:32:22,000 --> 00:32:28,000
very -- Europe has introduced it as a new right and it's still

485
00:32:28,000 --> 00:32:32,000
having -- there are some limitations to it, though.

486
00:32:32,000 --> 00:32:37,000
So policies are going to be written and all the lawsuits in

487
00:32:37,000 --> 00:32:41,000
the United States which are very interesting and we're waiting

488
00:32:41,000 --> 00:32:43,000
for them to be clarified.

489
00:32:43,000 --> 00:32:48,000
Yes, so Claire, the outreach plan is vast.

490
00:32:48,000 --> 00:32:53,000
Like we've scrolled through with Mayor, but yes, we are reaching

491
00:32:53,000 --> 00:33:00,000
out to a lot of the AI communities, startups, developers,

492
00:33:00,000 --> 00:33:04,000
conferences like new rips and others.

493
00:33:04,000 --> 00:33:06,000
Yeah.

494
00:33:06,000 --> 00:33:12,000
Granted, it's going to -- remember what I like to remind

495
00:33:12,000 --> 00:33:17,000
everyone, that the open source definition came out after at

496
00:33:17,000 --> 00:33:22,000
least 15 years of experience in the free software world and when

497
00:33:22,000 --> 00:33:28,000
the developers were few, computers were not as ubiquitous as

498
00:33:28,000 --> 00:33:34,000
they are now, and it took a while to become so well-known and

499
00:33:34,000 --> 00:33:41,000
widely respected, so we'll have to be doing this work of open

500
00:33:41,000 --> 00:33:43,000
source AI.

501
00:33:43,000 --> 00:33:45,000
We're defining it in a few months.

502
00:33:45,000 --> 00:33:49,000
So we're going to have to do a lot of work after -- continue to

503
00:33:49,000 --> 00:33:55,000
do a lot of this outreach work in the next years.

504
00:33:55,000 --> 00:34:00,000
Yes, if you're coming to PyCon, you'll find us there.

505
00:34:00,000 --> 00:34:05,000
That's our next stop.

506
00:34:05,000 --> 00:34:07,000
All right.

507
00:34:07,000 --> 00:34:08,000
Thanks, everyone.

508
00:34:08,000 --> 00:34:10,000
We host this every two weeks.

509
00:34:10,000 --> 00:34:14,000
I think that next week I'll be at PyCon, so we will not be able

510
00:34:14,000 --> 00:34:16,000
to have this.

511
00:34:16,000 --> 00:34:20,000
But we'll meet again in two weeks and the forums are still

512
00:34:20,000 --> 00:34:22,000
going to be active and available.

513
00:34:22,000 --> 00:34:24,000
Thanks, everyone.

