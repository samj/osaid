1
00:00:00,001 --> 00:00:08,440
to start the recording. Thank you. Yes. So welcome everyone to our biweekly town hall

2
00:00:08,440 --> 00:00:14,200
for the open source AI definition process. And you can hear I've got a little bit of

3
00:00:14,200 --> 00:00:22,500
a sore throat, but I'll hopefully be able to make it through. I will start. So these

4
00:00:22,500 --> 00:00:26,680
are our community agreements that we have at every meeting. Some of you may have already

5
00:00:26,680 --> 00:00:34,840
seen these. One mic, one speaker is about non-interrupting. Also when we get to the

6
00:00:34,840 --> 00:00:41,320
Q&A, if you have multiple questions, please ask your question and then pause and let at

7
00:00:41,320 --> 00:00:47,440
least one other person ask their own question before you ask a second question. Take space,

8
00:00:47,440 --> 00:00:53,320
make space is a similar spirit. You know, just to invite others to share space with

9
00:00:53,320 --> 00:01:01,760
us. And that some feel more shy and some feel more extroverted and everyone's voice matters.

10
00:01:01,760 --> 00:01:06,800
Kindness is just that the work is hard, but we don't have to be. And just to remind ourselves

11
00:01:06,800 --> 00:01:12,160
to be gentle and curious with each other, even when we disagree. Forward motion means

12
00:01:12,160 --> 00:01:18,520
focusing on what's possible and not letting obstacles prevent the process from moving

13
00:01:18,520 --> 00:01:24,680
forward. Similarly, solution seeking, just this work is very complex and it's vulnerable

14
00:01:24,680 --> 00:01:30,960
to suggest a solution, but that is how we move forward. And anything else that people

15
00:01:30,960 --> 00:01:37,320
would like to add to our community agreements for this meeting? You can say it in the chat

16
00:01:37,320 --> 00:01:47,400
if you want. Okay. I'll continue. So yes, we are creating an open source AI definition

17
00:01:47,400 --> 00:01:57,600
this year. And the current version is still 0.0.8. That's been the case for about a month.

18
00:01:57,600 --> 00:02:06,240
And these are the pieces of it. Maybe Nick could drop the link to the HackMD page into

19
00:02:06,240 --> 00:02:13,520
the chat because this is very hard to read. It just shows you the parts kind of as a map.

20
00:02:13,520 --> 00:02:21,680
So we have a preamble. We have the four freedoms, studies, modify, and share as applied to AI.

21
00:02:21,680 --> 00:02:29,120
And then we have the it's not circled, but we have preferred form for data information,

22
00:02:29,120 --> 00:02:34,200
code and model. There's some description there. And then the current version also has a legal

23
00:02:34,200 --> 00:02:41,360
thank you, Nick, has a legal checklist of what the licenses would be on the specific

24
00:02:41,360 --> 00:02:49,120
components that would be required in those three categories of data information, code

25
00:02:49,120 --> 00:02:56,160
and model. And what we're working on is versions. Yes. And we're open to comments. That's right.

26
00:02:56,160 --> 00:03:04,720
And you can actually comment on that document. HackMD is a commenting platform. So what we're

27
00:03:04,720 --> 00:03:12,840
working on right now is 0.0.9. And there will be some changes, a few changes. So in the

28
00:03:12,840 --> 00:03:19,080
preamble, we are clarifying that recipients of the freedoms are developers, players, and

29
00:03:19,080 --> 00:03:27,200
end users. So those freedoms of study, use, modification, and sharing. We are crediting

30
00:03:27,200 --> 00:03:35,760
the Free Software Foundation for initially developing these four freedoms because crediting

31
00:03:35,760 --> 00:03:42,120
people is a good thing to do. And so we're adding that in. And you can see in this larger

32
00:03:42,120 --> 00:03:51,200
box how those freedoms are enumerated or described. The language should not be surprising to anyone.

33
00:03:51,200 --> 00:03:56,400
And we did develop these four freedoms in a series of co-design workshops at the end

34
00:03:56,400 --> 00:04:08,320
of last year. Also in the four freedoms, we are going to, in 0.0.9, underline that the

35
00:04:08,320 --> 00:04:17,120
components must be free from encumbrance. That prevents any of those three user types,

36
00:04:17,120 --> 00:04:22,400
developer, deployer, or end user from exercising the freedoms. So just underlining that, yes,

37
00:04:22,400 --> 00:04:28,080
the four freedoms must be respected. And also if you're a little confused, you're not seeing

38
00:04:28,080 --> 00:04:34,600
0.0.9 in these images. You're seeing 0.0.8. But I'm just indicating where the changes

39
00:04:34,600 --> 00:04:40,640
will be. Also in preferred form, we are going to add

40
00:04:40,640 --> 00:04:48,480
definitions which will be just a phrase, not a sentence, for the terms OSD compliant, which

41
00:04:48,480 --> 00:04:53,120
is a requirement of data information, and OSD conformant, which is a requirement of

42
00:04:53,120 --> 00:05:00,900
model parameters so that the legal requirements are clear. And the code components just need

43
00:05:00,900 --> 00:05:08,880
to be licensed under an OSI approved license. So that's very straightforward. The code or

44
00:05:08,880 --> 00:05:14,480
software can exist under those licenses. And the licensing and legal requirements are slightly

45
00:05:14,480 --> 00:05:20,360
different for those other types of components. And so we're going to define what these terms

46
00:05:20,360 --> 00:05:26,760
mean, compliant and conformant, the next version. And then, oh, someone unmuted themselves.

47
00:05:26,760 --> 00:05:30,760
Please just stand by until the Q&A. If you have a question that you'd like to ask before

48
00:05:30,760 --> 00:05:39,520
the Q&A, you can drop it in the chat. Okay. So the checklist. The checklist, we are in

49
00:05:39,520 --> 00:05:46,240
the next version going to actually move it into a separate document. We realized that

50
00:05:46,240 --> 00:05:51,440
in trying to create the definition and also to operationalize the definition in the same

51
00:05:51,440 --> 00:05:57,880
process was a bit like jogging and juggling at the same time. And so we thought, let's

52
00:05:57,880 --> 00:06:03,640
just focus on the definition. Let's basically the definition will stop at preferred form.

53
00:06:03,640 --> 00:06:08,320
And then obviously operationalization of the definition is quite important. And that is

54
00:06:08,320 --> 00:06:15,720
the checklist. And we're just going to separate those documents and also those processes.

55
00:06:15,720 --> 00:06:21,600
One change that you will see in the checklist in version 9, and I guess we'll figure out

56
00:06:21,600 --> 00:06:26,000
how to label the versions of these documents as well. But if you're not, we will figure

57
00:06:26,000 --> 00:06:32,880
that out. It will be updated so that all the components are from the model openness framework.

58
00:06:32,880 --> 00:06:38,800
So right now, if you look on the left, the data information components are coming from

59
00:06:38,800 --> 00:06:47,320
the EU AI Act. So training methodologies and techniques, training, data scope and characteristics,

60
00:06:47,320 --> 00:06:52,560
training data providence, etc. Those are not coming from the model openness framework,

61
00:06:52,560 --> 00:06:58,120
which is a list of components from the Linux Foundation. It's coming from the EU AI Act.

62
00:06:58,120 --> 00:07:04,400
And we just because of all the great work that the Linux Foundation is doing to create

63
00:07:04,400 --> 00:07:13,240
an online compendium of AI systems and the openness of their components, we really want

64
00:07:13,240 --> 00:07:19,880
to be able to rely on that for our own definition. And so we are going to use their component

65
00:07:19,880 --> 00:07:26,680
list exclusively in our in our checklist. And I'll just read Stefano's comment, the

66
00:07:26,680 --> 00:07:33,440
data information piece is going to look the same in 0.0.9. Oh, not because it's decided,

67
00:07:33,440 --> 00:07:42,600
but because the topic is still being discussed. Okay, got it. So okay, so we're not yet. So

68
00:07:42,600 --> 00:07:52,400
eventually, we will be transitioning over to model openness framework components. That's

69
00:07:52,400 --> 00:08:01,480
not happening in 0.0.9. Thank you for the clarification. Okay, system validation. It's

70
00:08:01,480 --> 00:08:07,360
pretty much the same as last time. So thank you to Arctic and LLM 360 for helping to identify

71
00:08:07,360 --> 00:08:14,840
documentation, we found that it's really crucial to have creators help out with identifying

72
00:08:14,840 --> 00:08:20,560
the legal documents describing the rights and permissions associated with the components

73
00:08:20,560 --> 00:08:31,000
of their systems. So this screenshot of the progress and validation based on the process

74
00:08:31,000 --> 00:08:39,000
of the review, which is being done by volunteers. And also the the results of that review so

75
00:08:39,000 --> 00:08:49,360
far have not have not changed since last time. And yeah, we just find that we do need creators

76
00:08:49,360 --> 00:08:56,800
to help provide documentation in order to to know whether a system would meet the requirements

77
00:08:56,800 --> 00:09:04,720
of the open source AI definition. And Stefano's typing. I'll just wait for that if it's on

78
00:09:04,720 --> 00:09:15,760
validation. Okay, pause. I'll read I'll read the comments if it comes up. Okay, so what's

79
00:09:15,760 --> 00:09:21,440
next June, which we're almost at the end of through October, we do we want to obviously

80
00:09:21,440 --> 00:09:29,680
complete the validation phase resolve comments and release versions 0.7.9. And then cut the

81
00:09:29,680 --> 00:09:42,320
release candidate with sufficient endorsement, organizational endorsement. Okay, just keep

82
00:09:42,320 --> 00:09:49,680
going. And this is our timeline, we had to update our timeline, because it ended at June,

83
00:09:49,680 --> 00:09:56,920
I believe. And now we have to think through to the end of the project in October. So we

84
00:09:56,920 --> 00:10:06,200
will be at hospitals for good in New York, both the UN event and the side event. We will

85
00:10:06,200 --> 00:10:16,760
this is in July, I'll be doing a virtual event for sustain Africa. And this is to share the

86
00:10:16,760 --> 00:10:26,680
OSAID and also to get further feedback. Let me pause and read what Stefano is writing.

87
00:10:26,680 --> 00:10:32,200
The 0.0.9 draft includes a lot of small changes accumulated over two months since the release

88
00:10:32,200 --> 00:10:37,120
of the previous draft, but all the pieces of 0.0.9 can still change based on community

89
00:10:37,120 --> 00:10:49,840
feedback. Okay, that makes sense. Okay, so August, we'll be in Hong Kong for AI dev. And

90
00:10:49,840 --> 00:10:56,800
September. Gosh, let's see if I can pronounce this correctly with my cold. Merdeala. Okay,

91
00:10:56,800 --> 00:11:04,240
that's pretty good. And Buenos Aires. And then October, we will be launching the stable

92
00:11:04,240 --> 00:11:11,080
version of the definition at all things open in Raleigh in the US. And then we will also

93
00:11:11,080 --> 00:11:19,840
be doing a data workshop in Europe, in a city TBD. And the focus of that workshop, it will

94
00:11:19,840 --> 00:11:27,440
be to write a policy paper to try to resolve some of these challenges with the sharing

95
00:11:27,440 --> 00:11:41,240
of data sets. Yeah. Yeah. And as Stefano is saying, nothing is set in stone yet. Okay.

96
00:11:41,240 --> 00:11:46,920
So yes, and actually, you've actually already seen most of these. Yep. The different events

97
00:11:46,920 --> 00:11:55,240
that we're going to and when they are. So how to participate in this process. We do

98
00:11:55,240 --> 00:12:06,680
share updates on the process and opportunities for volunteering also discuss issues of you

99
00:12:06,680 --> 00:12:17,360
know, disagreement and difference of opinion on the public forum, which is discussed on

100
00:12:17,360 --> 00:12:23,400
open source.org, which you can join for free, you do have to sign up just to prevent spam.

101
00:12:23,400 --> 00:12:29,160
And then we have these bi weekly town halls, both for the this is one for the Europe, North

102
00:12:29,160 --> 00:12:37,320
America and our Americas time zone. And then we have a second one. That is Europe, Africa

103
00:12:37,320 --> 00:12:44,320
and Asia. So we cycle back between those two times. And then yes, if you would like to

104
00:12:44,320 --> 00:12:49,600
volunteer a validation, particularly I think about volunteering we need at this point is

105
00:12:49,600 --> 00:12:59,160
if you are the creator of a an AI system and would like to, you know, show that it is open

106
00:12:59,160 --> 00:13:05,660
according to the definition that we have now, that that would be the most valuable type

107
00:13:05,660 --> 00:13:13,080
of volunteer just because those are those are the those individuals that have this documentation

108
00:13:13,080 --> 00:13:21,280
information most at hand. Yes, there's also a blog that we update and I can share that

109
00:13:21,280 --> 00:13:27,800
and their summaries are shared every Monday on the blog. And yes, we highly recommend

110
00:13:27,800 --> 00:13:37,760
the weekly summaries. So yeah, we will now do a Q&A. And what you can do is, I think,

111
00:13:37,760 --> 00:13:42,880
raise, raise your hand and you can come off mute or you can just ask in the chat and I'll

112
00:13:42,880 --> 00:14:09,160
read it like I've been doing throughout the meeting up to you. Love to hear your thoughts.

113
00:14:09,160 --> 00:14:14,160
And for all those who who are not as familiar with our organization, Stefano, whose shots

114
00:14:14,160 --> 00:14:20,040
I was reading is the executive director of the OSI. So if anyone was wondering why, why

115
00:14:20,040 --> 00:14:24,880
is she reading aloud all these comments from this one participant? That is why he can't

116
00:14:24,880 --> 00:14:37,520
participate live today. But that's why I was reading his comments. So yeah, I will just

117
00:14:37,520 --> 00:14:44,360
be shy for a bit. Yeah, don't be shy. Yeah, don't be shy. You can actually click on the

118
00:14:44,360 --> 00:14:53,720
microphone and speak or you can just chat as well. It's OK. Yes. OK. Oh, you're welcome,

119
00:14:53,720 --> 00:15:11,040
Stefano. Yeah, thank you so much for coming. OK, yeah, go ahead, Gerardo, you can unmute

120
00:15:11,040 --> 00:15:26,240
yourself. Hi. Yes. What's your question? I've been participating in several standard committees

121
00:15:26,240 --> 00:15:37,360
on IEEE about the ethical use of AI and several AI definitions, and I'm finding that most

122
00:15:37,360 --> 00:15:52,120
of the people I met there have not yet, do not yet know about this initiative and I'm

123
00:15:52,120 --> 00:16:05,360
changing that. But I've been wondering if we don't need to push this discussion a little

124
00:16:05,360 --> 00:16:16,600
bit forward with the scientific community, especially with certain researchers that are

125
00:16:16,600 --> 00:16:30,000
dealing with this on several bases. I think I've covered most of the AI ethics discussions

126
00:16:30,000 --> 00:16:41,040
that are occurring, but it seems to me there's more going on that probably we should be a

127
00:16:41,040 --> 00:16:53,400
part of. OK, yeah, I may not be the person to respond to that. My primary role is or

128
00:16:53,400 --> 00:17:01,720
my role is running the co-design process of the definition itself within OSI. But Stefano's

129
00:17:01,720 --> 00:17:08,760
typing, but that sounds very, does sound very useful. And thank you for bringing those experiences

130
00:17:08,760 --> 00:17:14,840
you had in other organization and standards making processes here. That's very, very helpful

131
00:17:14,840 --> 00:17:23,320
to us. Gerardo, so we're working with several researchers and several organizations as well

132
00:17:23,320 --> 00:17:34,200
and we would love if you have contacts with other researchers with whom we could work

133
00:17:34,200 --> 00:17:39,720
with, that would be splendid. I just would like to highlight something because you mentioned

134
00:17:39,720 --> 00:17:47,800
ethics, right? So even though, yes, ethics is really important for us, we want to see

135
00:17:47,800 --> 00:17:57,880
open source AI being used for good, right? For the benefit of humanity. But ethics, as

136
00:17:57,880 --> 00:18:04,280
the open source AI definition is concerned, is out of scope. It doesn't mean it doesn't

137
00:18:04,280 --> 00:18:12,640
matter. It does matter that open source AI is used for the good, but it's just something

138
00:18:12,640 --> 00:18:19,280
out of scope. I'm not sure if that clarifies your question.

139
00:18:19,280 --> 00:18:31,440
Well, it's more on the way that for most of the discussions we are having, some of them

140
00:18:31,440 --> 00:18:39,960
are about the age of tools and so on. There is a need for those groups to know a little

141
00:18:39,960 --> 00:18:53,960
bit of the strains that we want to impose on these AI models to be labeled open source.

142
00:18:53,960 --> 00:19:02,960
Because it's something, when you're talking about the ethics of using AI, that also brings

143
00:19:02,960 --> 00:19:14,080
in the fact that the open source approach is more ethical in the terms of the way things

144
00:19:14,080 --> 00:19:22,120
are constructed, in terms of the transparency, the sharing of knowledge. And one of the concerns

145
00:19:22,120 --> 00:19:35,000
most of all of these groups are in is the issue of explainability. And probably that's

146
00:19:35,000 --> 00:19:41,520
something else, but probably something that we should be addressing. The explainability

147
00:19:41,520 --> 00:19:51,640
becomes easier when something is really transparent and clear and open source, more or less forces

148
00:19:51,640 --> 00:20:00,480
you into this. And so it's more the other way around. It's not that this issue depends

149
00:20:00,480 --> 00:20:10,280
on them. It's more that they, that's my part, they have to be aware of all of this.

150
00:20:10,280 --> 00:20:17,120
That's a very good point. In fact, we are in touch with a few researchers around explainable

151
00:20:17,120 --> 00:20:25,720
AI. And it's really, really interesting. So happy to connect with you. Thank you.

152
00:20:25,720 --> 00:20:33,360
Thank you. Thank you. Does anyone else have a different question? Thank you, Gerardo and

153
00:20:33,360 --> 00:20:36,360
Rick.

154
00:20:36,360 --> 00:20:48,920
Hello, Anastasia. So I see a comment about how the data information piece is going to

155
00:20:48,920 --> 00:20:55,920
stay the same in 0.9. Is that talking about how you're not going to change how you address

156
00:20:55,920 --> 00:21:02,560
the topic of data and the availability of information about the data? And I guess I'm

157
00:21:02,560 --> 00:21:09,040
curious about the general thinking since open data may not always be realistic, but it is

158
00:21:09,040 --> 00:21:18,240
this potentially important piece. So I'm curious about the focus or lack of focus on open data.

159
00:21:18,240 --> 00:21:20,240
I guess just in general.

160
00:21:20,240 --> 00:21:26,360
Yeah, I would actually, Stefano, if you want to chat or come off mute, I would love for

161
00:21:26,360 --> 00:21:35,720
you to answer that since it seems like you I mean, Stefano is is the leader on that particular

162
00:21:35,720 --> 00:21:53,000
element of this. If he is on the chat. He is. Oh, but he is not. I see his. So I'm I

163
00:21:53,000 --> 00:21:59,560
in the absence of Stefano, who made a correction on something in my slide. So obviously he

164
00:21:59,560 --> 00:22:06,880
has information I don't. I actually can't give anything other than what he just said.

165
00:22:06,880 --> 00:22:13,560
So there's two pieces of information shared. One one is that data information will not

166
00:22:13,560 --> 00:22:21,120
change from output. Oh, Stefano is coming back on. Stefano, we were wondering if you

167
00:22:21,120 --> 00:22:31,520
could answer. OK, I would still just love for you to answer Anastasia's question.

168
00:22:31,520 --> 00:22:44,240
Did you want her to ask again? Did you hear it? OK, I know all he knows. OK, so then I'll

169
00:22:44,240 --> 00:22:50,400
just continue. So there's two pieces of information shared once that data information will not

170
00:22:50,400 --> 00:23:00,840
change from 0.0.8, the link to which Nick shared and also that the checklist will conform

171
00:23:00,840 --> 00:23:10,600
to the MOF. So the way that I take that to mean. Is that.

172
00:23:10,600 --> 00:23:16,760
Stefano, do you want to do you want to speak? I don't know if you're maybe you're not

173
00:23:16,760 --> 00:23:23,920
know that you can speak. OK, it won't change only because the discussion is not complete.

174
00:23:23,920 --> 00:23:31,000
OK, OK, OK, can't speak. OK, yes. So there will be there will be a shift to the components

175
00:23:31,000 --> 00:23:37,800
coming from the model openness framework or MOF. And there may be I guess there may be

176
00:23:37,800 --> 00:23:44,520
other changes as well, because Stefano is saying the discussion is not complete.

177
00:23:44,520 --> 00:23:50,920
So maybe I can, Ximing, mirror. So there has been a lot of discussions around the data

178
00:23:50,920 --> 00:24:00,520
information and I'll share a blog post as well that Stef wrote, which is very interesting

179
00:24:00,520 --> 00:24:08,280
and also point a discussion topic that we have here. This is something that we're receiving

180
00:24:08,280 --> 00:24:17,240
a lot of feedback and hearing the working group members also participated in those discussions

181
00:24:17,240 --> 00:24:22,680
and to try to understand the role of data and data information. And this is actually

182
00:24:22,680 --> 00:24:29,080
a crucial point of the open source CI definition. So let me just drop some links here and we

183
00:24:29,080 --> 00:24:33,400
look forward to hearing more feedback on the discussion forum.

184
00:24:33,400 --> 00:24:42,520
OK, thank you. And Stefano has just written, we may even remove the text altogether and

185
00:24:42,520 --> 00:24:50,160
put a placeholder instead. And thank you, Nick, for that explainer link to the explainer

186
00:24:50,160 --> 00:24:56,760
post on data information. I I'm sorry if that wasn't satisfying, Anastasia. That is that

187
00:24:56,760 --> 00:25:02,520
is the level of response that I think we can give to you right now. But thank you for asking

188
00:25:02,520 --> 00:25:24,600
the question. Does anyone else have a question? Yes, Gerardo, go ahead.

189
00:25:24,600 --> 00:25:41,720
Just one more probably comes from the model we are working with, but I have an issue with

190
00:25:41,720 --> 00:25:49,120
defining codes that trains and codes that trains a model and code that uses a model

191
00:25:49,120 --> 00:26:00,440
of the same thing. And I think we should be splitting those and make it so that it's more

192
00:26:00,440 --> 00:26:08,160
explicit to say that both parts are to be open source, first of all, because they are

193
00:26:08,160 --> 00:26:25,280
probably not the same thing and and that most of the "open" AI things usually have that

194
00:26:25,280 --> 00:26:32,320
second part of the code open and open source, the thing that uses the model to generate

195
00:26:32,320 --> 00:26:41,280
stuff. But they keep close the code that was used to generate the model. And I think it

196
00:26:41,280 --> 00:26:50,840
may be important to split and make it explicit that both parts are considered codes and they

197
00:26:50,840 --> 00:27:05,120
are and they have to be open license and open source for the whole thing to be open source.

198
00:27:05,120 --> 00:27:12,720
And something I haven't yet seen and why it's defined here as just one thing, but usually

199
00:27:12,720 --> 00:27:20,960
it can become those two codes can be developed separately by different people on different

200
00:27:20,960 --> 00:27:32,600
teams. And there could be, let us say, the temptation to license to open license one

201
00:27:32,600 --> 00:27:35,600
and not the other.

202
00:27:35,600 --> 00:27:47,360
Okay, so regarding the three kind of categories of AI systems, we have data information, code

203
00:27:47,360 --> 00:27:54,520
and model, we are basically trying to have as few categories as possible that were still

204
00:27:54,520 --> 00:27:59,520
descriptive of the different types. And obviously, there were other types of categorization we

205
00:27:59,520 --> 00:28:07,480
could have used. That one seems pretty solid. In terms of the types of code, we are using

206
00:28:07,480 --> 00:28:18,520
this maybe, maybe you're aware of this, and maybe not. This list of types of code components

207
00:28:18,520 --> 00:28:26,040
or artifacts used to create AI systems, or used in AI systems from this model openness

208
00:28:26,040 --> 00:28:33,240
framework that was developed by a group of researchers and practitioners at and affiliated

209
00:28:33,240 --> 00:28:40,080
with the Linux Foundation. And we so as not to invent the wheel, because there's so much

210
00:28:40,080 --> 00:28:46,920
that we are doing ourselves, we said, How can we rely on the brilliance of others. And

211
00:28:46,920 --> 00:28:56,400
so we are using those code components. So we are we are not going to, I believe, change

212
00:28:56,400 --> 00:29:01,160
those components as listed in this document. And maybe Nick could even share a link to

213
00:29:01,160 --> 00:29:06,440
that it's a white paper is where those components live. Now, I think the Linux Foundation is

214
00:29:06,440 --> 00:29:12,040
also spinning up some websites and landing pages. But as of now, I know that they that

215
00:29:12,040 --> 00:29:18,400
you can find those in the white paper. So we are we're, we will just rely on that work

216
00:29:18,400 --> 00:29:24,760
that they've done. Let's maybe take one more question. And then Yeah, you're welcome to

217
00:29:24,760 --> 00:29:34,320
order. And then we'll call it a day. And Stefan, I was just saying we all care about open data.

218
00:29:34,320 --> 00:29:39,160
And we're very concerned about the issue of accessing data suitable to train an AI, which

219
00:29:39,160 --> 00:29:48,680
is true. Yep, absolutely. So, um, okay. Any, any other question from someone? I would say

220
00:29:48,680 --> 00:29:56,080
another question from someone who hasn't asked one yet. And I'll just keep reading what Stefan

221
00:29:56,080 --> 00:30:01,040
was saying, because he would be presenting with me if he were available. So he's saying

222
00:30:01,040 --> 00:30:05,960
we're also planning a conference specifically on this topic, which is that event in October,

223
00:30:05,960 --> 00:30:14,760
in in Europe in the city TBD is that comments or workshop on data, the issue of data and

224
00:30:14,760 --> 00:30:23,920
AI, which is such a substantial one. Um, okay, I'm not seeing any questions. Thank you. Thank

225
00:30:23,920 --> 00:30:30,200
you to everyone who came and who did ask a question that was really useful. Yeah, thank

226
00:30:30,200 --> 00:30:35,320
you, Gerardo. Thumbs up. Yeah, so I think we can turn off the recording. And thank you

227
00:30:35,320 --> 00:30:39,040
everyone for coming and have a have a wonderful weekend and do find us

